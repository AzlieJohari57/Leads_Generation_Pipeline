{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "028401e3",
   "metadata": {},
   "source": [
    "# Data Ingestion (Bronze 1)\n",
    "\n",
    "- Data ingestion from ACRA, MasterDB and SSIC mapping, this data will be merged and filter with selected industry to get the specific company havent been researched by MR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a1a49ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import time\n",
    "import scrapy\n",
    "from scrapy_playwright.page import PageMethod\n",
    "from bs4 import BeautifulSoup\n",
    "import nest_asyncio\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b54fb73",
   "metadata": {},
   "source": [
    "### Getting Master DB via Google API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab84012a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RefreshError",
     "evalue": "('invalid_grant: Invalid JWT Signature.', {'error': 'invalid_grant', 'error_description': 'Invalid JWT Signature.'})",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRefreshError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Using a large range to ensure we get all data (sheets typically don't exceed 1000 columns)\u001b[39;00m\n\u001b[32m     24\u001b[39m range_a1 = \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mMASTER DATABASE 2025 Template\u001b[39m\u001b[33m'\u001b[39m\u001b[33m!A:ZZ\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m result = \u001b[43msheet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspreadsheetId\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mrange_a1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m values = result.get(\u001b[33m'\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m'\u001b[39m, [])\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m values:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\googleapiclient\\_helpers.py:130\u001b[39m, in \u001b[36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement == POSITIONAL_WARNING:\n\u001b[32m    129\u001b[39m         logger.warning(message)\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\googleapiclient\\http.py:923\u001b[39m, in \u001b[36mHttpRequest.execute\u001b[39m\u001b[34m(self, http, num_retries)\u001b[39m\n\u001b[32m    920\u001b[39m     \u001b[38;5;28mself\u001b[39m.headers[\u001b[33m\"\u001b[39m\u001b[33mcontent-length\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.body))\n\u001b[32m    922\u001b[39m \u001b[38;5;66;03m# Handle retries for server-side errors.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m923\u001b[39m resp, content = \u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrequest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muri\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.response_callbacks:\n\u001b[32m    936\u001b[39m     callback(resp)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\googleapiclient\\http.py:191\u001b[39m, in \u001b[36m_retry_request\u001b[39m\u001b[34m(http, num_retries, req_type, sleep, rand, uri, method, *args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    190\u001b[39m     exception = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     resp, content = \u001b[43mhttp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# Retry on SSL errors and socket timeout errors.\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _ssl_SSLError \u001b[38;5;28;01mas\u001b[39;00m ssl_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\google_auth_httplib2.py:209\u001b[39m, in \u001b[36mAuthorizedHttp.request\u001b[39m\u001b[34m(self, uri, method, body, headers, redirections, connection_type, **kwargs)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# Make a copy of the headers. They will be modified by the credentials\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[38;5;66;03m# and we want to pass the original headers if we recurse.\u001b[39;00m\n\u001b[32m    207\u001b[39m request_headers = headers.copy() \u001b[38;5;28;01mif\u001b[39;00m headers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbefore_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_headers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[38;5;66;03m# Check if the body is a file-like stream, and if so, save the body\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# stream position so that it can be restored in case of refresh.\u001b[39;00m\n\u001b[32m    213\u001b[39m body_stream_position = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\google\\auth\\credentials.py:228\u001b[39m, in \u001b[36mCredentials.before_request\u001b[39m\u001b[34m(self, request, method, url, headers)\u001b[39m\n\u001b[32m    226\u001b[39m     \u001b[38;5;28mself\u001b[39m._non_blocking_refresh(request)\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_blocking_refresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m metrics.add_metric_header(headers, \u001b[38;5;28mself\u001b[39m._metric_header_for_usage())\n\u001b[32m    231\u001b[39m \u001b[38;5;28mself\u001b[39m.apply(headers)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\google\\auth\\credentials.py:191\u001b[39m, in \u001b[36mCredentials._blocking_refresh\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_blocking_refresh\u001b[39m(\u001b[38;5;28mself\u001b[39m, request):\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.valid:\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\google\\auth\\credentials.py:365\u001b[39m, in \u001b[36mCredentialsWithTrustBoundary.refresh\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrefresh\u001b[39m(\u001b[38;5;28mself\u001b[39m, request):\n\u001b[32m    360\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Refreshes the access token and the trust boundary.\u001b[39;00m\n\u001b[32m    361\u001b[39m \n\u001b[32m    362\u001b[39m \u001b[33;03m    This method calls the subclass's token refresh logic and then\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[33;03m    refreshes the trust boundary if applicable.\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_refresh_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    366\u001b[39m     \u001b[38;5;28mself\u001b[39m._refresh_trust_boundary(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\google\\oauth2\\service_account.py:459\u001b[39m, in \u001b[36mCredentials._refresh_token\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     assertion = \u001b[38;5;28mself\u001b[39m._make_authorization_grant_assertion()\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     access_token, expiry, _ = \u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjwt_grant\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_token_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massertion\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    462\u001b[39m     \u001b[38;5;28mself\u001b[39m.token = access_token\n\u001b[32m    463\u001b[39m     \u001b[38;5;28mself\u001b[39m.expiry = expiry\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\google\\oauth2\\_client.py:299\u001b[39m, in \u001b[36mjwt_grant\u001b[39m\u001b[34m(request, token_uri, assertion, can_retry)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Implements the JWT Profile for OAuth 2.0 Authorization Grants.\u001b[39;00m\n\u001b[32m    276\u001b[39m \n\u001b[32m    277\u001b[39m \u001b[33;03mFor more details, see `rfc7523 section 4`_.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    295\u001b[39m \u001b[33;03m.. _rfc7523 section 4: https://tools.ietf.org/html/rfc7523#section-4\u001b[39;00m\n\u001b[32m    296\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    297\u001b[39m body = {\u001b[33m\"\u001b[39m\u001b[33massertion\u001b[39m\u001b[33m\"\u001b[39m: assertion, \u001b[33m\"\u001b[39m\u001b[33mgrant_type\u001b[39m\u001b[33m\"\u001b[39m: _JWT_GRANT_TYPE}\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m response_data = \u001b[43m_token_endpoint_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcan_retry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcan_retry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPI_CLIENT_HEADER\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoken_request_access_token_sa_assertion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    310\u001b[39m     access_token = response_data[\u001b[33m\"\u001b[39m\u001b[33maccess_token\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\google\\oauth2\\_client.py:270\u001b[39m, in \u001b[36m_token_endpoint_request\u001b[39m\u001b[34m(request, token_uri, body, access_token, use_json, can_retry, headers, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m response_status_ok, response_data, retryable_error = _token_endpoint_request_no_throw(\n\u001b[32m    260\u001b[39m     request,\n\u001b[32m    261\u001b[39m     token_uri,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m     **kwargs\n\u001b[32m    268\u001b[39m )\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response_status_ok:\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[43m_handle_error_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretryable_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response_data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\google\\oauth2\\_client.py:69\u001b[39m, in \u001b[36m_handle_error_response\u001b[39m\u001b[34m(response_data, retryable_error)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m     67\u001b[39m     error_details = json.dumps(response_data)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exceptions.RefreshError(\n\u001b[32m     70\u001b[39m     error_details, response_data, retryable=retryable_error\n\u001b[32m     71\u001b[39m )\n",
      "\u001b[31mRefreshError\u001b[39m: ('invalid_grant: Invalid JWT Signature.', {'error': 'invalid_grant', 'error_description': 'Invalid JWT Signature.'})"
     ]
    }
   ],
   "source": [
    "\n",
    "#Service Acc: masterdb@axiomatic-atlas-476707-k8.iam.gserviceaccount.com\n",
    "\n",
    "from google.oauth2.service_account import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "sheet_id = '1ipwIl7fciIlddvOUqGLpNlVQufw7Xd26Qa-YuJcx-xE'\n",
    "\n",
    "SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "\n",
    "# Prefer env var GOOGLE_APPLICATION_CREDENTIALS, else fall back to local credentials.json\n",
    "SERVICE_ACCOUNT_FILE = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\", \"credentials.json\")\n",
    "if not os.path.exists(SERVICE_ACCOUNT_FILE):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Service account file not found at '{SERVICE_ACCOUNT_FILE}'. \"\n",
    "        \"Set GOOGLE_APPLICATION_CREDENTIALS to the full path, or place credentials.json next to this notebook.\"\n",
    "    )\n",
    "\n",
    "credentials = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "service = build('sheets', 'v4', credentials=credentials)\n",
    "sheet = service.spreadsheets()\n",
    "\n",
    "\n",
    "# Using a large range to ensure we get all data (sheets typically don't exceed 1000 columns)\n",
    "range_a1 = \"'MASTER DATABASE 2025 Template'!A:ZZ\"\n",
    "result = sheet.values().get(spreadsheetId=sheet_id, range=range_a1).execute()\n",
    "values = result.get('values', [])\n",
    "\n",
    "if values:\n",
    "    header = values[0]\n",
    "    data_rows = values[1:]\n",
    "    \n",
    "    # Find the maximum number of columns across all rows\n",
    "    max_len = max([len(header)] + [len(r) for r in data_rows]) if data_rows else len(header)\n",
    "    \n",
    "    # Extend header if data rows have more columns\n",
    "    if len(header) < max_len:\n",
    "        header = header + [f'col_{i+1}' for i in range(len(header), max_len)]\n",
    "    \n",
    "    # Normalize all rows to have the same length\n",
    "    normalized_rows = [row + [''] * (max_len - len(row)) for row in data_rows]\n",
    "    \n",
    "    Master_DB_df = pd.DataFrame(normalized_rows, columns=header)\n",
    "    print(f\"Successfully loaded {len(Master_DB_df)} rows and {len(Master_DB_df.columns)} columns\")\n",
    "else:\n",
    "    Master_DB_df = pd.DataFrame()\n",
    "    print(\"No data found in the sheet\")\n",
    "\n",
    "# --- CLEANING AND STANDARDIZING THE MASTER DB ---\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "def clean_uen(u: str) -> str | None:\n",
    "    \"\"\"Clean UEN: remove non-alphanumeric, convert to uppercase.\"\"\"\n",
    "    if pd.isna(u) or u == '':\n",
    "        return None\n",
    "    cleaned = re.sub(r\"[^A-Z0-9]\", \"\", str(u).upper().strip())\n",
    "    return None if cleaned == '' else cleaned\n",
    "\n",
    "def clean_text(text: str) -> str | None:\n",
    "    \"\"\"Clean text: strip, uppercase, handle NaN values.\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return None\n",
    "    text = str(text).strip().upper()\n",
    "    return None if text in ('', 'NAN', 'NONE') else text\n",
    "\n",
    "def clean_ssic_code(value) -> int | None:\n",
    "    \"\"\"Convert SSIC code to integer, handling empty strings and invalid values.\"\"\"\n",
    "    if pd.isna(value) or value == '':\n",
    "        return None\n",
    "    try:\n",
    "        # Remove any non-numeric characters and convert\n",
    "        cleaned = re.sub(r\"[^0-9]\", \"\", str(value).strip())\n",
    "        return int(cleaned) if cleaned else None\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert column names to uppercase, replace non-alphanumeric with single underscore.\"\"\"\n",
    "    new_cols = [\n",
    "        re.sub(r\"_+\", \"_\", re.sub(r\"[^A-Z0-9]\", \"_\", col.upper().strip())).strip(\"_\")\n",
    "        for col in df.columns\n",
    "    ]\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "# --- PROCESS DATA ---\n",
    "# Select relevant columns first (more efficient than copying entire dataframe)\n",
    "columns_to_keep = [\n",
    "    \"Company Registration Number (UEN)\",\n",
    "    \"ACRA REGISTERED NAME\",\n",
    "    \"Brand/Deal Name/Business Name\",\n",
    "    \"Primary SSIC Code\",\n",
    "    \"PIC NAME 1 Contact Number\",\n",
    "    \"PIC 1 email address\",\n",
    "    \"Website URL\",\n",
    "    \"Parent Industry Type\",\n",
    "    \"Sub Industry\"\n",
    "]\n",
    "\n",
    "# Filter columns that exist in the dataframe\n",
    "existing_cols = [c for c in columns_to_keep if c in Master_DB_df.columns]\n",
    "if not existing_cols:\n",
    "    raise ValueError(\"None of the required columns found in the dataframe\")\n",
    "\n",
    "master_db_df = Master_DB_df[existing_cols].copy()\n",
    "\n",
    "# Standardize column names\n",
    "master_db_df = standardize_columns(master_db_df)\n",
    "\n",
    "# Find and process UEN column\n",
    "uen_cols = [c for c in master_db_df.columns if \"UEN\" in c]\n",
    "if not uen_cols:\n",
    "    raise ValueError(\"UEN column not found after standardization\")\n",
    "uen_col = uen_cols[0]\n",
    "\n",
    "# Clean UEN using vectorized operations (faster than apply)\n",
    "master_db_df[\"UEN\"] = master_db_df[uen_col].astype(str).str.upper().str.replace(r\"[^A-Z0-9]\", \"\", regex=True)\n",
    "master_db_df[\"UEN\"] = master_db_df[\"UEN\"].replace(['', 'NAN', 'NONE'], None)\n",
    "master_db_df = master_db_df.drop(columns=[uen_col])\n",
    "\n",
    "# Rename columns\n",
    "rename_map = {\n",
    "    \"BRAND_DEAL_NAME_BUSINESS_NAME\": \"BRAND_NAME\",\n",
    "    \"PRIMARY_SSIC_CODE\": \"SSIC_CODE\",\n",
    "}\n",
    "master_db_df = master_db_df.rename(columns={k: v for k, v in rename_map.items() if k in master_db_df.columns})\n",
    "\n",
    "# Clean text columns using vectorized operations\n",
    "for col in [\"ACRA_REGISTERED_NAME\", \"BRAND_NAME\"]:\n",
    "    if col in master_db_df.columns:\n",
    "        master_db_df[col] = (\n",
    "            master_db_df[col].astype(str).str.strip().str.upper()\n",
    "            .replace(['', 'NAN', 'NONE'], None)\n",
    "        )\n",
    "\n",
    "# Convert SSIC_CODE to integer (handles empty strings and invalid values)\n",
    "if \"SSIC_CODE\" in master_db_df.columns:\n",
    "    master_db_df[\"SSIC_CODE\"] = master_db_df[\"SSIC_CODE\"].apply(clean_ssic_code)\n",
    "\n",
    "# Keep only required columns that exist\n",
    "required_cols = [\"UEN\", \"ACRA_REGISTERED_NAME\", \"BRAND_NAME\", \"SSIC_CODE\"]\n",
    "available_cols = [c for c in required_cols if c in master_db_df.columns]\n",
    "master_db_df = master_db_df[available_cols].copy()\n",
    "\n",
    "# Filter out rows with missing or empty UEN\n",
    "master_db_df = master_db_df[\n",
    "    master_db_df[\"UEN\"].notna() & \n",
    "    (master_db_df[\"UEN\"].astype(str).str.strip() != \"\")\n",
    "].copy()\n",
    "\n",
    "print(f\"Final dataset: {len(master_db_df)} rows, {len(master_db_df.columns)} columns\")\n",
    "master_db_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b75d1db",
   "metadata": {},
   "source": [
    "### Getting ACRA Data (Filter by Live, Live Company only & non relevant ssic code)\n",
    "- last downloaded oct 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3b6d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = \"Acra_Data\"\n",
    "\n",
    "# Get all CSV file paths inside the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "# Read and combine all CSVs\n",
    "# Using low_memory=False to avoid DtypeWarning for mixed types\n",
    "df = pd.concat((pd.read_csv(f, low_memory=False) for f in csv_files), ignore_index=True)\n",
    "\n",
    "\n",
    "df.columns = df.columns.str.upper()\n",
    "\n",
    "\n",
    "acra_data = df[[\n",
    "    \"UEN\",\n",
    "    \"ENTITY_NAME\",\n",
    "    \"BUSINESS_CONSTITUTION_DESCRIPTION\",\n",
    "    \"ENTITY_TYPE_DESCRIPTION\",\n",
    "    \"ENTITY_STATUS_DESCRIPTION\",\n",
    "    \"REGISTRATION_INCORPORATION_DATE\",\n",
    "    \"PRIMARY_SSIC_CODE\",\n",
    "    \"SECONDARY_SSIC_CODE\",\n",
    "    \"STREET_NAME\",\n",
    "    \"POSTAL_CODE\"\n",
    "]].copy()\n",
    "\n",
    "# Convert to proper data types\n",
    "acra_data['UEN'] = acra_data['UEN'].astype('string')\n",
    "acra_data['ENTITY_NAME'] = acra_data['ENTITY_NAME'].astype('string')\n",
    "acra_data['BUSINESS_CONSTITUTION_DESCRIPTION'] = acra_data['BUSINESS_CONSTITUTION_DESCRIPTION'].astype('string')\n",
    "acra_data['ENTITY_TYPE_DESCRIPTION'] = acra_data['ENTITY_TYPE_DESCRIPTION'].astype('string')\n",
    "acra_data['ENTITY_STATUS_DESCRIPTION'] = acra_data['ENTITY_STATUS_DESCRIPTION'].astype('string')\n",
    "acra_data['REGISTRATION_INCORPORATION_DATE'] = pd.to_datetime(acra_data['REGISTRATION_INCORPORATION_DATE'], errors='coerce')\n",
    "\n",
    "# Clean string columns â€” trim, remove extra spaces, uppercase\n",
    "for col in [\n",
    "    'UEN',\n",
    "    'ENTITY_NAME',\n",
    "    'BUSINESS_CONSTITUTION_DESCRIPTION',\n",
    "    'ENTITY_TYPE_DESCRIPTION',\n",
    "    'ENTITY_STATUS_DESCRIPTION',\n",
    "    'STREET_NAME',\n",
    "    'POSTAL_CODE'\n",
    "]:\n",
    "    acra_data[col] = (\n",
    "        acra_data[col]\n",
    "        .fillna('')\n",
    "        .str.strip()\n",
    "        .str.replace(r'\\s+', ' ', regex=True)\n",
    "        .str.upper()\n",
    "    )\n",
    "\n",
    "# Replace placeholders with NaN for standardization\n",
    "acra_data.replace(['NA', 'N/A', '-', ''], np.nan, inplace=True)\n",
    "\n",
    "# Convert registration date to dd-mm-yyyy string (optional)\n",
    "acra_data['REGISTRATION_INCORPORATION_DATE'] = acra_data['REGISTRATION_INCORPORATION_DATE'].dt.strftime('%d-%m-%Y')\n",
    "\n",
    "# Filter only live entities (LIVE COMPANY or LIVE)\n",
    "acra_data = acra_data[\n",
    "    acra_data['ENTITY_STATUS_DESCRIPTION'].isin(['LIVE COMPANY', 'LIVE'])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Exclude specific PRIMARY_SSIC_CODE values (supposedly the data would be 600k plus but when we exclude this would lessen)\n",
    "exclude_codes = [\n",
    "    46900, 47719, 47749, 47539, 47536, 56123,\n",
    "    10711, 10712, 10719, 10732, 10733, 93209\n",
    "]\n",
    "\n",
    "acra_data = acra_data[~acra_data['PRIMARY_SSIC_CODE'].isin(exclude_codes)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c868b6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "acra_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a0b909",
   "metadata": {},
   "source": [
    "### Getting SSIC Industry code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9385c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIG ---\n",
    "file_path = \"./SSIC_Code/mapped_ssic_code.xlsx\"\n",
    "\n",
    "# --- LOAD DATA ---\n",
    "mapped_ssic_code = pd.read_excel(file_path)\n",
    "\n",
    "# --- STANDARDIZE COLUMN NAMES ---\n",
    "# Uppercase, strip spaces, replace spaces with underscores\n",
    "mapped_ssic_code.columns = (\n",
    "    mapped_ssic_code.columns\n",
    "    .str.strip()\n",
    "    .str.upper()\n",
    "    .str.replace(\" \", \"_\")\n",
    ")\n",
    "\n",
    "# --- KEEP ONLY DESIRED COLUMNS ---\n",
    "columns_to_keep = [\"PARENT_INDUSTRY\", \"INDUSTRY_TYPE\", \"SUB_INDUSTRY\", \"SSIC_CODES\", \"DESCRIPTION\"]\n",
    "mapped_ssic_code = mapped_ssic_code[columns_to_keep].copy()\n",
    "\n",
    "# --- CLEAN SSIC_CODES COLUMN ---\n",
    "mapped_ssic_code[\"SSIC_CODES\"] = (\n",
    "    pd.to_numeric(mapped_ssic_code[\"SSIC_CODES\"], errors=\"coerce\")  # safely convert to numeric\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# --- CLEAN TEXT COLUMNS ---\n",
    "text_cols = [\"PARENT_INDUSTRY\", \"INDUSTRY_TYPE\", \"SUB_INDUSTRY\", \"DESCRIPTION\"]\n",
    "mapped_ssic_code[text_cols] = mapped_ssic_code[text_cols].apply(\n",
    "    lambda col: col.astype(str).str.strip().str.title()\n",
    ")\n",
    "\n",
    "# --- REMOVE DUPLICATES & RESET INDEX ---\n",
    "mapped_ssic_code = mapped_ssic_code.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "mapped_ssic_code.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ade2b",
   "metadata": {},
   "source": [
    "### Merge ACRA data with SSIC code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a45182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PRIMARY_SSIC_CODE to int\n",
    "acra_data[\"PRIMARY_SSIC_CODE\"] = (\n",
    "    pd.to_numeric(acra_data[\"PRIMARY_SSIC_CODE\"], errors=\"coerce\")\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Merge based on SSIC code\n",
    "acra_data_filtered = acra_data.merge(\n",
    "    mapped_ssic_code,\n",
    "    how=\"left\",\n",
    "    left_on=\"PRIMARY_SSIC_CODE\",\n",
    "    right_on=\"SSIC_CODES\"\n",
    ")\n",
    "\n",
    "# Optional: drop the duplicate 'SSIC CODES' column (keep only PRIMARY_SSIC_CODE)\n",
    "acra_data_filtered = acra_data_filtered.drop(columns=[\"SSIC_CODES\"], errors=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd083f3",
   "metadata": {},
   "source": [
    "### FIlter Acra data with Master DB to get list of companies havent been researched  by MR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure both UEN columns are strings for accurate matching\n",
    "acra_data_filtered['UEN'] = acra_data_filtered['UEN'].astype(str).str.strip().str.upper()\n",
    "master_db_df['UEN'] = master_db_df['UEN'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Filter out rows in acra_data_filtered whose UEN is already in master_db_df\n",
    "acra_data_filtered = acra_data_filtered[~acra_data_filtered['UEN'].isin(master_db_df['UEN'])]\n",
    "\n",
    "acra_data_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb93431",
   "metadata": {},
   "source": [
    "### Filter by  Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad39e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuition, child care and education training\n",
    "\n",
    "ssic_codes = [\n",
    "    \"85332\", \"8536\", \"85360\", \"85403\", \"85404\",\n",
    "    \"855\", \"8550\", \"85501\", \"85502\", \"85503\", \"85504\", \"85505\", \"85506\", \"85507\", \"85508\", \"85509\",\n",
    "    \"856\", \"8560\", \"85601\", \"85602\", \"85609\",\n",
    "    \"87022\", \"8891\", \"88911\", \"88912\", \"88991\",\n",
    "    \"96094\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "acra_data_filtered_by_industry = acra_data_filtered[\n",
    "    (\n",
    "        (acra_data_filtered[\"ENTITY_STATUS_DESCRIPTION\"].str.lower() == \"live\") |\n",
    "        (acra_data_filtered[\"ENTITY_STATUS_DESCRIPTION\"].str.lower() == \"live company\")\n",
    "    )\n",
    "    &\n",
    "    (acra_data_filtered[\"PRIMARY_SSIC_CODE\"].astype(str).isin(ssic_codes))\n",
    "]\n",
    "\n",
    "\n",
    "acra_data_filtered_by_industry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f51ce53",
   "metadata": {},
   "source": [
    "### Filter with existing Fresh Leads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141a7455",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter with existing Fresh Leads\n",
    "\n",
    "# --- UPDATE HERE: Remove rows if UEN exists in recordowl_results.xlsx ---\n",
    "Fresh_Leads = pd.read_excel(\"./Golden_Data/Fresh_Leads_Nov11_Latest.xlsx\")\n",
    "\n",
    "# Select only the required columns\n",
    "columns_to_select = [\n",
    "    \"Company Registration Number (UEN)\",\n",
    "    \"ACRA REGISTERED NAME\",\n",
    "    \"Brand/Deal Name/Business Name\"\n",
    "]\n",
    "\n",
    "# Check which columns exist and select them\n",
    "existing_columns = [col for col in columns_to_select if col in Fresh_Leads.columns]\n",
    "\n",
    "if len(existing_columns) != len(columns_to_select):\n",
    "    missing = [col for col in columns_to_select if col not in Fresh_Leads.columns]\n",
    "    print(f\"Warning: Missing columns: {missing}\")\n",
    "    print(f\"Available columns: {list(Fresh_Leads.columns)}\")\n",
    "\n",
    "Fresh_Leads = Fresh_Leads[existing_columns]\n",
    "\n",
    "if \"Company Registration Number (UEN)\" in Fresh_Leads.columns and \"UEN\" in acra_data_filtered_by_industry.columns:\n",
    "    filtered = acra_data_filtered_by_industry[~acra_data_filtered_by_industry[\"UEN\"].isin(Fresh_Leads[\"Company Registration Number (UEN)\"])]\n",
    "else:\n",
    "    raise ValueError(\"Column 'UEN' not found in one of the dataframes.\")\n",
    "\n",
    "# get sample data \n",
    "acra_data_filtered_by_industry = filtered.sample(n=10).reset_index(drop=True)\n",
    "\n",
    "acra_data_filtered_by_industry.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7baa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "acra_data_filtered_by_industry.to_parquet(\"./Staging/Bronze/bronze_data_1.parquet\", index=False, engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2474478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
