{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "982b7eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import time\n",
    "import scrapy\n",
    "from scrapy_playwright.page import PageMethod\n",
    "from bs4 import BeautifulSoup\n",
    "import nest_asyncio\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "380182f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fresh_Leads_formatted = pd.read_csv(\"Golden_Data/Fresh_Leads_with_PhoneNumber_Nov11.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d5ad28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing duplicates:\n",
      "  Total rows: 105\n",
      "  Rows with Website URL: 64\n",
      "  Unique URLs: 64\n",
      "  Duplicate URLs: 0\n",
      "\n",
      "After removing duplicates:\n",
      "  Rows with Website URL: 64\n",
      "  Unique URLs: 64\n",
      "  Duplicates removed: 0\n",
      "  Rows with duplicate URLs set to NaN: 0\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate Website URLs - keep first occurrence, set duplicates to NaN\n",
    "if 'Website URL' in Fresh_Leads_formatted.columns:\n",
    "    # Count duplicates before removal\n",
    "    total_rows = len(Fresh_Leads_formatted)\n",
    "    valid_urls = Fresh_Leads_formatted['Website URL'].notna()\n",
    "    unique_urls_before = Fresh_Leads_formatted[valid_urls]['Website URL'].nunique()\n",
    "    duplicate_count = Fresh_Leads_formatted[valid_urls]['Website URL'].duplicated().sum()\n",
    "    \n",
    "    print(f\"Before removing duplicates:\")\n",
    "    print(f\"  Total rows: {total_rows}\")\n",
    "    print(f\"  Rows with Website URL: {valid_urls.sum()}\")\n",
    "    print(f\"  Unique URLs: {unique_urls_before}\")\n",
    "    print(f\"  Duplicate URLs: {duplicate_count}\")\n",
    "    \n",
    "    # Mark duplicates (keep first occurrence, mark subsequent as duplicates)\n",
    "    # Convert to string and normalize (lowercase, strip whitespace) for comparison\n",
    "    Fresh_Leads_formatted['Website URL_cleaned'] = Fresh_Leads_formatted['Website URL'].astype(str).str.strip().str.lower()\n",
    "    \n",
    "    # Find duplicates (keep first occurrence)\n",
    "    is_duplicate = Fresh_Leads_formatted['Website URL_cleaned'].duplicated(keep='first')\n",
    "    \n",
    "    # Set duplicate URLs to NaN (excluding rows where URL was already NaN)\n",
    "    mask_to_remove = is_duplicate & (Fresh_Leads_formatted['Website URL'].notna())\n",
    "    Fresh_Leads_formatted.loc[mask_to_remove, 'Website URL'] = pd.NA\n",
    "    \n",
    "    # Drop the temporary cleaning column\n",
    "    Fresh_Leads_formatted = Fresh_Leads_formatted.drop(columns=['Website URL_cleaned'])\n",
    "    \n",
    "    # Count after removal\n",
    "    valid_urls_after = Fresh_Leads_formatted['Website URL'].notna()\n",
    "    unique_urls_after = Fresh_Leads_formatted[valid_urls_after]['Website URL'].nunique()\n",
    "    \n",
    "    print(f\"\\nAfter removing duplicates:\")\n",
    "    print(f\"  Rows with Website URL: {valid_urls_after.sum()}\")\n",
    "    print(f\"  Unique URLs: {unique_urls_after}\")\n",
    "    print(f\"  Duplicates removed: {duplicate_count}\")\n",
    "    print(f\"  Rows with duplicate URLs set to NaN: {mask_to_remove.sum()}\")\n",
    "else:\n",
    "    print(\"Warning: 'Website URL' column not found in dataframe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27b04023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing: Facebook Page\n",
      "============================================================\n",
      "Before removing duplicates:\n",
      "  Rows with Facebook Page: 32\n",
      "  Unique values: 32\n",
      "  Duplicate values: 0\n",
      "\n",
      "After removing duplicates:\n",
      "  Rows with Facebook Page: 32\n",
      "  Unique values: 32\n",
      "  Duplicates removed: 0\n",
      "  ✅ Successfully removed 0 duplicate values\n",
      "\n",
      "============================================================\n",
      "Processing: Instagram URL\n",
      "============================================================\n",
      "Before removing duplicates:\n",
      "  Rows with Instagram URL: 22\n",
      "  Unique values: 22\n",
      "  Duplicate values: 0\n",
      "\n",
      "After removing duplicates:\n",
      "  Rows with Instagram URL: 22\n",
      "  Unique values: 22\n",
      "  Duplicates removed: 0\n",
      "  ✅ Successfully removed 0 duplicate values\n",
      "\n",
      "============================================================\n",
      "Processing: PIC 1 email address\n",
      "============================================================\n",
      "Before removing duplicates:\n",
      "  Rows with PIC 1 email address: 47\n",
      "  Unique values: 42\n",
      "  Duplicate values: 5\n",
      "\n",
      "After removing duplicates:\n",
      "  Rows with PIC 1 email address: 42\n",
      "  Unique values: 42\n",
      "  Duplicates removed: 5\n",
      "  ✅ Successfully removed 5 duplicate values\n",
      "\n",
      "============================================================\n",
      "Duplicate removal complete for Facebook Page, Instagram URL, and PIC 1 email address!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate Facebook Page, Instagram URL, and PIC 1 email address - keep first occurrence, set duplicates to NaN\n",
    "columns_to_clean = ['Facebook Page', 'Instagram URL', 'PIC 1 email address']\n",
    "\n",
    "for col_name in columns_to_clean:\n",
    "    if col_name in Fresh_Leads_formatted.columns:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing: {col_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Count duplicates before removal\n",
    "        valid_values = Fresh_Leads_formatted[col_name].notna()\n",
    "        unique_before = Fresh_Leads_formatted[valid_values][col_name].nunique()\n",
    "        duplicate_count = Fresh_Leads_formatted[valid_values][col_name].duplicated().sum()\n",
    "        \n",
    "        print(f\"Before removing duplicates:\")\n",
    "        print(f\"  Rows with {col_name}: {valid_values.sum()}\")\n",
    "        print(f\"  Unique values: {unique_before}\")\n",
    "        print(f\"  Duplicate values: {duplicate_count}\")\n",
    "        \n",
    "        # Normalize values for comparison (lowercase, strip whitespace)\n",
    "        temp_col = f'{col_name}_cleaned'\n",
    "        Fresh_Leads_formatted[temp_col] = Fresh_Leads_formatted[col_name].astype(str).str.strip().str.lower()\n",
    "        \n",
    "        # Find duplicates (keep first occurrence)\n",
    "        is_duplicate = Fresh_Leads_formatted[temp_col].duplicated(keep='first')\n",
    "        \n",
    "        # Set duplicate values to NaN (excluding rows where value was already NaN)\n",
    "        mask_to_remove = is_duplicate & (Fresh_Leads_formatted[col_name].notna())\n",
    "        Fresh_Leads_formatted.loc[mask_to_remove, col_name] = pd.NA\n",
    "        \n",
    "        # Drop the temporary cleaning column\n",
    "        Fresh_Leads_formatted = Fresh_Leads_formatted.drop(columns=[temp_col])\n",
    "        \n",
    "        # Count after removal\n",
    "        valid_after = Fresh_Leads_formatted[col_name].notna()\n",
    "        unique_after = Fresh_Leads_formatted[valid_after][col_name].nunique()\n",
    "        \n",
    "        print(f\"\\nAfter removing duplicates:\")\n",
    "        print(f\"  Rows with {col_name}: {valid_after.sum()}\")\n",
    "        print(f\"  Unique values: {unique_after}\")\n",
    "        print(f\"  Duplicates removed: {mask_to_remove.sum()}\")\n",
    "        print(f\"  ✅ Successfully removed {mask_to_remove.sum()} duplicate values\")\n",
    "    else:\n",
    "        print(f\"⚠️  Warning: '{col_name}' column not found in dataframe\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Duplicate removal complete for Facebook Page, Instagram URL, and PIC 1 email address!\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "164c7a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ePOS Code',\n",
       " 'Company Code',\n",
       " 'Date',\n",
       " 'ACRA REGISTERED NAME',\n",
       " 'Brand/Deal Name/Business Name',\n",
       " 'Sub Domain Link (If Lead is already available in Backend) Fill only when EPOS client',\n",
       " 'Tele Sales or MR (For KPI - Internal)',\n",
       " 'Name of the Market Researcher',\n",
       " 'Original Source (Marketing)',\n",
       " 'Marketing Source (Do not fill anything if the leads are from Hubspot, EPOS clients)',\n",
       " 'Company Registration date / Date Established',\n",
       " 'Company Registration Number (UEN)',\n",
       " 'Primary SSIC Code',\n",
       " 'Secondary SSIC Code',\n",
       " 'Hubspot ID (Company)',\n",
       " 'Hubspot ID(Deal)',\n",
       " 'Hubspot ID(Contact)',\n",
       " 'Website URL',\n",
       " 'Business Type',\n",
       " 'Facebook Page',\n",
       " 'Instagram URL',\n",
       " 'Linkedin URL',\n",
       " 'Tik Tok URL',\n",
       " 'Ownership Type',\n",
       " 'Parent Industry Type',\n",
       " 'Industry Type',\n",
       " 'Sub Industry',\n",
       " 'Business model',\n",
       " 'Presence of Multiple Outlets',\n",
       " 'Number of Outlets (Write in #)',\n",
       " 'Region',\n",
       " 'Planning Area',\n",
       " 'Business Location Type',\n",
       " 'Registered Address (Block & Street)',\n",
       " 'Registered Address  (Unit #)',\n",
       " 'Registered Address  (Postal code)',\n",
       " 'Operational Address \\n(Block & Street)',\n",
       " 'Operational Address \\n(Unit #)',\n",
       " 'Operational Address \\n(Postal Code)',\n",
       " 'Operational Address Type',\n",
       " 'First Name',\n",
       " 'Last Name',\n",
       " 'PIC Name 1 Designation',\n",
       " 'PIC NAME 1 Contact Number',\n",
       " 'PIC 1 email address',\n",
       " 'First Name 2',\n",
       " 'Last Name 2',\n",
       " 'PIC Name 2 Designation',\n",
       " 'PIC NAME 2 Contact Number',\n",
       " 'PIC 2 email address',\n",
       " 'First Name 3',\n",
       " 'Last Name 3',\n",
       " 'PIC Name Designation 3',\n",
       " 'PIC NAME 3 Contact Number',\n",
       " 'PIC 3 email address',\n",
       " 'FB/Insta/Tik Tok/Linkedin Contact',\n",
       " 'Current ePOS Client ?',\n",
       " 'If ePOS Client, which product they are using?',\n",
       " 'Is this deal part of the Gov List?',\n",
       " 'Source from Market Researcher',\n",
       " 'Contact Number from Lusha?',\n",
       " 'Phone number Verified ?']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fresh_Leads_formatted.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe35ada2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGGRESSIVELY cleaning: 'Operational Address \n",
      "(Unit #)'\n",
      "\n",
      "Removing 0 date values...\n",
      "\n",
      "✅ Done! Removed 0 dates.\n",
      "Remaining non-null values: 96\n",
      "\n",
      "Sample of remaining values:\n",
      "   Operational Address \\n(Unit #)\n",
      "0                               4\n",
      "1                               8\n",
      "2                             590\n",
      "3                               1\n",
      "4                               3\n",
      "5                              76\n",
      "6                             02A\n",
      "7                             309\n",
      "8                               7\n",
      "9                               4\n",
      "10                            103\n",
      "11                            607\n",
      "12                             31\n",
      "13                              1\n",
      "14                              0\n"
     ]
    }
   ],
   "source": [
    "# AGGRESSIVE DATE REMOVAL for Operational Address (Unit #) - Remove ALL dates!\n",
    "# Find the column\n",
    "op_col = None\n",
    "for col in Fresh_Leads_formatted.columns:\n",
    "    if 'operational' in col.lower() and 'unit' in col.lower() and 'address' in col.lower():\n",
    "        op_col = col\n",
    "        break\n",
    "\n",
    "if op_col:\n",
    "    print(f\"AGGRESSIVELY cleaning: '{op_col}'\\n\")\n",
    "    s = Fresh_Leads_formatted[op_col].astype(str)\n",
    "    to_remove = []\n",
    "    month_names = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'sept', 'oct', 'nov', 'dec']\n",
    "    \n",
    "    for idx, val in s.items():\n",
    "        if pd.isna(val) or str(val).strip().lower() in ['nan', 'none', '', 'nat']:\n",
    "            continue\n",
    "        val_str = str(val).strip()\n",
    "        val_lower = val_str.lower()\n",
    "        \n",
    "        # KEEP only if it has a letter (02A, 12W, 330G) - these are unit numbers\n",
    "        if re.search(r'[A-Za-z]', val_str):\n",
    "            continue  # Has letter = unit number, KEEP IT\n",
    "        \n",
    "        # No letters - check if it's a date and REMOVE\n",
    "        is_date = False\n",
    "        \n",
    "        # Check for month names\n",
    "        for month in month_names:\n",
    "            if month in val_lower:\n",
    "                is_date = True\n",
    "                break\n",
    "        \n",
    "        # Check date patterns\n",
    "        if re.search(r'\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}', val_str):\n",
    "            is_date = True\n",
    "        elif re.search(r'\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}', val_str):\n",
    "            is_date = True\n",
    "        elif re.match(r'^\\d{1,2}[-/]\\d{1,2}$', val_str):\n",
    "            # Could be MM/DD - check if ranges suggest date\n",
    "            parts = re.split(r'[-/]', val_str)\n",
    "            if len(parts) == 2:\n",
    "                try:\n",
    "                    p1, p2 = int(parts[0]), int(parts[1])\n",
    "                    if (1 <= p1 <= 31) and (1 <= p2 <= 12):\n",
    "                        is_date = True\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Try pandas date parsing\n",
    "        if not is_date:\n",
    "            try:\n",
    "                pd.to_datetime(val_str, errors='raise')\n",
    "                is_date = True\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if is_date:\n",
    "            to_remove.append(idx)\n",
    "    \n",
    "    # Remove all dates\n",
    "    print(f\"Removing {len(to_remove)} date values...\")\n",
    "    if to_remove:\n",
    "        print(\"Examples of dates being removed:\")\n",
    "        for idx in to_remove[:15]:\n",
    "            print(f\"  Row {idx}: {s.loc[idx]}\")\n",
    "        if len(to_remove) > 15:\n",
    "            print(f\"  ... and {len(to_remove) - 15} more\")\n",
    "    \n",
    "    Fresh_Leads_formatted.loc[to_remove, op_col] = pd.NA\n",
    "    \n",
    "    print(f\"\\n✅ Done! Removed {len(to_remove)} dates.\")\n",
    "    print(f\"Remaining non-null values: {Fresh_Leads_formatted[op_col].notna().sum()}\")\n",
    "    print(\"\\nSample of remaining values:\")\n",
    "    print(Fresh_Leads_formatted[Fresh_Leads_formatted[op_col].notna()][[op_col]].head(15))\n",
    "else:\n",
    "    print(\"Column not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "56b585af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fresh_Leads_formatted.to_csv(\"Golden_Data/Fresh_Leads_with_PhoneNumber_Nov11.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
