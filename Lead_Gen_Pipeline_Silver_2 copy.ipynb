{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f1d1a2",
   "metadata": {},
   "source": [
    "# Data Mining in RecordOwl (Silver 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b601322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import time\n",
    "import scrapy\n",
    "from scrapy_playwright.page import PageMethod\n",
    "from bs4 import BeautifulSoup\n",
    "import nest_asyncio\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from fuzzywuzzy import fuzz, process\n",
    "import re\n",
    "import random\n",
    "import textwrap\n",
    "from copy import deepcopy\n",
    "from apify_client import ApifyClient\n",
    "from urllib.error import HTTPError\n",
    "from requests.exceptions import ConnectionError, RequestException\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c77d5d",
   "metadata": {},
   "source": [
    "### Ingesting from previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86bf8e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 rows from ./Staging/Bronze/bronze_data_1.parquet\n",
      "(10, 14)\n"
     ]
    }
   ],
   "source": [
    "parquet_path = \"./Staging/Bronze/bronze_data_1.parquet\"\n",
    "if os.path.exists(parquet_path):\n",
    "    acra_data_filtered_by_industry = pd.read_parquet(parquet_path, engine=\"fastparquet\")\n",
    "    print(f\"Loaded {len(acra_data_filtered_by_industry)} rows from {parquet_path}\")\n",
    "    print(acra_data_filtered_by_industry.shape)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Parquet file not found at {parquet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6348882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UEN</th>\n",
       "      <th>ENTITY_NAME</th>\n",
       "      <th>BUSINESS_CONSTITUTION_DESCRIPTION</th>\n",
       "      <th>ENTITY_TYPE_DESCRIPTION</th>\n",
       "      <th>ENTITY_STATUS_DESCRIPTION</th>\n",
       "      <th>REGISTRATION_INCORPORATION_DATE</th>\n",
       "      <th>PRIMARY_SSIC_CODE</th>\n",
       "      <th>SECONDARY_SSIC_CODE</th>\n",
       "      <th>STREET_NAME</th>\n",
       "      <th>POSTAL_CODE</th>\n",
       "      <th>PARENT_INDUSTRY</th>\n",
       "      <th>INDUSTRY_TYPE</th>\n",
       "      <th>SUB_INDUSTRY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53431824W</td>\n",
       "      <td>TUTORSVILLE.SG</td>\n",
       "      <td>SOLE-PROPRIETOR</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>07-04-2021</td>\n",
       "      <td>85509</td>\n",
       "      <td>na</td>\n",
       "      <td>COMPASSVALE WALK</td>\n",
       "      <td>540230</td>\n",
       "      <td>Others</td>\n",
       "      <td>Educational</td>\n",
       "      <td>Tuition &amp; Enrichment Centers</td>\n",
       "      <td>Training Courses N.E.C.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202344030R</td>\n",
       "      <td>CHEM AFFINITY LEARNING CENTRE PTE. LTD.</td>\n",
       "      <td>None</td>\n",
       "      <td>LOCAL COMPANY</td>\n",
       "      <td>LIVE COMPANY</td>\n",
       "      <td>04-11-2023</td>\n",
       "      <td>85509</td>\n",
       "      <td>na</td>\n",
       "      <td>BEACH ROAD</td>\n",
       "      <td>189695</td>\n",
       "      <td>Others</td>\n",
       "      <td>Educational</td>\n",
       "      <td>Tuition &amp; Enrichment Centers</td>\n",
       "      <td>Training Courses N.E.C.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T15LL1885G</td>\n",
       "      <td>EDUREACH SERVICES LLP</td>\n",
       "      <td>None</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>11-11-2015</td>\n",
       "      <td>85509</td>\n",
       "      <td>74901</td>\n",
       "      <td>TAMPINES STREET 23</td>\n",
       "      <td>527201</td>\n",
       "      <td>Others</td>\n",
       "      <td>Educational</td>\n",
       "      <td>Tuition &amp; Enrichment Centers</td>\n",
       "      <td>Training Courses N.E.C.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53200915X</td>\n",
       "      <td>THINK ARTS</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>06-10-2011</td>\n",
       "      <td>85509</td>\n",
       "      <td>na</td>\n",
       "      <td>YARROW GARDENS</td>\n",
       "      <td>455021</td>\n",
       "      <td>Others</td>\n",
       "      <td>Educational</td>\n",
       "      <td>Tuition &amp; Enrichment Centers</td>\n",
       "      <td>Training Courses N.E.C.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201733719E</td>\n",
       "      <td>JUS INFANTS @ MACPHERSON PTE. LTD.</td>\n",
       "      <td>None</td>\n",
       "      <td>LOCAL COMPANY</td>\n",
       "      <td>LIVE COMPANY</td>\n",
       "      <td>22-11-2017</td>\n",
       "      <td>88911</td>\n",
       "      <td>na</td>\n",
       "      <td>KALLANG PUDDING ROAD</td>\n",
       "      <td>349318</td>\n",
       "      <td>Others</td>\n",
       "      <td>Hospital</td>\n",
       "      <td>Social Services (Without Accommodations)</td>\n",
       "      <td>Infant Care Services; Child Minding Services F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>53227394W</td>\n",
       "      <td>MATHS TABLET</td>\n",
       "      <td>SOLE-PROPRIETOR</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>04-12-2012</td>\n",
       "      <td>85509</td>\n",
       "      <td>na</td>\n",
       "      <td>ANG MO KIO AVENUE 10</td>\n",
       "      <td>560555</td>\n",
       "      <td>Others</td>\n",
       "      <td>Educational</td>\n",
       "      <td>Tuition &amp; Enrichment Centers</td>\n",
       "      <td>Training Courses N.E.C.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>202209857Z</td>\n",
       "      <td>YORK EDUCATION PTE. LTD.</td>\n",
       "      <td>None</td>\n",
       "      <td>LOCAL COMPANY</td>\n",
       "      <td>LIVE COMPANY</td>\n",
       "      <td>22-03-2022</td>\n",
       "      <td>88911</td>\n",
       "      <td>85101</td>\n",
       "      <td>CASHEW ROAD</td>\n",
       "      <td>679637</td>\n",
       "      <td>Others</td>\n",
       "      <td>Hospital</td>\n",
       "      <td>Social Services (Without Accommodations)</td>\n",
       "      <td>Infant Care Services; Child Minding Services F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>201711911W</td>\n",
       "      <td>MAPLEBEAR LEARNING GARDEN PTE. LTD.</td>\n",
       "      <td>None</td>\n",
       "      <td>LOCAL COMPANY</td>\n",
       "      <td>LIVE COMPANY</td>\n",
       "      <td>01-05-2017</td>\n",
       "      <td>88911</td>\n",
       "      <td>88912</td>\n",
       "      <td>BRADDELL ROAD</td>\n",
       "      <td>579713</td>\n",
       "      <td>Others</td>\n",
       "      <td>Hospital</td>\n",
       "      <td>Social Services (Without Accommodations)</td>\n",
       "      <td>Infant Care Services; Child Minding Services F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>201540131W</td>\n",
       "      <td>4HANDS DENTAL ASSISTING TRAINING SCHOOL PTE. LTD.</td>\n",
       "      <td>None</td>\n",
       "      <td>LOCAL COMPANY</td>\n",
       "      <td>LIVE COMPANY</td>\n",
       "      <td>10-11-2015</td>\n",
       "      <td>88991</td>\n",
       "      <td>na</td>\n",
       "      <td>JURONG WEST STREET 64</td>\n",
       "      <td>641684</td>\n",
       "      <td>Others</td>\n",
       "      <td>Hospital</td>\n",
       "      <td>Social Services (Without Accommodations)</td>\n",
       "      <td>Job Training And Vocational Rehabilitation Ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>202337418G</td>\n",
       "      <td>OUT OF THE BOX ACADEMY (CLEMENTI) PTE. LTD.</td>\n",
       "      <td>None</td>\n",
       "      <td>LOCAL COMPANY</td>\n",
       "      <td>LIVE COMPANY</td>\n",
       "      <td>18-09-2023</td>\n",
       "      <td>88912</td>\n",
       "      <td>85509</td>\n",
       "      <td>CLEMENTI AVENUE 3</td>\n",
       "      <td>120433</td>\n",
       "      <td>Others</td>\n",
       "      <td>Hospital</td>\n",
       "      <td>Social Services (Without Accommodations)</td>\n",
       "      <td>Student Care Services; Child Minding Services ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          UEN                                        ENTITY_NAME  \\\n",
       "0   53431824W                                     TUTORSVILLE.SG   \n",
       "1  202344030R            CHEM AFFINITY LEARNING CENTRE PTE. LTD.   \n",
       "2  T15LL1885G                              EDUREACH SERVICES LLP   \n",
       "3   53200915X                                         THINK ARTS   \n",
       "4  201733719E                 JUS INFANTS @ MACPHERSON PTE. LTD.   \n",
       "5   53227394W                                       MATHS TABLET   \n",
       "6  202209857Z                           YORK EDUCATION PTE. LTD.   \n",
       "7  201711911W                MAPLEBEAR LEARNING GARDEN PTE. LTD.   \n",
       "8  201540131W  4HANDS DENTAL ASSISTING TRAINING SCHOOL PTE. LTD.   \n",
       "9  202337418G        OUT OF THE BOX ACADEMY (CLEMENTI) PTE. LTD.   \n",
       "\n",
       "  BUSINESS_CONSTITUTION_DESCRIPTION           ENTITY_TYPE_DESCRIPTION  \\\n",
       "0                   SOLE-PROPRIETOR  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "1                              None                     LOCAL COMPANY   \n",
       "2                              None     LIMITED LIABILITY PARTNERSHIP   \n",
       "3                       PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "4                              None                     LOCAL COMPANY   \n",
       "5                   SOLE-PROPRIETOR  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "6                              None                     LOCAL COMPANY   \n",
       "7                              None                     LOCAL COMPANY   \n",
       "8                              None                     LOCAL COMPANY   \n",
       "9                              None                     LOCAL COMPANY   \n",
       "\n",
       "  ENTITY_STATUS_DESCRIPTION REGISTRATION_INCORPORATION_DATE  \\\n",
       "0                      LIVE                      07-04-2021   \n",
       "1              LIVE COMPANY                      04-11-2023   \n",
       "2                      LIVE                      11-11-2015   \n",
       "3                      LIVE                      06-10-2011   \n",
       "4              LIVE COMPANY                      22-11-2017   \n",
       "5                      LIVE                      04-12-2012   \n",
       "6              LIVE COMPANY                      22-03-2022   \n",
       "7              LIVE COMPANY                      01-05-2017   \n",
       "8              LIVE COMPANY                      10-11-2015   \n",
       "9              LIVE COMPANY                      18-09-2023   \n",
       "\n",
       "   PRIMARY_SSIC_CODE SECONDARY_SSIC_CODE            STREET_NAME POSTAL_CODE  \\\n",
       "0              85509                  na       COMPASSVALE WALK      540230   \n",
       "1              85509                  na             BEACH ROAD      189695   \n",
       "2              85509               74901     TAMPINES STREET 23      527201   \n",
       "3              85509                  na         YARROW GARDENS      455021   \n",
       "4              88911                  na   KALLANG PUDDING ROAD      349318   \n",
       "5              85509                  na   ANG MO KIO AVENUE 10      560555   \n",
       "6              88911               85101            CASHEW ROAD      679637   \n",
       "7              88911               88912          BRADDELL ROAD      579713   \n",
       "8              88991                  na  JURONG WEST STREET 64      641684   \n",
       "9              88912               85509      CLEMENTI AVENUE 3      120433   \n",
       "\n",
       "  PARENT_INDUSTRY INDUSTRY_TYPE                              SUB_INDUSTRY  \\\n",
       "0          Others   Educational              Tuition & Enrichment Centers   \n",
       "1          Others   Educational              Tuition & Enrichment Centers   \n",
       "2          Others   Educational              Tuition & Enrichment Centers   \n",
       "3          Others   Educational              Tuition & Enrichment Centers   \n",
       "4          Others      Hospital  Social Services (Without Accommodations)   \n",
       "5          Others   Educational              Tuition & Enrichment Centers   \n",
       "6          Others      Hospital  Social Services (Without Accommodations)   \n",
       "7          Others      Hospital  Social Services (Without Accommodations)   \n",
       "8          Others      Hospital  Social Services (Without Accommodations)   \n",
       "9          Others      Hospital  Social Services (Without Accommodations)   \n",
       "\n",
       "                                         DESCRIPTION  \n",
       "0                            Training Courses N.E.C.  \n",
       "1                            Training Courses N.E.C.  \n",
       "2                            Training Courses N.E.C.  \n",
       "3                            Training Courses N.E.C.  \n",
       "4  Infant Care Services; Child Minding Services F...  \n",
       "5                            Training Courses N.E.C.  \n",
       "6  Infant Care Services; Child Minding Services F...  \n",
       "7  Infant Care Services; Child Minding Services F...  \n",
       "8  Job Training And Vocational Rehabilitation Ser...  \n",
       "9  Student Care Services; Child Minding Services ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acra_data_filtered_by_industry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83b183e",
   "metadata": {},
   "source": [
    "### Mining RecordOwl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "907848f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# acra_data_filtered_by_industry = pd.DataFrame({\n",
    "#     \"UEN\": [\"201711911W\"]\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f852275e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîé Processing Batch 1/2 (5 UENs)\n",
      "============================================================\n",
      "  üìã Added UEN to batch: 53431824W\n",
      "  üìã Added UEN to batch: 202344030R\n",
      "  üìã Added UEN to batch: T15LL1885G\n",
      "  üìã Added UEN to batch: 53200915X\n",
      "  üìã Added UEN to batch: 201733719E\n",
      "  üì° Starting Apify run for batch: 53431824W, 202344030R, T15LL1885G, 53200915X, 201733719E (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:47:28.219Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:47:28.221Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:47:28.287Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:47:28.486Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:47:29.904Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:47:30.024Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:47:30.819Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:47:31.000Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:47:40.966Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 2.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 3.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:47:40.967Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:47:57.410Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 53431824W\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:47:57.416Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Search input found\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:47:57.417Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Waiting for page to stabilize...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:48:00.861Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m UEN typed successfully: 53431824W\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:48:00.862Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Clicking submit button...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:48:00.877Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Submit button clicked\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:48:07.254Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Page stabilized after submit\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:48:07.255Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Verifying company links are present...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:48:07.259Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Company links confirmed\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:48:07.270Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Found company link: https://recordowl.com/company/tutorsville-sg\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:48:07.271Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Navigating to company page...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:48:09.655Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Company page loaded\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:48:14.656Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Waiting for page content...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:48:14.663Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Content loaded\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:48:17.664Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Verifying UEN on company page...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:48:17.668Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m ‚úì UEN verified on page: 53431824W\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:48:17.725Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Successfully extracted HTML content (1118529 chars)\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:48:18.211Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:48:18.837Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[null,1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":36609,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":36609,\"requestsTotal\":1,\"crawlerRuntimeMillis\":48095}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:48:18.838Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> 2025-11-13T02:48:18.849Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:VX99G6Zb7OzSz3Y6r]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚è≥ Waiting for run to complete...\n",
      "  ‚úÖ Run succeeded with data\n",
      "  ‚è≥ Waiting for dataset to be ready...\n",
      "  ‚è≥ Dataset not ready, waiting 5s more...\n",
      "  ‚è≥ Dataset not ready, waiting 10s more...\n",
      "  ‚è≥ Dataset not ready, waiting 15s more...\n",
      "  ‚è≥ Dataset not ready, waiting 20s more...\n",
      "  üìä Dataset has 1 item(s)\n",
      "\n",
      "  üîç Processing results for 53431824W...\n",
      "  ‚úÖ Successfully scraped 53431824W (1118529 chars of HTML)\n",
      "  ‚úÖ Extracted: 0 email(s), Phone: None found\n",
      "\n",
      "  üîç Processing results for 202344030R...\n",
      "  ‚ö†Ô∏è No dataset item found for UEN 202344030R\n",
      "  ‚ùå No dataset item returned\n",
      "\n",
      "  üîç Processing results for T15LL1885G...\n",
      "  ‚ö†Ô∏è No dataset item found for UEN T15LL1885G\n",
      "  ‚ùå No dataset item returned\n",
      "\n",
      "  üîç Processing results for 53200915X...\n",
      "  ‚ö†Ô∏è No dataset item found for UEN 53200915X\n",
      "  ‚ùå No dataset item returned\n",
      "\n",
      "  üîç Processing results for 201733719E...\n",
      "  ‚ö†Ô∏è No dataset item found for UEN 201733719E\n",
      "  ‚ùå No dataset item returned\n",
      "\n",
      "  üí§ Sleeping for 26s before next batch...\n",
      "\n",
      "============================================================\n",
      "üîé Processing Batch 2/2 (5 UENs)\n",
      "============================================================\n",
      "  üìã Added UEN to batch: 53227394W\n",
      "  üìã Added UEN to batch: 202209857Z\n",
      "  üìã Added UEN to batch: 201711911W\n",
      "  üìã Added UEN to batch: 201540131W\n",
      "  üìã Added UEN to batch: 202337418G\n",
      "  üì° Starting Apify run for batch: 53227394W, 202209857Z, 201711911W, 201540131W, 202337418G (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:49:58.383Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:49:58.385Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:49:58.424Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:49:58.637Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:49:59.307Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:49:59.566Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:00.742Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:00.888Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:08.657Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 2.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 3.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:08.659Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:29.932Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 53227394W\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:29.946Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Search input found\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:29.948Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Waiting for page to stabilize...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:33.290Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m UEN typed successfully: 53227394W\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:33.291Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Clicking submit button...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:33.361Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Submit button clicked\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:38.849Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Page stabilized after submit\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:38.851Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Verifying company links are present...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:38.973Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Company links confirmed\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:38.985Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Found company link: https://recordowl.com/company/maths-tablet\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:38.987Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Navigating to company page...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:48.620Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Company page loaded\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:53.655Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Waiting for page content...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:53.660Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Content loaded\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:56.659Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Verifying UEN on company page...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:56.661Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m ‚úì UEN verified on page: 53227394W\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:56.748Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Successfully extracted HTML content (1134724 chars)\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:57.498Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:57.940Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[null,1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":47740,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":47740,\"requestsTotal\":1,\"crawlerRuntimeMillis\":57278}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:57.942Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> Status: RUNNING, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:pGuGaOMdCMNL1C3UT]\u001b[0m -> 2025-11-13T02:50:58.249Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 476\u001b[39m\n\u001b[32m    454\u001b[39m run_input = {\n\u001b[32m    455\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mstartUrls\u001b[39m\u001b[33m\"\u001b[39m: start_urls,\n\u001b[32m    456\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33museChrome\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    472\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mproxyRotation\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mRECOMMENDED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    473\u001b[39m }\n\u001b[32m    475\u001b[39m \u001b[38;5;66;03m# Use retry logic for 403 errors (5 attempts = more chances to recover)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m run, error = \u001b[43mrun_apify_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muen_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m run:\n\u001b[32m    479\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ‚ùå Apify call failed for batch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mrun_apify_with_retry\u001b[39m\u001b[34m(client, run_input, uen_batch, max_retries)\u001b[39m\n\u001b[32m     68\u001b[39m uen_list_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(uen_batch)\n\u001b[32m     69\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  üì° Starting Apify run for batch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muen_list_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (attempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m run = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapify/puppeteer-scraper\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ‚è≥ Waiting for run to complete...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     73\u001b[39m run_client = client.run(run[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\apify_client\\_logging.py:86\u001b[39m, in \u001b[36m_injects_client_details_to_log_context.<locals>.wrapper\u001b[39m\u001b[34m(resource_client, *args, **kwargs)\u001b[39m\n\u001b[32m     83\u001b[39m log_context.client_method.set(fun.\u001b[34m__qualname__\u001b[39m)\n\u001b[32m     84\u001b[39m log_context.resource_id.set(resource_client.resource_id)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\apify_client\\clients\\resource_clients\\actor.py:362\u001b[39m, in \u001b[36mActorClient.call\u001b[39m\u001b[34m(self, run_input, content_type, build, max_items, max_total_charge_usd, restart_on_error, memory_mbytes, timeout_secs, webhooks, force_permission_level, wait_secs, logger)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logger == \u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    360\u001b[39m     logger = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_status_message_watcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_logger\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_streamed_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_logger\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstarted_run\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_for_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_secs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_secs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\apify_client\\clients\\resource_clients\\log.py:530\u001b[39m, in \u001b[36mStatusMessageWatcherSync.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\n\u001b[32m    527\u001b[39m     \u001b[38;5;28mself\u001b[39m, exc_type: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mBaseException\u001b[39;00m] | \u001b[38;5;28;01mNone\u001b[39;00m, exc_val: \u001b[38;5;167;01mBaseException\u001b[39;00m | \u001b[38;5;28;01mNone\u001b[39;00m, exc_tb: TracebackType | \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    528\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    529\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Cancel the logging task.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\apify_client\\clients\\resource_clients\\log.py:515\u001b[39m, in \u001b[36mStatusMessageWatcherSync.stop\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._logging_thread:\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mLogging thread is not active\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_sleep_time_s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[38;5;28mself\u001b[39m._stop_logging = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    517\u001b[39m \u001b[38;5;28mself\u001b[39m._logging_thread.join()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "client = ApifyClient(\"apify_api_xqctmgUBzh5ukWumUVT9SwlnOxEdft4dpNI6\")\n",
    "\n",
    "SOCIAL_MEDIA_DOMAINS = [\n",
    "    \"facebook.com\", \"linkedin.com\", \"instagram.com\", \"youtube.com\",\n",
    "    \"tiktok.com\", \"twitter.com\", \"x.com\", \"pinterest.com\"\n",
    "]\n",
    "\n",
    "def fetch_dataset_items_safe(dataset_client, max_retries=5, initial_wait=3):\n",
    "    \"\"\"Safely fetch dataset items with multiple retry strategies.\"\"\"\n",
    "    dataset_items = []\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Strategy 1: Try using iterate_items() (streaming)\n",
    "            try:\n",
    "                dataset_items = list(dataset_client.iterate_items())\n",
    "                if dataset_items:\n",
    "                    return dataset_items\n",
    "            except (HTTPError, ConnectionError, ProtocolError, Exception) as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = initial_wait * (2 ** attempt)  # Exponential backoff\n",
    "                    print(f\"  ‚ö†Ô∏è Iteration method failed (attempt {attempt + 1}/{max_retries}), trying direct fetch in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è Iteration method failed after all retries, trying direct fetch...\")\n",
    "            \n",
    "            # Strategy 2: Try using list_items() (direct pagination)\n",
    "            try:\n",
    "                offset = 0\n",
    "                limit = 100\n",
    "                while True:\n",
    "                    page = dataset_client.list_items(offset=offset, limit=limit, clean=True)\n",
    "                    if not page.items:\n",
    "                        break\n",
    "                    dataset_items.extend(page.items)\n",
    "                    if len(page.items) < limit:\n",
    "                        break\n",
    "                    offset += limit\n",
    "                \n",
    "                if dataset_items:\n",
    "                    return dataset_items\n",
    "            except (HTTPError, ConnectionError, ProtocolError, Exception) as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = initial_wait * (2 ** attempt)\n",
    "                    print(f\"  ‚ö†Ô∏è Direct fetch failed (attempt {attempt + 1}/{max_retries}), retrying in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"  ‚ùå All fetch methods failed: {e}\")\n",
    "                    return []\n",
    "                    \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = initial_wait * (2 ** attempt)\n",
    "                print(f\"  ‚ö†Ô∏è Unexpected error (attempt {attempt + 1}/{max_retries}), retrying in {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"  ‚ùå Failed after all retries: {e}\")\n",
    "                return []\n",
    "    \n",
    "    return dataset_items\n",
    "\n",
    "def run_apify_with_retry(client, run_input, uen_batch, max_retries=3):\n",
    "    \"\"\"Run Apify with exponential backoff on 403 errors AND verify dataset has items.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            uen_list_str = \", \".join(uen_batch)\n",
    "            print(f\"  üì° Starting Apify run for batch: {uen_list_str} (attempt {attempt + 1}/{max_retries})...\")\n",
    "            run = client.actor(\"apify/puppeteer-scraper\").call(run_input=run_input)\n",
    "            \n",
    "            print(f\"  ‚è≥ Waiting for run to complete...\")\n",
    "            run_client = client.run(run[\"id\"])\n",
    "            run_info = run_client.wait_for_finish()\n",
    "            \n",
    "            # CRITICAL FIX: Check if run actually scraped pages, not just if it \"succeeded\"\n",
    "            if run_info and \"status\" in run_info:\n",
    "                status = run_info.get(\"status\")\n",
    "                \n",
    "                # Even if status is \"SUCCEEDED\", verify dataset actually has items\n",
    "                if status == \"SUCCEEDED\" and \"defaultDatasetId\" in run:\n",
    "                    # Quick check if dataset has any items\n",
    "                    try:\n",
    "                        dataset_check = client.dataset(run[\"defaultDatasetId\"])\n",
    "                        time.sleep(2)  # Brief wait for dataset to be ready\n",
    "                        test_items = dataset_check.list_items(limit=1, clean=True)\n",
    "                        \n",
    "                        if test_items.items and len(test_items.items) > 0:\n",
    "                            # Dataset has items - true success!\n",
    "                            print(f\"  ‚úÖ Run succeeded with data\")\n",
    "                            return run, None\n",
    "                        else:\n",
    "                            # Status says \"SUCCEEDED\" but dataset is EMPTY - this is a failure!\n",
    "                            print(f\"  ‚ö†Ô∏è Run completed but dataset is empty (likely 403 block)\")\n",
    "                            # Treat as 403 and retry\n",
    "                            if attempt < max_retries - 1:\n",
    "                                wait_time = 30 * (2 ** attempt)\n",
    "                                print(f\"  üîÑ Retrying in {wait_time}s...\")\n",
    "                                time.sleep(wait_time)\n",
    "                                continue\n",
    "                            else:\n",
    "                                return None, \"Dataset empty after all retries (403 blocking)\"\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚ö†Ô∏è Could not verify dataset: {e}\")\n",
    "                        # If we can't check dataset, try to use the run anyway\n",
    "                        return run, None\n",
    "                \n",
    "                elif status != \"SUCCEEDED\":\n",
    "                    # Check error message for 403\n",
    "                    error_msg = str(run_info)\n",
    "                    if \"403\" in error_msg or \"blocked\" in error_msg.lower():\n",
    "                        if attempt < max_retries - 1:\n",
    "                            wait_time = 30 * (2 ** attempt)  # 30s, 60s, 120s\n",
    "                            print(f\"  üö´ Request blocked (403), waiting {wait_time}s before retry...\")\n",
    "                            time.sleep(wait_time)\n",
    "                            continue\n",
    "            \n",
    "            return run, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_str = str(e)\n",
    "            if \"403\" in error_str or \"blocked\" in error_str.lower():\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 30 * (2 ** attempt)\n",
    "                    print(f\"  üö´ Request blocked (403), waiting {wait_time}s before retry...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "            return None, f\"Apify call failed: {str(e)}\"\n",
    "    \n",
    "    return None, \"Max retries exceeded due to 403 blocking\"\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# Process UENs in batches of 2\n",
    "BATCH_SIZE = 5\n",
    "total_rows = len(acra_data_filtered_by_industry)\n",
    "total_batches = (total_rows + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "for batch_idx in range(0, total_rows, BATCH_SIZE):\n",
    "    batch = acra_data_filtered_by_industry.iloc[batch_idx:batch_idx + BATCH_SIZE]\n",
    "    batch_num = (batch_idx // BATCH_SIZE) + 1\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üîé Processing Batch {batch_num}/{total_batches} ({len(batch)} UENs)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Build startUrls with userData for each UEN in batch\n",
    "    start_urls = []\n",
    "    uen_batch = []\n",
    "    for _, row in batch.iterrows():\n",
    "        uen = str(row[\"UEN\"]).strip()\n",
    "        uen_batch.append(uen)\n",
    "        start_urls.append({\n",
    "            \"url\": \"https://recordowl.com/\",\n",
    "            \"userData\": {\"uen\": uen}\n",
    "        })\n",
    "        print(f\"  üìã Added UEN to batch: {uen}\")\n",
    "\n",
    "    # Build pageFunction that reads UEN from request.userData\n",
    "    page_function = \"\"\"\n",
    "    async function pageFunction(context) {\n",
    "        const { page, log, request } = context;\n",
    "        const uen = request?.userData?.uen || \"\";\n",
    "        \n",
    "        if (!uen) {\n",
    "            log.error(\"Missing UEN in request.userData\");\n",
    "            return { status: 'error', uen: null, error: 'Missing UEN' };\n",
    "        }\n",
    "        \n",
    "        log.info(\"Visiting RecordOwl for UEN: \" + uen);\n",
    "\n",
    "        try {\n",
    "            // Step 1: Wait for search input\n",
    "            await page.waitForSelector(\"input[placeholder='Search company name, industry, or address']\", { timeout: 30000 });\n",
    "            log.info(\"Search input found\");\n",
    "            \n",
    "            // Step 2: Type UEN into search box with error handling and navigation protection\n",
    "            try {\n",
    "                // Wait for page to be stable (no navigation happening)\n",
    "                log.info(\"Waiting for page to stabilize...\");\n",
    "                await new Promise(r => setTimeout(r, 2000)); // Wait for any auto-navigation to complete\n",
    "                \n",
    "                // Wait for input to be present and stable\n",
    "                await page.waitForSelector(\"input[placeholder='Search company name, industry, or address']\", { \n",
    "                    timeout: 30000,\n",
    "                    visible: true \n",
    "                });\n",
    "                \n",
    "                // Re-find input right before typing (in case page navigated)\n",
    "                let input = await page.$(\"input[placeholder='Search company name, industry, or address']\");\n",
    "                if (!input) {\n",
    "                    log.error(\"Input element not found after wait\");\n",
    "                    return { status: 'error', uen, error: 'Input element not found' };\n",
    "                }\n",
    "                \n",
    "                // Clear and type with retry logic\n",
    "                let typed = false;\n",
    "                for (let attempt = 0; attempt < 3; attempt++) {\n",
    "                    try {\n",
    "                        // Re-find input on each attempt (in case context was destroyed)\n",
    "                        input = await page.$(\"input[placeholder='Search company name, industry, or address']\");\n",
    "                        if (!input) {\n",
    "                            throw new Error(\"Input not found on attempt \" + (attempt + 1));\n",
    "                        }\n",
    "                        \n",
    "                        // Click to focus\n",
    "                        await input.click({ clickCount: 3 });\n",
    "                        await new Promise(r => setTimeout(r, 300)); // Small delay after click\n",
    "                        \n",
    "                        // Clear input first\n",
    "                        await page.evaluate((selector) => {\n",
    "                            const el = document.querySelector(selector);\n",
    "                            if (el) el.value = '';\n",
    "                        }, \"input[placeholder='Search company name, industry, or address']\");\n",
    "                        \n",
    "                        // Type UEN\n",
    "                        await input.type(uen, { delay: 100 });\n",
    "                        typed = true;\n",
    "                        log.info(\"UEN typed successfully: \" + uen);\n",
    "                        break;\n",
    "                    } catch (typeErr) {\n",
    "                        if (typeErr.message.includes(\"Execution context was destroyed\") || \n",
    "                            typeErr.message.includes(\"navigation\")) {\n",
    "                            log.info(\"Navigation occurred during typing (attempt \" + (attempt + 1) + \"/3), retrying...\");\n",
    "                            // Wait for page to stabilize after navigation\n",
    "                            await new Promise(r => setTimeout(r, 2000));\n",
    "                            // Re-wait for input\n",
    "                            await page.waitForSelector(\"input[placeholder='Search company name, industry, or address']\", { \n",
    "                                timeout: 10000,\n",
    "                                visible: true \n",
    "                            });\n",
    "                            continue;\n",
    "                        } else {\n",
    "                            throw typeErr;\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                if (!typed) {\n",
    "                    log.error(\"Failed to type UEN after all retries\");\n",
    "                    return { status: 'error', uen, error: 'Failed to type UEN after retries' };\n",
    "                }\n",
    "                \n",
    "            } catch (typeErr) {\n",
    "                log.error(\"Error typing UEN: \" + typeErr.message);\n",
    "                return { status: 'error', uen, error: 'Failed to type UEN: ' + typeErr.message };\n",
    "            }\n",
    "\n",
    "            // Step 3: Submit search with flexible waiting strategy\n",
    "            try {\n",
    "                log.info(\"Clicking submit button...\");\n",
    "                \n",
    "                // Click submit button first\n",
    "                await page.click(\"button[type='submit']\");\n",
    "                log.info(\"Submit button clicked\");\n",
    "                \n",
    "                // Wait for either navigation OR results to appear (more flexible)\n",
    "                // Strategy: Wait for results to appear, with navigation as optional\n",
    "                try {\n",
    "                    // Option 1: Wait for navigation (if it happens) - non-blocking\n",
    "                    const navigationPromise = page.waitForNavigation({ \n",
    "                        waitUntil: 'networkidle2', \n",
    "                        timeout: 30000 \n",
    "                    }).catch(() => {\n",
    "                        log.info(\"Navigation did not occur (may be client-side routing)\");\n",
    "                        return null;\n",
    "                    });\n",
    "                    \n",
    "                    // Option 2: Wait for results to appear (more reliable)\n",
    "                    const resultsPromise = page.waitForSelector(\"a[href*='/company/']\", { \n",
    "                        timeout: 60000 \n",
    "                    });\n",
    "                    \n",
    "                    // Wait for either navigation or results (whichever happens first)\n",
    "                    await Promise.race([\n",
    "                        navigationPromise,\n",
    "                        resultsPromise\n",
    "                    ]);\n",
    "                    \n",
    "                    // Give page time to stabilize\n",
    "                    await new Promise(r => setTimeout(r, 2000));\n",
    "                    log.info(\"Page stabilized after submit\");\n",
    "                    \n",
    "                } catch (waitErr) {\n",
    "                    // If both navigation and results wait failed, try one more time for results\n",
    "                    log.info(\"Initial wait failed, trying again for results: \" + waitErr.message);\n",
    "                    try {\n",
    "                        await page.waitForSelector(\"a[href*='/company/']\", { timeout: 30000 });\n",
    "                        log.info(\"Results found on retry\");\n",
    "                    } catch (retryErr) {\n",
    "                        log.info(\"No company links found after submit, might be not found\");\n",
    "                        return { status: 'not_found', uen };\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "            } catch (navErr) {\n",
    "                log.error(\"Error during submit: \" + navErr.message);\n",
    "                // Don't fail immediately - try to check if results are already there\n",
    "                try {\n",
    "                    const hasResults = await page.$(\"a[href*='/company/']\");\n",
    "                    if (hasResults) {\n",
    "                        log.info(\"Results found despite submit error\");\n",
    "                    } else {\n",
    "                        return { status: 'error', uen, error: 'Submit failed: ' + navErr.message };\n",
    "                    }\n",
    "                } catch (checkErr) {\n",
    "                    return { status: 'error', uen, error: 'Submit failed: ' + navErr.message };\n",
    "                }\n",
    "            }\n",
    "\n",
    "            // Step 4: Verify search results are present\n",
    "            log.info(\"Verifying company links are present...\");\n",
    "            try {\n",
    "                // Double-check that results are actually there\n",
    "                await page.waitForSelector(\"a[href*='/company/']\", { timeout: 10000 });\n",
    "                log.info(\"Company links confirmed\");\n",
    "            } catch (e) {\n",
    "                log.info(\"No company links found, might be not found\");\n",
    "                return { status: 'not_found', uen };\n",
    "            }\n",
    "\n",
    "            // Step 5: Find the correct company link (in a new execution context after navigation)\n",
    "            let companyLink;\n",
    "            try {\n",
    "                companyLink = await page.evaluate((searchUen) => {\n",
    "                    const links = Array.from(document.querySelectorAll(\"a[href*='/company/']\"));\n",
    "                    \n",
    "                    // Find link where UEN appears in text or URL\n",
    "                    const uenUpper = searchUen.toUpperCase();\n",
    "                    const uenLower = searchUen.toLowerCase();\n",
    "                    \n",
    "                    for (const a of links) {\n",
    "                        const text = (a.innerText || \"\").toUpperCase();\n",
    "                        const href = (a.href || \"\").toLowerCase();\n",
    "                        \n",
    "                        // Check if UEN appears in text or URL (case-insensitive)\n",
    "                        if (text.includes(uenUpper) || href.includes(uenLower)) {\n",
    "                            console.log(\"Found UEN match: \" + a.href);\n",
    "                            return a.href;\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    // Fallback: Take first company link if available\n",
    "                    if (links.length > 0) {\n",
    "                        console.log(\"No exact UEN match, using first link: \" + links[0].href);\n",
    "                        return links[0].href;\n",
    "                    }\n",
    "                    \n",
    "                    console.log(\"No company links found\");\n",
    "                    return null;\n",
    "                }, uen);\n",
    "                \n",
    "                if (!companyLink) {\n",
    "                    log.info(\"No company links found on results page\");\n",
    "                    return { status: 'not_found', uen };\n",
    "                }\n",
    "                log.info(\"Found company link: \" + companyLink);\n",
    "            } catch (evalErr) {\n",
    "                log.error(\"Error finding company link: \" + evalErr.message);\n",
    "                return { status: 'error', uen, error: 'Failed to find company link: ' + evalErr.message };\n",
    "            }\n",
    "\n",
    "            // Step 6: Navigate to company page if not already there\n",
    "            if (page.url() !== companyLink) {\n",
    "                try {\n",
    "                    log.info(\"Navigating to company page...\");\n",
    "                    await page.goto(companyLink, { \n",
    "                        waitUntil: 'networkidle2', \n",
    "                        timeout: 60000 \n",
    "                    });\n",
    "                    log.info(\"Company page loaded\");\n",
    "                    \n",
    "                    // Critical: Wait for page to fully stabilize\n",
    "                    await new Promise(r => setTimeout(r, 5000));\n",
    "                } catch (gotoErr) {\n",
    "                    log.error(\"Error navigating to company page: \" + gotoErr.message);\n",
    "                    return { status: 'error', uen, error: 'Failed to load company page: ' + gotoErr.message };\n",
    "                }\n",
    "            }\n",
    "\n",
    "            // Step 7: Wait for content to load (with multiple fallback strategies)\n",
    "            log.info(\"Waiting for page content...\");\n",
    "            try {\n",
    "                await Promise.race([\n",
    "                    page.waitForSelector('dt', { timeout: 15000 }),\n",
    "                    page.waitForSelector('dl', { timeout: 15000 }),\n",
    "                    page.waitForSelector('.max-w-7xl', { timeout: 15000 }),\n",
    "                    new Promise(r => setTimeout(r, 10000)) // Fallback: just wait 10s\n",
    "                ]);\n",
    "                log.info(\"Content loaded\");\n",
    "            } catch (contentErr) {\n",
    "                log.info(\"Content wait timeout, but continuing: \" + contentErr.message);\n",
    "            }\n",
    "            \n",
    "            // Additional stabilization wait\n",
    "            await new Promise(r => setTimeout(r, 3000));\n",
    "            \n",
    "            // Step 7.5: VERIFY we're on the correct company page\n",
    "            log.info(\"Verifying UEN on company page...\");\n",
    "            try {\n",
    "                const pageUEN = await page.evaluate((searchUen) => {\n",
    "                    const pageText = (document.body.innerText || \"\").toUpperCase();\n",
    "                    return pageText.includes(searchUen.toUpperCase());\n",
    "                }, uen);\n",
    "                \n",
    "                if (pageUEN) {\n",
    "                    log.info(\"‚úì UEN verified on page: \" + uen);\n",
    "                } else {\n",
    "                    log.info(\"‚ö† Warning: UEN not found in page text, but continuing...\");\n",
    "                }\n",
    "            } catch (verifyErr) {\n",
    "                log.info(\"Could not verify UEN, but continuing: \" + verifyErr.message);\n",
    "            }\n",
    "            \n",
    "            // Step 8: Extract content (in stable context) - ONLY VISIBLE ELEMENTS\n",
    "            let html_content, title, url;\n",
    "            try {\n",
    "                // Get only the visible HTML content by removing hidden elements\n",
    "                await page.evaluate(() => {\n",
    "                    // Remove all elements that are hidden from view\n",
    "                    const allElements = document.querySelectorAll('*');\n",
    "                    allElements.forEach(el => {\n",
    "                        const style = window.getComputedStyle(el);\n",
    "                        // Mark hidden elements with a special attribute\n",
    "                        if (style.display === 'none' || \n",
    "                            style.visibility === 'hidden' || \n",
    "                            style.opacity === '0' ||\n",
    "                            el.hidden ||\n",
    "                            el.hasAttribute('hidden')) {\n",
    "                            el.setAttribute('data-hidden-element', 'true');\n",
    "                        }\n",
    "                    });\n",
    "                });\n",
    "                \n",
    "                html_content = await page.content();\n",
    "                title = await page.title();\n",
    "                url = page.url();\n",
    "                log.info(\"Successfully extracted HTML content (\" + html_content.length + \" chars)\");\n",
    "            } catch (extractErr) {\n",
    "                log.error(\"Error extracting content: \" + extractErr.message);\n",
    "                return { status: 'error', uen, error: 'Failed to extract content: ' + extractErr.message };\n",
    "            }\n",
    "\n",
    "            return { status: 'success', uen, url, title, html_content };\n",
    "            \n",
    "        } catch (err) {\n",
    "            log.error(\"Unexpected error in pageFunction: \" + err.message);\n",
    "            log.error(\"Stack: \" + err.stack);\n",
    "            return { status: 'error', uen, error: err.message };\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    run_input = {\n",
    "        \"startUrls\": start_urls,\n",
    "        \"useChrome\": True,\n",
    "        \"headless\": True,\n",
    "        \"stealth\": True,\n",
    "        \"pageFunction\": page_function,\n",
    "        \"ignoreSslErrors\": False,\n",
    "        \"ignoreCorsAndCsp\": False,\n",
    "        \"maxRequestRetries\": 3,\n",
    "        \"maxRequestsPerCrawl\": len(start_urls),  # Allow all UENs in batch\n",
    "        \"maxConcurrency\": 5,  # Process 2 UENs in parallel\n",
    "        \"pageLoadTimeoutSecs\": 90,\n",
    "        \"pageFunctionTimeoutSecs\": 180,\n",
    "        \"waitUntil\": [\"networkidle2\"],\n",
    "        \"proxyConfiguration\": {\n",
    "            \"useApifyProxy\": True,\n",
    "            \"apifyProxyGroups\": [\"RESIDENTIAL\"],\n",
    "        },\n",
    "        \"proxyRotation\": \"RECOMMENDED\",\n",
    "    }\n",
    "\n",
    "    # Use retry logic for 403 errors (5 attempts = more chances to recover)\n",
    "    run, error = run_apify_with_retry(client, run_input, uen_batch, max_retries=5)\n",
    "\n",
    "    if error or not run:\n",
    "        print(f\"  ‚ùå Apify call failed for batch: {error}\")\n",
    "        # Add error results for all UENs in batch\n",
    "        for uen in uen_batch:\n",
    "            all_results.append({\n",
    "                \"UEN\": uen,\n",
    "                \"Emails\": None,\n",
    "                \"Phones\": None,\n",
    "                \"Website\": None,\n",
    "                \"Facebook\": None,\n",
    "                \"LinkedIn\": None,\n",
    "                \"Instagram\": None,\n",
    "                \"TikTok\": None,\n",
    "                \"address\": None,\n",
    "                \"RecordOwl_Link\": None,\n",
    "                \"Error\": error or \"No run returned\"\n",
    "            })\n",
    "        time.sleep(10)  # Longer sleep after failure\n",
    "        continue\n",
    "\n",
    "    if not run or \"defaultDatasetId\" not in run:\n",
    "        print(f\"  ‚ö†Ô∏è No valid dataset returned for batch\")\n",
    "        # Add error results for all UENs in batch\n",
    "        for uen in uen_batch:\n",
    "            all_results.append({\n",
    "                \"UEN\": uen,\n",
    "                \"Emails\": None,\n",
    "                \"Phones\": None,\n",
    "                \"Website\": None,\n",
    "                \"Facebook\": None,\n",
    "                \"LinkedIn\": None,\n",
    "                \"Instagram\": None,\n",
    "                \"TikTok\": None,\n",
    "                \"address\": None,\n",
    "                \"RecordOwl_Link\": None,\n",
    "                \"Error\": \"No dataset returned\"\n",
    "            })\n",
    "        continue\n",
    "\n",
    "    # Wait for dataset to be ready with progressive checking\n",
    "    print(f\"  ‚è≥ Waiting for dataset to be ready...\")\n",
    "    time.sleep(8)  # Increased wait for concurrent requests (both need to complete)\n",
    "    \n",
    "    # Try to fetch dataset with progressive waits\n",
    "    dataset_client = client.dataset(run[\"defaultDatasetId\"])\n",
    "    for check_attempt in range(5):\n",
    "        try:\n",
    "            # Check if dataset has expected number of items\n",
    "            test_fetch = dataset_client.list_items(limit=len(uen_batch) + 1, clean=True)\n",
    "            if test_fetch.items and len(test_fetch.items) >= len(uen_batch):\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if check_attempt < 4:\n",
    "            additional_wait = 5 * (check_attempt + 1)\n",
    "            print(f\"  ‚è≥ Dataset not ready, waiting {additional_wait}s more...\")\n",
    "            time.sleep(additional_wait)\n",
    "    \n",
    "    # Fetch dataset items with improved error handling\n",
    "    dataset_items = fetch_dataset_items_safe(\n",
    "        dataset_client,\n",
    "        max_retries=5,\n",
    "        initial_wait=5\n",
    "    )\n",
    "    \n",
    "    # Process items\n",
    "    if not dataset_items:\n",
    "        print(f\"  ‚ö†Ô∏è Dataset is empty - no items returned!\")\n",
    "    else:\n",
    "        print(f\"  üìä Dataset has {len(dataset_items)} item(s)\")\n",
    "    \n",
    "    # Create a mapping of UEN to dataset item\n",
    "    uen_to_item = {}\n",
    "    for item in dataset_items:\n",
    "        item_uen = item.get(\"uen\")\n",
    "        if item_uen:\n",
    "            uen_to_item[item_uen] = item\n",
    "    \n",
    "    # Process each UEN in the batch\n",
    "    for uen in uen_batch:\n",
    "        print(f\"\\n  üîç Processing results for {uen}...\")\n",
    "        \n",
    "        item = uen_to_item.get(uen)\n",
    "        scraped_html, record_owl_url = None, None\n",
    "        \n",
    "        if item:\n",
    "            if item.get(\"status\") == \"success\":\n",
    "                scraped_html = item.get(\"html_content\", \"\")\n",
    "                record_owl_url = item.get(\"url\")\n",
    "                if scraped_html:\n",
    "                    print(f\"  ‚úÖ Successfully scraped {uen} ({len(scraped_html)} chars of HTML)\")\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è Status is 'success' but html_content is empty for {uen}\")\n",
    "            elif item.get(\"status\") == \"not_found\":\n",
    "                print(f\"  ‚ö†Ô∏è Company not found for UEN {uen}\")\n",
    "            elif item.get(\"status\") == \"error\":\n",
    "                print(f\"  ‚ùå Error for {uen}: {item.get('error')}\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è Unknown item status for {uen}: {item.get('status')}\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è No dataset item found for UEN {uen}\")\n",
    "\n",
    "        if not scraped_html:\n",
    "            # Determine the specific reason for failure\n",
    "            if not item:\n",
    "                error_reason = \"No dataset item returned\"\n",
    "            elif item.get(\"status\") == \"not_found\":\n",
    "                error_reason = \"Company not found on RecordOwl\"\n",
    "            elif item.get(\"status\") == \"error\":\n",
    "                error_reason = f\"Scraping error: {item.get('error', 'Unknown')}\"\n",
    "            else:\n",
    "                error_reason = \"No HTML content retrieved (unknown reason)\"\n",
    "            \n",
    "            print(f\"  ‚ùå {error_reason}\")\n",
    "            \n",
    "            all_results.append({\n",
    "                \"UEN\": uen,\n",
    "                \"Emails\": None,\n",
    "                \"Phones\": None,\n",
    "                \"Website\": None,\n",
    "                \"Facebook\": None,\n",
    "                \"LinkedIn\": None,\n",
    "                \"Instagram\": None,\n",
    "                \"TikTok\": None,\n",
    "                \"address\": None,\n",
    "                \"RecordOwl_Link\": record_owl_url or None,\n",
    "                \"Error\": error_reason\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Parse HTML (keep all existing parsing logic unchanged)\n",
    "        try:\n",
    "            soup = BeautifulSoup(scraped_html, \"html.parser\")\n",
    "            \n",
    "            # ========== CLEAN HTML: REMOVE HIDDEN/UNWANTED ELEMENTS ==========\n",
    "            # Remove hidden elements\n",
    "            for elem in soup.find_all(attrs={\"data-hidden-element\": \"true\"}):\n",
    "                elem.decompose()\n",
    "            \n",
    "            # Target company overview (exclude officer/director personal data)\n",
    "            overview_tab = (soup.select_one(\"#overview\") or \n",
    "                           soup.select_one(\"[aria-labelledby*='overview']\") or\n",
    "                           soup.select_one(\"div[role='tabpanel']\"))\n",
    "            \n",
    "            if overview_tab:\n",
    "                parent = overview_tab\n",
    "            else:\n",
    "                parent = soup.select_one(\"div.max-w-7xl.mx-auto.lg\\\\:py-6.sm\\\\:px-6.lg\\\\:px-8\")\n",
    "                if parent:\n",
    "                    # Remove officer/shareholder sections\n",
    "                    for unwanted in parent.select(\"#officers, #shareholders, #appointments, \"\n",
    "                                                 \"[id*='officer'], [id*='shareholder'], [id*='appointment']\"):\n",
    "                        unwanted.decompose()\n",
    "            \n",
    "            # Remove non-visible content\n",
    "            if parent:\n",
    "                for unwanted in parent.select(\"script, style, noscript, [style*='display:none']\"):\n",
    "                    unwanted.decompose()\n",
    "            # ========== END CLEAN HTML ==========\n",
    "\n",
    "            emails, phones, website = [], [], None\n",
    "            facebook_links, linkedin_links, instagram_links, tiktok_links = [], [], [], []\n",
    "            \n",
    "            # Helper function to check if element is visible\n",
    "            def is_element_visible(element):\n",
    "                \"\"\"Check if a BeautifulSoup element appears to be visible (not hidden).\"\"\"\n",
    "                if element is None:\n",
    "                    return False\n",
    "                # Check for hidden attribute\n",
    "                if element.has_attr('data-hidden-element'):\n",
    "                    return False\n",
    "                # Check for common hidden styles\n",
    "                style = element.get('style', '')\n",
    "                if any(hidden_style in style.lower() for hidden_style in ['display:none', 'display: none', 'visibility:hidden', 'visibility: hidden']):\n",
    "                    return False\n",
    "                # Check for hidden/aria-hidden attributes\n",
    "                if element.get('hidden') or element.get('aria-hidden') == 'true':\n",
    "                    return False\n",
    "                return True\n",
    "\n",
    "            if parent:\n",
    "                # Extract emails\n",
    "                for a in parent.select(\"a[href^=mailto]\"):\n",
    "                    email = a.get(\"href\", \"\").replace(\"mailto:\", \"\").strip()\n",
    "                    if email and email not in emails and \"@\" in email:\n",
    "                        emails.append(email)\n",
    "\n",
    "                # ========== COMPREHENSIVE SINGAPORE PHONE EXTRACTION ==========\n",
    "                # (Keep all existing phone extraction code exactly as is)\n",
    "                # ... [All the phone extraction code remains unchanged] ...\n",
    "                \n",
    "                def validate_sg_phone(digits_str):\n",
    "                    \"\"\"\n",
    "                    Validate and format Singapore phone number from digit-only string.\n",
    "                    \"\"\"\n",
    "                    if not digits_str or len(digits_str) < 8:\n",
    "                        return None\n",
    "                    \n",
    "                    non_sg_codes = [\n",
    "                        \"60\", \"62\", \"63\", \"66\", \"84\", \"95\", \"855\", \"856\", \"880\",\n",
    "                        \"81\", \"82\", \"86\", \"852\", \"853\", \"886\",\n",
    "                        \"91\", \"92\", \"93\", \"94\",\n",
    "                        \"61\", \"64\",\n",
    "                        \"90\", \"98\",\n",
    "                    ]\n",
    "                    \n",
    "                    if len(digits_str) == 8 and digits_str[0] in \"689\":\n",
    "                        return \"+65\" + digits_str\n",
    "                    \n",
    "                    if len(digits_str) >= 9:\n",
    "                        for code in non_sg_codes:\n",
    "                            if digits_str.startswith(code):\n",
    "                                return None\n",
    "                    \n",
    "                    if len(digits_str) == 10 and digits_str.startswith(\"65\") and digits_str[2] in \"689\":\n",
    "                        return \"+\" + digits_str\n",
    "                        \n",
    "                    elif len(digits_str) > 10:\n",
    "                        for i in range(len(digits_str) - 9):\n",
    "                            if digits_str[i:i+2] == \"65\" and digits_str[i+2] in \"689\":\n",
    "                                if i > 0:\n",
    "                                    prev_digits = digits_str[max(0, i-2):i]\n",
    "                                    is_part_of_other_code = any(\n",
    "                                        code.endswith(prev_digits + \"65\") \n",
    "                                        for code in non_sg_codes\n",
    "                                    )\n",
    "                                    if is_part_of_other_code:\n",
    "                                        continue\n",
    "                                return \"+\" + digits_str[i:i+10]\n",
    "                    \n",
    "                    return None\n",
    "                \n",
    "                # Method 1: Extract from tel: links\n",
    "                tel_links = [link for link in parent.select(\"a[href^='tel:'], a[href^='tel']\") \n",
    "                            if is_element_visible(link)]\n",
    "                \n",
    "                for a in tel_links:\n",
    "                    tel_href = a.get(\"href\", \"\").replace(\"tel:\", \"\").strip()\n",
    "                    digits_only = re.sub(r\"\\D\", \"\", tel_href)\n",
    "                    formatted = validate_sg_phone(digits_only)\n",
    "                    if formatted and formatted not in phones:\n",
    "                        phones.append(formatted)\n",
    "                \n",
    "                # Method 2: Extract from dt/dd structure\n",
    "                company_keywords = [\"company contact\", \"business contact\", \"office phone\", \n",
    "                                  \"main phone\", \"business phone\", \"company phone\", \"contact number\", \n",
    "                                  \"phone\", \"tel\", \"mobile\", \"call\", \"contact no\"]\n",
    "                exclude_keywords = [\"officer\", \"charge\", \"employee\", \"shareholder\", \"director\", \n",
    "                                  \"registration\", \"person\", \"individual\", \"member\", \"partner\",\n",
    "                                  \"manager\", \"owner\", \"proprietor\", \"authorized\", \"representative\",\n",
    "                                  \"appointment\", \"designation\", \"name of\", \"appointed\"]\n",
    "                \n",
    "                visible_dt_tags = [dt for dt in parent.select(\"dt\") if is_element_visible(dt)]\n",
    "                \n",
    "                for dt in visible_dt_tags:\n",
    "                    dt_text = dt.get_text(strip=True).lower()\n",
    "                    \n",
    "                    is_company = any(kw in dt_text for kw in company_keywords)\n",
    "                    is_personal = any(excl in dt_text for excl in exclude_keywords)\n",
    "                    \n",
    "                    if is_company and not is_personal:\n",
    "                        dd = dt.find_next_sibling(\"dd\")\n",
    "                        if dd and is_element_visible(dd):\n",
    "                            number_text = dd.get_text(\" \", strip=True)\n",
    "                            all_digits = re.sub(r\"\\D\", \"\", number_text)\n",
    "                            formatted = validate_sg_phone(all_digits)\n",
    "                            if formatted and formatted not in phones:\n",
    "                                phones.append(formatted)\n",
    "                \n",
    "                # Method 3: Fallback text search\n",
    "                if not phones:\n",
    "                    full_text = parent.get_text()\n",
    "                    \n",
    "                    sg_patterns = [\n",
    "                        r\"\\+[\\s\\-\\.]*65[\\s\\-\\.]*[689][\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d\",\n",
    "                        r\"\\([\\s\\-\\.]*\\+?[\\s\\-\\.]*65[\\s\\-\\.]*\\)[\\s\\-\\.]*[689][\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d\",\n",
    "                        r\"\\+?[\\s\\-\\.]*65[\\s\\-\\.]*\\([\\s\\-\\.]*[689][\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\)[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d\",\n",
    "                        r\"(?<!\\d)65[\\s\\-\\.]+[689][\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d(?!\\d)\",\n",
    "                        r\"(?<!\\d)[689][\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d[\\s\\-\\.]*\\d(?!\\d)\",\n",
    "                        r\"(?<!\\d)65[689]\\d{7}(?!\\d)\",\n",
    "                    ]\n",
    "                    \n",
    "                    for pattern in sg_patterns:\n",
    "                        matches = re.findall(pattern, full_text)\n",
    "                        for match in matches:\n",
    "                            digits = re.sub(r\"\\D\", \"\", match)\n",
    "                            formatted = validate_sg_phone(digits)\n",
    "                            if formatted and formatted not in phones:\n",
    "                                phones.append(formatted)\n",
    "                # ========== END PHONE EXTRACTION ==========\n",
    "\n",
    "                # Extract website\n",
    "                valid_websites = []\n",
    "                for a in parent.select(\"a[href^=http]\"):\n",
    "                    href = a.get(\"href\", \"\").strip()\n",
    "                    href_lower = href.lower()\n",
    "                    if not any(domain in href_lower for domain in SOCIAL_MEDIA_DOMAINS):\n",
    "                        if not any(skip in href_lower for skip in [\"recordowl\", \"apify.com\"]):\n",
    "                            if any(tld in href for tld in [\".com\", \".sg\", \".net\", \".org\", \".co\"]):\n",
    "                                valid_websites.append(href)\n",
    "                website = valid_websites[0] if valid_websites else None\n",
    "\n",
    "            # Extract social media links from entire page\n",
    "            for a in soup.find_all(\"a\", href=True):\n",
    "                href = a[\"href\"].strip().lower()\n",
    "                if \"facebook.com\" in href and href not in facebook_links:\n",
    "                    facebook_links.append(href)\n",
    "                elif \"linkedin.com\" in href and href not in linkedin_links:\n",
    "                    linkedin_links.append(href)\n",
    "                elif \"instagram.com\" in href and href not in instagram_links:\n",
    "                    instagram_links.append(href)\n",
    "                elif \"tiktok.com\" in href and href not in tiktok_links:\n",
    "                    tiktok_links.append(href)\n",
    "\n",
    "            # Extract registered address\n",
    "            address = None\n",
    "            try:\n",
    "                label_candidates = [\"registered address\", \"registered office address\", \"address\", \"principal place of business\"]\n",
    "                for dt in soup.select(\"dt\"):\n",
    "                    dt_text_lower = dt.get_text(\" \", strip=True).lower()\n",
    "                    if any(lbl in dt_text_lower for lbl in label_candidates):\n",
    "                        dd = dt.find_next_sibling(\"dd\")\n",
    "                        if dd:\n",
    "                            candidate = \" \".join(dd.get_text(\" \", strip=True).split())\n",
    "                            if candidate:\n",
    "                                address = candidate\n",
    "                                break\n",
    "                if not address:\n",
    "                    addr_el = (soup.select_one(\"#address\") or\n",
    "                               soup.select_one(\"[id*='address']\") or\n",
    "                               soup.select_one(\"[aria-labelledby*='address']\"))\n",
    "                    if addr_el:\n",
    "                        candidate = \" \".join(addr_el.get_text(\" \", strip=True).split())\n",
    "                        if candidate:\n",
    "                            address = candidate\n",
    "            except Exception:\n",
    "                address = None\n",
    "\n",
    "            all_results.append({\n",
    "                \"UEN\": uen,\n",
    "                \"Emails\": emails if emails else None,\n",
    "                \"Phones\": phones if phones else None,\n",
    "                \"Website\": website,\n",
    "                \"Facebook\": list(set(facebook_links)) if facebook_links else None,\n",
    "                \"LinkedIn\": list(set(linkedin_links)) if linkedin_links else None,\n",
    "                \"Instagram\": list(set(instagram_links)) if instagram_links else None,\n",
    "                \"TikTok\": list(set(tiktok_links)) if tiktok_links else None,\n",
    "                \"address\": address,\n",
    "                \"RecordOwl_Link\": record_owl_url,\n",
    "            })\n",
    "            \n",
    "            # Print extraction results with actual phone numbers\n",
    "            if phones:\n",
    "                phone_list = \", \".join(phones)\n",
    "                print(f\"  ‚úÖ Extracted: {len(emails) if emails else 0} email(s), {len(phones)} phone(s): {phone_list}\")\n",
    "            else:\n",
    "                print(f\"  ‚úÖ Extracted: {len(emails) if emails else 0} email(s), Phone: None found\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error parsing HTML for {uen}: {e}\")\n",
    "            all_results.append({\n",
    "                \"UEN\": uen,\n",
    "                \"Emails\": None,\n",
    "                \"Phones\": None,\n",
    "                \"Website\": None,\n",
    "                \"Facebook\": None,\n",
    "                \"LinkedIn\": None,\n",
    "                \"Instagram\": None,\n",
    "                \"TikTok\": None,\n",
    "                \"address\": None,\n",
    "                \"RecordOwl_Link\": record_owl_url or None,\n",
    "                \"Error\": f\"HTML parsing error: {str(e)}\"\n",
    "            })\n",
    "\n",
    "    # Dynamic sleep time to avoid rate limiting and 403 blocks (between batches)\n",
    "    base_sleep = 20\n",
    "    random_addition = (batch_num % 10) + 5\n",
    "    sleep_time = base_sleep + random_addition\n",
    "\n",
    "    print(f\"\\n  üí§ Sleeping for {sleep_time}s before next batch...\")\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "    # Extra delay after every 5th batch to further avoid detection\n",
    "    if batch_num % 5 == 0:\n",
    "        extra_wait = 30\n",
    "        print(f\"  üõë Checkpoint pause: waiting extra {extra_wait}s...\")\n",
    "        time.sleep(extra_wait)\n",
    "\n",
    "New_Fresh_Leads = pd.DataFrame(all_results)\n",
    "\n",
    "# Ensure 'address' appears right after 'UEN'\n",
    "if 'address' in New_Fresh_Leads.columns and 'UEN' in New_Fresh_Leads.columns:\n",
    "    cols = list(New_Fresh_Leads.columns)\n",
    "    cols.insert(1, cols.pop(cols.index('address')))\n",
    "    New_Fresh_Leads = New_Fresh_Leads.loc[:, cols]\n",
    "\n",
    "print(\"\\n‚úÖ Scraping complete!\")\n",
    "print(f\"\\nüìä Results summary:\")\n",
    "print(f\"   Total processed: {len(New_Fresh_Leads)}\")\n",
    "print(f\"   With emails: {New_Fresh_Leads['Emails'].notna().sum()}\")\n",
    "print(f\"   With phones: {New_Fresh_Leads['Phones'].notna().sum()}\")\n",
    "print(f\"   With websites: {New_Fresh_Leads['Website'].notna().sum()}\")\n",
    "\n",
    "New_Fresh_Leads.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10eeee3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cef4a99e",
   "metadata": {},
   "source": [
    "### Address Formatting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f7d39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compile patterns for speed\n",
    "POSTAL_RE = re.compile(r\"(?:\\bSingapore\\b\\s*)?(?P<postal>\\d{6})(?!\\d)\", re.IGNORECASE)\n",
    "UNIT_RES = [\n",
    "    re.compile(r\"#\\s*[A-Za-z0-9]{1,4}\\s*[-‚Äì]\\s*[A-Za-z0-9]{1,4}\", re.IGNORECASE),\n",
    "    re.compile(r\"\\bunit\\s*[#:]?\\s*[A-Za-z0-9]{1,4}\\s*[-‚Äì]\\s*[A-Za-z0-9]{1,4}\\b\", re.IGNORECASE),\n",
    "    re.compile(r\"\\bunit\\s*[#:]?\\s*[A-Za-z0-9]{1,5}\\b\", re.IGNORECASE),\n",
    "]\n",
    "\n",
    "def normalize_spaces(text: str) -> str:\n",
    "    text = re.sub(r\"[\\n\\r\\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text.strip(\" ,;|/\")\n",
    "\n",
    "def extract_postal(text: str) -> tuple[str, str | None]:\n",
    "    if not text:\n",
    "        return text, None\n",
    "    matches = list(POSTAL_RE.finditer(text))\n",
    "    if matches:\n",
    "        m = matches[-1]\n",
    "        postal = m.group(\"postal\")\n",
    "        start, end = m.span()\n",
    "        cleaned = text[:start] + text[end:]\n",
    "        cleaned = re.sub(r\"\\bSingapore\\b\", \"\", cleaned, flags=re.IGNORECASE)\n",
    "        return normalize_spaces(cleaned), postal\n",
    "    return normalize_spaces(text), None\n",
    "\n",
    "def extract_unit(text: str) -> tuple[str, str | None]:\n",
    "    if not text:\n",
    "        return text, None\n",
    "    for rx in UNIT_RES:\n",
    "        m = rx.search(text)\n",
    "        if m:\n",
    "            unit_raw = m.group(0)\n",
    "            cleaned = normalize_spaces(text[:m.start()] + text[m.end():])\n",
    "            unit_digits = re.sub(r\"^unit\\s*[#:]?\\s*\", \"\", unit_raw, flags=re.IGNORECASE)\n",
    "            unit_digits = normalize_spaces(unit_digits)\n",
    "            unit_digits = unit_digits.replace(' ‚Äì ', '-').replace('‚Äì', '-').replace(' ', '')\n",
    "            unit_digits = unit_digits.lstrip('#')\n",
    "            return cleaned, unit_digits\n",
    "    return normalize_spaces(text), None\n",
    "\n",
    "def clean_street(text: str) -> str | None:\n",
    "    if not text:\n",
    "        return None\n",
    "    text = normalize_spaces(text)\n",
    "    text = re.sub(r\"\\s*,\\s*\", \", \", text)\n",
    "    return text if text.isupper() else text.title()\n",
    "\n",
    "def split_address_sg(address: str) -> dict:\n",
    "    if not isinstance(address, str) or not address.strip():\n",
    "        return {\"street\": None, \"unit\": None, \"postal_code\": None, \"address_clean\": None}\n",
    "    raw = normalize_spaces(address)\n",
    "    without_postal, postal = extract_postal(raw)\n",
    "    without_unit, unit = extract_unit(without_postal)\n",
    "    without_unit = normalize_spaces(re.sub(r\"\\bSingapore\\b\", \"\", without_unit, flags=re.IGNORECASE))\n",
    "    street = clean_street(without_unit)\n",
    "    address_clean = normalize_spaces(\" \".join(x for x in [street or \"\", unit or \"\", f\"Singapore {postal}\" if postal else \"\"] if x))\n",
    "    return {\"street\": street, \"unit\": unit, \"postal_code\": postal, \"address_clean\": address_clean}\n",
    "\n",
    "# Apply to current result DF -> create a new dataframe with clean components\n",
    "if 'address' not in New_Fresh_Leads.columns:\n",
    "    raise ValueError(\"Column 'address' not found in New_Fresh_Leads. Run the scraping cell first.\")\n",
    "\n",
    "parsed_df = pd.DataFrame(list(New_Fresh_Leads[\"address\"].apply(split_address_sg)))\n",
    "\n",
    "# New DataFrame with clean address fields and without raw 'address'\n",
    "Cleaned_New_Fresh_Leads = New_Fresh_Leads.copy()\n",
    "if 'address' in Cleaned_New_Fresh_Leads.columns:\n",
    "    Cleaned_New_Fresh_Leads = Cleaned_New_Fresh_Leads.drop(columns=['address'])\n",
    "Cleaned_New_Fresh_Leads[\"operational_street\"] = parsed_df[\"street\"]\n",
    "Cleaned_New_Fresh_Leads[\"operational_unit\"] = parsed_df[\"unit\"]\n",
    "Cleaned_New_Fresh_Leads[\"operational_postal_code\"] = parsed_df[\"postal_code\"]\n",
    "Cleaned_New_Fresh_Leads[\"operational_address\"] = parsed_df[\"address_clean\"]\n",
    "\n",
    "# Save full result to a new DataFrame and display all columns\n",
    "New_Fresh_Leads_Operational = Cleaned_New_Fresh_Leads.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3285ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "New_Fresh_Leads_Operational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d354f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New_Fresh_Leads_Operational.to_csv(\"New_Fresh_Leads_Operational.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
