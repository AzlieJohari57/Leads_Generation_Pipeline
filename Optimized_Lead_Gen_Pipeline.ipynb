{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "328e86f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from apify_client import ApifyClient\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from requests.exceptions import HTTPError, ConnectionError\n",
    "from urllib3.exceptions import ProtocolError\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767bf86c",
   "metadata": {},
   "source": [
    "### Getting Master DB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa7f0eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UEN</th>\n",
       "      <th>ACRA_REGISTERED_NAME</th>\n",
       "      <th>BRAND_NAME</th>\n",
       "      <th>SSIC_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04799400B</td>\n",
       "      <td>AIK BEE TEXTILE CO</td>\n",
       "      <td>AIK BEE TEXTILE CO</td>\n",
       "      <td>46411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03376200K</td>\n",
       "      <td>SERANGOON GARDEN CLINIC AND DISPENSARY</td>\n",
       "      <td>GARDEN CLINIC</td>\n",
       "      <td>550263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06239600E</td>\n",
       "      <td>SALON DE BENZIMEN</td>\n",
       "      <td>SALON DE BENZIMEN</td>\n",
       "      <td>96021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06952000C</td>\n",
       "      <td>SU LAN LADIES FASHION</td>\n",
       "      <td>SU LAN LADIES FASHION</td>\n",
       "      <td>14103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10381600C</td>\n",
       "      <td>SIN HAI PRINTING SERVICE</td>\n",
       "      <td>SIN HAI PRINTING SERVICE</td>\n",
       "      <td>18113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7444</th>\n",
       "      <td>201734006N</td>\n",
       "      <td>MISTER MOBILE HOUGANG PTE. LTD.</td>\n",
       "      <td>MISTER MOBILE (HOUGANG)</td>\n",
       "      <td>95120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7445</th>\n",
       "      <td>202210879W</td>\n",
       "      <td>MISTER MOBILE CHINATOWN PTE. LTD.</td>\n",
       "      <td>MISTER MOBILE (CHINATOWN)</td>\n",
       "      <td>47411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7446</th>\n",
       "      <td>202205507G</td>\n",
       "      <td>MISTER MOBILE PTE. LTD.</td>\n",
       "      <td>MISTER MOBILE HQ</td>\n",
       "      <td>64202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7454</th>\n",
       "      <td>53473046M</td>\n",
       "      <td>BLOONIES</td>\n",
       "      <td>BLOONIES</td>\n",
       "      <td>47742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7455</th>\n",
       "      <td>53478373B</td>\n",
       "      <td>BLOOMSNBALLOONS</td>\n",
       "      <td>BLOOMS AND BALLOONS</td>\n",
       "      <td>47742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6734 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             UEN                    ACRA_REGISTERED_NAME  \\\n",
       "0      04799400B                      AIK BEE TEXTILE CO   \n",
       "1      03376200K  SERANGOON GARDEN CLINIC AND DISPENSARY   \n",
       "2      06239600E                       SALON DE BENZIMEN   \n",
       "3      06952000C                   SU LAN LADIES FASHION   \n",
       "4      10381600C                SIN HAI PRINTING SERVICE   \n",
       "...          ...                                     ...   \n",
       "7444  201734006N         MISTER MOBILE HOUGANG PTE. LTD.   \n",
       "7445  202210879W       MISTER MOBILE CHINATOWN PTE. LTD.   \n",
       "7446  202205507G                 MISTER MOBILE PTE. LTD.   \n",
       "7454   53473046M                                BLOONIES   \n",
       "7455   53478373B                         BLOOMSNBALLOONS   \n",
       "\n",
       "                     BRAND_NAME  SSIC_CODE  \n",
       "0            AIK BEE TEXTILE CO      46411  \n",
       "1                 GARDEN CLINIC     550263  \n",
       "2             SALON DE BENZIMEN      96021  \n",
       "3         SU LAN LADIES FASHION      14103  \n",
       "4      SIN HAI PRINTING SERVICE      18113  \n",
       "...                         ...        ...  \n",
       "7444    MISTER MOBILE (HOUGANG)      95120  \n",
       "7445  MISTER MOBILE (CHINATOWN)      47411  \n",
       "7446           MISTER MOBILE HQ      64202  \n",
       "7454                   BLOONIES      47742  \n",
       "7455        BLOOMS AND BALLOONS      47742  \n",
       "\n",
       "[6734 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- CONFIG ---\n",
    "file_path = \"./Master DB/Master_DB_oct22.xlsx\"\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "def clean_uen(u: str) -> str | None:\n",
    "    if pd.isna(u):\n",
    "        return None\n",
    "    return re.sub(r\"[^A-Z0-9]\", \"\", str(u).upper().strip())\n",
    "\n",
    "def clean_text(text: str) -> str | None:\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    text = str(text).strip().upper()\n",
    "    return None if text == \"NAN\" else text\n",
    "\n",
    "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert all column names to uppercase, replace non-alphanumeric with single underscore, remove trailing underscores.\"\"\"\n",
    "    new_cols = []\n",
    "    for col in df.columns:\n",
    "        col_std = re.sub(r\"[^A-Z0-9]\", \"_\", col.upper().strip())\n",
    "        col_std = re.sub(r\"_+\", \"_\", col_std)  # Replace multiple underscores with single\n",
    "        col_std = col_std.strip(\"_\")  # Remove leading/trailing underscores\n",
    "        new_cols.append(col_std)\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "# --- LOAD DATA ---\n",
    "master_db_df = pd.read_excel(file_path)\n",
    "\n",
    "# --- SELECT RELEVANT COLUMNS ---\n",
    "columns_to_keep = [\n",
    "    \"Company Registration Number (UEN)\",\n",
    "    \"ACRA REGISTERED NAME\",\n",
    "    \"Brand/Deal Name/Business Name\",\n",
    "    \"Primary SSIC Code\",\n",
    "    \"PIC NAME 1 Contact Number\",\n",
    "    \"PIC 1 email address\",\n",
    "    \"Website URL\",\n",
    "    \"Parent Industry Type\",\n",
    "    \"Sub Industry\"\n",
    "]\n",
    "master_db_df = master_db_df[columns_to_keep].copy()\n",
    "\n",
    "# --- STANDARDIZE COLUMN NAMES ---\n",
    "master_db_df = standardize_columns(master_db_df)\n",
    "\n",
    "# --- CLEANING & RENAME SPECIFIC COLUMNS ---\n",
    "# Dynamically find the UEN column (first column containing 'UEN')\n",
    "uen_col = [c for c in master_db_df.columns if \"UEN\" in c][0]\n",
    "master_db_df[\"UEN\"] = master_db_df[uen_col].apply(clean_uen)\n",
    "master_db_df = master_db_df.drop(columns=[uen_col])\n",
    "\n",
    "# Rename other columns consistently\n",
    "rename_map = {\n",
    "    \"BRAND_DEAL_NAME_BUSINESS_NAME\": \"BRAND_NAME\",\n",
    "    \"PRIMARY_SSIC_CODE\": \"SSIC_CODE\",\n",
    "    \"ACRA_REGISTERED_NAME\": \"ACRA_REGISTERED_NAME\"\n",
    "}\n",
    "master_db_df = master_db_df.rename(columns={k: v for k, v in rename_map.items() if k in master_db_df.columns})\n",
    "\n",
    "# Clean text columns\n",
    "for col in [\"ACRA_REGISTERED_NAME\", \"BRAND_NAME\"]:\n",
    "    if col in master_db_df.columns:\n",
    "        master_db_df[col] = master_db_df[col].apply(clean_text)\n",
    "\n",
    "# Convert SSIC_CODE to integer if exists\n",
    "if \"SSIC_CODE\" in master_db_df.columns:\n",
    "    master_db_df[\"SSIC_CODE\"] = master_db_df[\"SSIC_CODE\"].astype(\"Int64\")\n",
    "\n",
    "# Keep only required columns if they exist\n",
    "required_cols = [\"UEN\", \"ACRA_REGISTERED_NAME\", \"BRAND_NAME\", \"SSIC_CODE\"]\n",
    "master_db_df = master_db_df[[c for c in required_cols if c in master_db_df.columns]]\n",
    "\n",
    "# Filter out rows with missing or empty UEN\n",
    "master_db_df = master_db_df[master_db_df[\"UEN\"].notna() & (master_db_df[\"UEN\"].str.strip() != \"\")]\n",
    "\n",
    "master_db_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e98cff",
   "metadata": {},
   "source": [
    "### Getting ACRA Data (Filter by Live, Live Company only & non relevant ssic code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e678fbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Folder containing your CSVs\n",
    "# -------------------------------------------------------------\n",
    "folder_path = \"Acra_Data\"\n",
    "\n",
    "# Get all CSV file paths inside the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "# Read and combine all CSVs\n",
    "# Using low_memory=False to avoid DtypeWarning for mixed types\n",
    "df = pd.concat((pd.read_csv(f, low_memory=False) for f in csv_files), ignore_index=True)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Convert all column names to uppercase\n",
    "# -------------------------------------------------------------\n",
    "df.columns = df.columns.str.upper()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Select relevant columns (now in uppercase)\n",
    "# -------------------------------------------------------------\n",
    "acra_data = df[[\n",
    "    \"UEN\",\n",
    "    \"ENTITY_NAME\",\n",
    "    \"BUSINESS_CONSTITUTION_DESCRIPTION\",\n",
    "    \"ENTITY_TYPE_DESCRIPTION\",\n",
    "    \"ENTITY_STATUS_DESCRIPTION\",\n",
    "    \"REGISTRATION_INCORPORATION_DATE\",\n",
    "    \"PRIMARY_SSIC_CODE\",\n",
    "    \"STREET_NAME\",\n",
    "    \"POSTAL_CODE\"\n",
    "]].copy()\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Convert to proper data types\n",
    "# -------------------------------------------------------------\n",
    "acra_data['UEN'] = acra_data['UEN'].astype('string')\n",
    "acra_data['ENTITY_NAME'] = acra_data['ENTITY_NAME'].astype('string')\n",
    "acra_data['BUSINESS_CONSTITUTION_DESCRIPTION'] = acra_data['BUSINESS_CONSTITUTION_DESCRIPTION'].astype('string')\n",
    "acra_data['ENTITY_TYPE_DESCRIPTION'] = acra_data['ENTITY_TYPE_DESCRIPTION'].astype('string')\n",
    "acra_data['ENTITY_STATUS_DESCRIPTION'] = acra_data['ENTITY_STATUS_DESCRIPTION'].astype('string')\n",
    "acra_data['REGISTRATION_INCORPORATION_DATE'] = pd.to_datetime(acra_data['REGISTRATION_INCORPORATION_DATE'], errors='coerce')\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Clean string columns — trim, remove extra spaces, uppercase\n",
    "# -------------------------------------------------------------\n",
    "for col in [\n",
    "    'UEN',\n",
    "    'ENTITY_NAME',\n",
    "    'BUSINESS_CONSTITUTION_DESCRIPTION',\n",
    "    'ENTITY_TYPE_DESCRIPTION',\n",
    "    'ENTITY_STATUS_DESCRIPTION',\n",
    "    'STREET_NAME',\n",
    "    'POSTAL_CODE'\n",
    "]:\n",
    "    acra_data[col] = (\n",
    "        acra_data[col]\n",
    "        .fillna('')\n",
    "        .str.strip()\n",
    "        .str.replace(r'\\s+', ' ', regex=True)\n",
    "        .str.upper()\n",
    "    )\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Replace placeholders with NaN for standardization\n",
    "# -------------------------------------------------------------\n",
    "acra_data.replace(['NA', 'N/A', '-', ''], np.nan, inplace=True)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Convert registration date to dd-mm-yyyy string (optional)\n",
    "# -------------------------------------------------------------\n",
    "acra_data['REGISTRATION_INCORPORATION_DATE'] = acra_data['REGISTRATION_INCORPORATION_DATE'].dt.strftime('%d-%m-%Y')\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Filter only live entities (LIVE COMPANY or LIVE)\n",
    "# -------------------------------------------------------------\n",
    "acra_data = acra_data[\n",
    "    acra_data['ENTITY_STATUS_DESCRIPTION'].isin(['LIVE COMPANY', 'LIVE'])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Exclude specific PRIMARY_SSIC_CODE values (supposedly the data would be 600k plus but when we exclude this would lessen)\n",
    "# -------------------------------------------------------------\n",
    "exclude_codes = [\n",
    "    46900, 47719, 47749, 47539, 47536, 56123,\n",
    "    10711, 10712, 10719, 10732, 10733, 93209\n",
    "]\n",
    "\n",
    "acra_data = acra_data[~acra_data['PRIMARY_SSIC_CODE'].isin(exclude_codes)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37b264bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UEN</th>\n",
       "      <th>ENTITY_NAME</th>\n",
       "      <th>BUSINESS_CONSTITUTION_DESCRIPTION</th>\n",
       "      <th>ENTITY_TYPE_DESCRIPTION</th>\n",
       "      <th>ENTITY_STATUS_DESCRIPTION</th>\n",
       "      <th>REGISTRATION_INCORPORATION_DATE</th>\n",
       "      <th>PRIMARY_SSIC_CODE</th>\n",
       "      <th>STREET_NAME</th>\n",
       "      <th>POSTAL_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00182000A</td>\n",
       "      <td>AIK SENG HENG</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>07-02-1975</td>\n",
       "      <td>46302</td>\n",
       "      <td>FISHERY PORT ROAD</td>\n",
       "      <td>619742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00233500W</td>\n",
       "      <td>ASIA STORE</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>28-10-1974</td>\n",
       "      <td>46411</td>\n",
       "      <td>SIMS AVENUE</td>\n",
       "      <td>387509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00733000J</td>\n",
       "      <td>AIK CHE HIONG</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>02-11-1974</td>\n",
       "      <td>32909</td>\n",
       "      <td>ANG MO KIO INDUSTRIAL PARK 2A</td>\n",
       "      <td>568049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00927000X</td>\n",
       "      <td>A WALIMOHAMED BROS</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>12-11-1974</td>\n",
       "      <td>46411</td>\n",
       "      <td>JELLICOE ROAD</td>\n",
       "      <td>208767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01173000E</td>\n",
       "      <td>ANG TECK MOH DEPARTMENT STORE</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>30-10-1974</td>\n",
       "      <td>47711</td>\n",
       "      <td>WOODLANDS STREET 12</td>\n",
       "      <td>738623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537323</th>\n",
       "      <td>T25LL0518K</td>\n",
       "      <td>ZEUS BARBERS LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>16-05-2025</td>\n",
       "      <td>96021</td>\n",
       "      <td>KELANTAN LANE</td>\n",
       "      <td>200031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537324</th>\n",
       "      <td>T25LL0858C</td>\n",
       "      <td>ZENSE SPACE LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>01-08-2025</td>\n",
       "      <td>43301</td>\n",
       "      <td>YISHUN INDUSTRIAL STREET 1</td>\n",
       "      <td>768161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537325</th>\n",
       "      <td>T25LL0870A</td>\n",
       "      <td>ZIQZEQ PROCUREMENT LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>04-08-2025</td>\n",
       "      <td>70209</td>\n",
       "      <td>SIN MING LANE</td>\n",
       "      <td>573969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537326</th>\n",
       "      <td>T25LL1049B</td>\n",
       "      <td>ZHONG XIN TRAVEL LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>08-09-2025</td>\n",
       "      <td>79102</td>\n",
       "      <td>JALAN BAHAGIA</td>\n",
       "      <td>320034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537327</th>\n",
       "      <td>T25LL1066B</td>\n",
       "      <td>ZDT DRIVES LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>14-09-2025</td>\n",
       "      <td>47533</td>\n",
       "      <td>FERNVALE ROAD</td>\n",
       "      <td>792466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>537328 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               UEN                    ENTITY_NAME  \\\n",
       "0        00182000A                  AIK SENG HENG   \n",
       "1        00233500W                     ASIA STORE   \n",
       "2        00733000J                  AIK CHE HIONG   \n",
       "3        00927000X             A WALIMOHAMED BROS   \n",
       "4        01173000E  ANG TECK MOH DEPARTMENT STORE   \n",
       "...            ...                            ...   \n",
       "537323  T25LL0518K               ZEUS BARBERS LLP   \n",
       "537324  T25LL0858C                ZENSE SPACE LLP   \n",
       "537325  T25LL0870A         ZIQZEQ PROCUREMENT LLP   \n",
       "537326  T25LL1049B           ZHONG XIN TRAVEL LLP   \n",
       "537327  T25LL1066B                 ZDT DRIVES LLP   \n",
       "\n",
       "       BUSINESS_CONSTITUTION_DESCRIPTION           ENTITY_TYPE_DESCRIPTION  \\\n",
       "0                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "1                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "2                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "3                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "4                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "...                                  ...                               ...   \n",
       "537323                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "537324                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "537325                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "537326                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "537327                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "\n",
       "       ENTITY_STATUS_DESCRIPTION REGISTRATION_INCORPORATION_DATE  \\\n",
       "0                           LIVE                      07-02-1975   \n",
       "1                           LIVE                      28-10-1974   \n",
       "2                           LIVE                      02-11-1974   \n",
       "3                           LIVE                      12-11-1974   \n",
       "4                           LIVE                      30-10-1974   \n",
       "...                          ...                             ...   \n",
       "537323                      LIVE                      16-05-2025   \n",
       "537324                      LIVE                      01-08-2025   \n",
       "537325                      LIVE                      04-08-2025   \n",
       "537326                      LIVE                      08-09-2025   \n",
       "537327                      LIVE                      14-09-2025   \n",
       "\n",
       "        PRIMARY_SSIC_CODE                    STREET_NAME POSTAL_CODE  \n",
       "0                   46302              FISHERY PORT ROAD      619742  \n",
       "1                   46411                    SIMS AVENUE      387509  \n",
       "2                   32909  ANG MO KIO INDUSTRIAL PARK 2A      568049  \n",
       "3                   46411                  JELLICOE ROAD      208767  \n",
       "4                   47711            WOODLANDS STREET 12      738623  \n",
       "...                   ...                            ...         ...  \n",
       "537323              96021                  KELANTAN LANE      200031  \n",
       "537324              43301     YISHUN INDUSTRIAL STREET 1      768161  \n",
       "537325              70209                  SIN MING LANE      573969  \n",
       "537326              79102                  JALAN BAHAGIA      320034  \n",
       "537327              47533                  FERNVALE ROAD      792466  \n",
       "\n",
       "[537328 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acra_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec969fc",
   "metadata": {},
   "source": [
    "### Getting SSIC Industry code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f4bc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PARENT_INDUSTRY</th>\n",
       "      <th>INDUSTRY_TYPE</th>\n",
       "      <th>SUB_INDUSTRY</th>\n",
       "      <th>SSIC_CODES</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Retail</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Fashion &amp; Apparel</td>\n",
       "      <td>47711</td>\n",
       "      <td>Retail Sale Of Clothing For Adults</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Retail</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Fashion &amp; Apparel</td>\n",
       "      <td>47712</td>\n",
       "      <td>Retail Sale Of Children And Infants' Clothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Retail</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Fashion &amp; Apparel</td>\n",
       "      <td>47715</td>\n",
       "      <td>Retail Sale Of Sewing And Clothing Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Retail</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Fashion &amp; Apparel</td>\n",
       "      <td>47719</td>\n",
       "      <td>Retail Sale Of Clothing, Footwear And Leather ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Retail</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Fashion &amp; Apparel</td>\n",
       "      <td>47510</td>\n",
       "      <td>Retail Sale Of Textiles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PARENT_INDUSTRY INDUSTRY_TYPE       SUB_INDUSTRY  SSIC_CODES  \\\n",
       "0          Retail        Retail  Fashion & Apparel       47711   \n",
       "1          Retail        Retail  Fashion & Apparel       47712   \n",
       "2          Retail        Retail  Fashion & Apparel       47715   \n",
       "3          Retail        Retail  Fashion & Apparel       47719   \n",
       "4          Retail        Retail  Fashion & Apparel       47510   \n",
       "\n",
       "                                         DESCRIPTION  \n",
       "0                 Retail Sale Of Clothing For Adults  \n",
       "1      Retail Sale Of Children And Infants' Clothing  \n",
       "2     Retail Sale Of Sewing And Clothing Accessories  \n",
       "3  Retail Sale Of Clothing, Footwear And Leather ...  \n",
       "4                            Retail Sale Of Textiles  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- CONFIG ---\n",
    "file_path = \"./SSIC_Code/mapped_ssic_code.xlsx\"\n",
    "\n",
    "# --- LOAD DATA ---\n",
    "mapped_ssic_code = pd.read_excel(file_path)\n",
    "\n",
    "# --- STANDARDIZE COLUMN NAMES ---\n",
    "# Uppercase, strip spaces, replace spaces with underscores\n",
    "mapped_ssic_code.columns = (\n",
    "    mapped_ssic_code.columns\n",
    "    .str.strip()\n",
    "    .str.upper()\n",
    "    .str.replace(\" \", \"_\")\n",
    ")\n",
    "\n",
    "# --- KEEP ONLY DESIRED COLUMNS ---\n",
    "columns_to_keep = [\"PARENT_INDUSTRY\", \"INDUSTRY_TYPE\", \"SUB_INDUSTRY\", \"SSIC_CODES\", \"DESCRIPTION\"]\n",
    "mapped_ssic_code = mapped_ssic_code[columns_to_keep].copy()\n",
    "\n",
    "# --- CLEAN SSIC_CODES COLUMN ---\n",
    "mapped_ssic_code[\"SSIC_CODES\"] = (\n",
    "    pd.to_numeric(mapped_ssic_code[\"SSIC_CODES\"], errors=\"coerce\")  # safely convert to numeric\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# --- CLEAN TEXT COLUMNS ---\n",
    "text_cols = [\"PARENT_INDUSTRY\", \"INDUSTRY_TYPE\", \"SUB_INDUSTRY\", \"DESCRIPTION\"]\n",
    "mapped_ssic_code[text_cols] = mapped_ssic_code[text_cols].apply(\n",
    "    lambda col: col.astype(str).str.strip().str.title()\n",
    ")\n",
    "\n",
    "# --- REMOVE DUPLICATES & RESET INDEX ---\n",
    "mapped_ssic_code = mapped_ssic_code.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "mapped_ssic_code.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac763a73",
   "metadata": {},
   "source": [
    "### Merge ACRA data with SSIC code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e62740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PRIMARY_SSIC_CODE to int\n",
    "acra_data[\"PRIMARY_SSIC_CODE\"] = (\n",
    "    pd.to_numeric(acra_data[\"PRIMARY_SSIC_CODE\"], errors=\"coerce\")\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Merge based on SSIC code\n",
    "acra_data_filtered = acra_data.merge(\n",
    "    mapped_ssic_code,\n",
    "    how=\"left\",\n",
    "    left_on=\"PRIMARY_SSIC_CODE\",\n",
    "    right_on=\"SSIC_CODES\"\n",
    ")\n",
    "\n",
    "# Optional: drop the duplicate 'SSIC CODES' column (keep only PRIMARY_SSIC_CODE)\n",
    "acra_data_filtered = acra_data_filtered.drop(columns=[\"SSIC_CODES\"], errors=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ede8f881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UEN</th>\n",
       "      <th>ENTITY_NAME</th>\n",
       "      <th>BUSINESS_CONSTITUTION_DESCRIPTION</th>\n",
       "      <th>ENTITY_TYPE_DESCRIPTION</th>\n",
       "      <th>ENTITY_STATUS_DESCRIPTION</th>\n",
       "      <th>REGISTRATION_INCORPORATION_DATE</th>\n",
       "      <th>PRIMARY_SSIC_CODE</th>\n",
       "      <th>STREET_NAME</th>\n",
       "      <th>POSTAL_CODE</th>\n",
       "      <th>PARENT_INDUSTRY</th>\n",
       "      <th>INDUSTRY_TYPE</th>\n",
       "      <th>SUB_INDUSTRY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00182000A</td>\n",
       "      <td>AIK SENG HENG</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>07-02-1975</td>\n",
       "      <td>46302</td>\n",
       "      <td>FISHERY PORT ROAD</td>\n",
       "      <td>619742</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Food, Beverages &amp; Tobacco</td>\n",
       "      <td>Wholesale Of Livestock, Meat, Poultry, Eggs An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00233500W</td>\n",
       "      <td>ASIA STORE</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>28-10-1974</td>\n",
       "      <td>46411</td>\n",
       "      <td>SIMS AVENUE</td>\n",
       "      <td>387509</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Household Goods</td>\n",
       "      <td>Wholesale Of Textiles And Leathers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00733000J</td>\n",
       "      <td>AIK CHE HIONG</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>02-11-1974</td>\n",
       "      <td>32909</td>\n",
       "      <td>ANG MO KIO INDUSTRIAL PARK 2A</td>\n",
       "      <td>568049</td>\n",
       "      <td>Others</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Other Specialised Manufacturing &amp; Distribution</td>\n",
       "      <td>Other Manufacturing Industries N.E.C.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00927000X</td>\n",
       "      <td>A WALIMOHAMED BROS</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>12-11-1974</td>\n",
       "      <td>46411</td>\n",
       "      <td>JELLICOE ROAD</td>\n",
       "      <td>208767</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Household Goods</td>\n",
       "      <td>Wholesale Of Textiles And Leathers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01173000E</td>\n",
       "      <td>ANG TECK MOH DEPARTMENT STORE</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>30-10-1974</td>\n",
       "      <td>47711</td>\n",
       "      <td>WOODLANDS STREET 12</td>\n",
       "      <td>738623</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Fashion &amp; Apparel</td>\n",
       "      <td>Retail Sale Of Clothing For Adults</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537323</th>\n",
       "      <td>T25LL0518K</td>\n",
       "      <td>ZEUS BARBERS LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>16-05-2025</td>\n",
       "      <td>96021</td>\n",
       "      <td>KELANTAN LANE</td>\n",
       "      <td>200031</td>\n",
       "      <td>Services</td>\n",
       "      <td>Services</td>\n",
       "      <td>Hair Salons &amp; Barbershops</td>\n",
       "      <td>Hairdressing Salons/Shops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537324</th>\n",
       "      <td>T25LL0858C</td>\n",
       "      <td>ZENSE SPACE LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>01-08-2025</td>\n",
       "      <td>43301</td>\n",
       "      <td>YISHUN INDUSTRIAL STREET 1</td>\n",
       "      <td>768161</td>\n",
       "      <td>Others</td>\n",
       "      <td>Built Environment &amp; Infrastructure</td>\n",
       "      <td>Construction</td>\n",
       "      <td>Renovation Contractors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537325</th>\n",
       "      <td>T25LL0870A</td>\n",
       "      <td>ZIQZEQ PROCUREMENT LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>04-08-2025</td>\n",
       "      <td>70209</td>\n",
       "      <td>SIN MING LANE</td>\n",
       "      <td>573969</td>\n",
       "      <td>Others</td>\n",
       "      <td>Finance, Legal &amp; Real Estate</td>\n",
       "      <td>Legal, Accounting &amp; Consultancy Activities</td>\n",
       "      <td>Management Consultancy Services N.E.C.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537326</th>\n",
       "      <td>T25LL1049B</td>\n",
       "      <td>ZHONG XIN TRAVEL LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>08-09-2025</td>\n",
       "      <td>79102</td>\n",
       "      <td>JALAN BAHAGIA</td>\n",
       "      <td>320034</td>\n",
       "      <td>Others</td>\n",
       "      <td>Tourism, Agency</td>\n",
       "      <td>Travel Agencies &amp; Tour Operators</td>\n",
       "      <td>Travel Agencies And Tour Operators (Mainly Out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537327</th>\n",
       "      <td>T25LL1066B</td>\n",
       "      <td>ZDT DRIVES LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>14-09-2025</td>\n",
       "      <td>47533</td>\n",
       "      <td>FERNVALE ROAD</td>\n",
       "      <td>792466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>537328 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               UEN                    ENTITY_NAME  \\\n",
       "0        00182000A                  AIK SENG HENG   \n",
       "1        00233500W                     ASIA STORE   \n",
       "2        00733000J                  AIK CHE HIONG   \n",
       "3        00927000X             A WALIMOHAMED BROS   \n",
       "4        01173000E  ANG TECK MOH DEPARTMENT STORE   \n",
       "...            ...                            ...   \n",
       "537323  T25LL0518K               ZEUS BARBERS LLP   \n",
       "537324  T25LL0858C                ZENSE SPACE LLP   \n",
       "537325  T25LL0870A         ZIQZEQ PROCUREMENT LLP   \n",
       "537326  T25LL1049B           ZHONG XIN TRAVEL LLP   \n",
       "537327  T25LL1066B                 ZDT DRIVES LLP   \n",
       "\n",
       "       BUSINESS_CONSTITUTION_DESCRIPTION           ENTITY_TYPE_DESCRIPTION  \\\n",
       "0                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "1                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "2                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "3                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "4                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "...                                  ...                               ...   \n",
       "537323                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "537324                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "537325                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "537326                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "537327                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "\n",
       "       ENTITY_STATUS_DESCRIPTION REGISTRATION_INCORPORATION_DATE  \\\n",
       "0                           LIVE                      07-02-1975   \n",
       "1                           LIVE                      28-10-1974   \n",
       "2                           LIVE                      02-11-1974   \n",
       "3                           LIVE                      12-11-1974   \n",
       "4                           LIVE                      30-10-1974   \n",
       "...                          ...                             ...   \n",
       "537323                      LIVE                      16-05-2025   \n",
       "537324                      LIVE                      01-08-2025   \n",
       "537325                      LIVE                      04-08-2025   \n",
       "537326                      LIVE                      08-09-2025   \n",
       "537327                      LIVE                      14-09-2025   \n",
       "\n",
       "        PRIMARY_SSIC_CODE                    STREET_NAME POSTAL_CODE  \\\n",
       "0                   46302              FISHERY PORT ROAD      619742   \n",
       "1                   46411                    SIMS AVENUE      387509   \n",
       "2                   32909  ANG MO KIO INDUSTRIAL PARK 2A      568049   \n",
       "3                   46411                  JELLICOE ROAD      208767   \n",
       "4                   47711            WOODLANDS STREET 12      738623   \n",
       "...                   ...                            ...         ...   \n",
       "537323              96021                  KELANTAN LANE      200031   \n",
       "537324              43301     YISHUN INDUSTRIAL STREET 1      768161   \n",
       "537325              70209                  SIN MING LANE      573969   \n",
       "537326              79102                  JALAN BAHAGIA      320034   \n",
       "537327              47533                  FERNVALE ROAD      792466   \n",
       "\n",
       "       PARENT_INDUSTRY                       INDUSTRY_TYPE  \\\n",
       "0               Others                     Wholesale Trade   \n",
       "1               Others                     Wholesale Trade   \n",
       "2               Others                       Manufacturing   \n",
       "3               Others                     Wholesale Trade   \n",
       "4               Retail                              Retail   \n",
       "...                ...                                 ...   \n",
       "537323        Services                            Services   \n",
       "537324          Others  Built Environment & Infrastructure   \n",
       "537325          Others        Finance, Legal & Real Estate   \n",
       "537326          Others                     Tourism, Agency   \n",
       "537327             NaN                                 NaN   \n",
       "\n",
       "                                          SUB_INDUSTRY  \\\n",
       "0                            Food, Beverages & Tobacco   \n",
       "1                                      Household Goods   \n",
       "2       Other Specialised Manufacturing & Distribution   \n",
       "3                                      Household Goods   \n",
       "4                                    Fashion & Apparel   \n",
       "...                                                ...   \n",
       "537323                       Hair Salons & Barbershops   \n",
       "537324                                    Construction   \n",
       "537325      Legal, Accounting & Consultancy Activities   \n",
       "537326                Travel Agencies & Tour Operators   \n",
       "537327                                             NaN   \n",
       "\n",
       "                                              DESCRIPTION  \n",
       "0       Wholesale Of Livestock, Meat, Poultry, Eggs An...  \n",
       "1                      Wholesale Of Textiles And Leathers  \n",
       "2                   Other Manufacturing Industries N.E.C.  \n",
       "3                      Wholesale Of Textiles And Leathers  \n",
       "4                      Retail Sale Of Clothing For Adults  \n",
       "...                                                   ...  \n",
       "537323                          Hairdressing Salons/Shops  \n",
       "537324                             Renovation Contractors  \n",
       "537325             Management Consultancy Services N.E.C.  \n",
       "537326  Travel Agencies And Tour Operators (Mainly Out...  \n",
       "537327                                                NaN  \n",
       "\n",
       "[537328 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acra_data_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9289e8b3",
   "metadata": {},
   "source": [
    "### FIlter Acra data with Master DB to get list of companies havent been researched  by MR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c28478d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(533824, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Ensure both UEN columns are strings for accurate matching\n",
    "acra_data_filtered['UEN'] = acra_data_filtered['UEN'].astype(str).str.strip().str.upper()\n",
    "master_db_df['UEN'] = master_db_df['UEN'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Filter out rows in acra_data_filtered whose UEN is already in master_db_df\n",
    "acra_data_filtered = acra_data_filtered[~acra_data_filtered['UEN'].isin(master_db_df['UEN'])]\n",
    "\n",
    "acra_data_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bd24ab",
   "metadata": {},
   "source": [
    "### Filter by  Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa058f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UEN</th>\n",
       "      <th>ENTITY_NAME</th>\n",
       "      <th>BUSINESS_CONSTITUTION_DESCRIPTION</th>\n",
       "      <th>ENTITY_TYPE_DESCRIPTION</th>\n",
       "      <th>ENTITY_STATUS_DESCRIPTION</th>\n",
       "      <th>REGISTRATION_INCORPORATION_DATE</th>\n",
       "      <th>PRIMARY_SSIC_CODE</th>\n",
       "      <th>STREET_NAME</th>\n",
       "      <th>POSTAL_CODE</th>\n",
       "      <th>PARENT_INDUSTRY</th>\n",
       "      <th>INDUSTRY_TYPE</th>\n",
       "      <th>SUB_INDUSTRY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00182000A</td>\n",
       "      <td>AIK SENG HENG</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>07-02-1975</td>\n",
       "      <td>46302</td>\n",
       "      <td>FISHERY PORT ROAD</td>\n",
       "      <td>619742</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Food, Beverages &amp; Tobacco</td>\n",
       "      <td>Wholesale Of Livestock, Meat, Poultry, Eggs An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00233500W</td>\n",
       "      <td>ASIA STORE</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>28-10-1974</td>\n",
       "      <td>46411</td>\n",
       "      <td>SIMS AVENUE</td>\n",
       "      <td>387509</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Household Goods</td>\n",
       "      <td>Wholesale Of Textiles And Leathers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00927000X</td>\n",
       "      <td>A WALIMOHAMED BROS</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>12-11-1974</td>\n",
       "      <td>46411</td>\n",
       "      <td>JELLICOE ROAD</td>\n",
       "      <td>208767</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Household Goods</td>\n",
       "      <td>Wholesale Of Textiles And Leathers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>04129500E</td>\n",
       "      <td>AIK HOE &amp; CO</td>\n",
       "      <td>SOLE-PROPRIETOR</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>23-01-1975</td>\n",
       "      <td>46551</td>\n",
       "      <td>KELANTAN ROAD</td>\n",
       "      <td>200028</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Machinery, Equipment &amp; Supplies</td>\n",
       "      <td>Wholesale Of Marine Equipment And Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>04545400X</td>\n",
       "      <td>AIK HUAT AND COMPANY</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>17-01-1975</td>\n",
       "      <td>46441</td>\n",
       "      <td>KAKI BUKIT AVENUE 1</td>\n",
       "      <td>417943</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Household Goods</td>\n",
       "      <td>Wholesale Of Sporting Goods And Equipment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537268</th>\n",
       "      <td>T17LP0162L</td>\n",
       "      <td>ZYA HOLDINGS LIMITED PARTNERSHIP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>21-10-2017</td>\n",
       "      <td>46100</td>\n",
       "      <td>NATHAN ROAD</td>\n",
       "      <td>248728</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Other Specialised Wholesale</td>\n",
       "      <td>Wholesale On A Fee Or Commission Basis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537298</th>\n",
       "      <td>T22LL0564C</td>\n",
       "      <td>ZEN ENGINEERING &amp; TRADING LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>31-05-2022</td>\n",
       "      <td>46543</td>\n",
       "      <td>TOH GUAN ROAD EAST</td>\n",
       "      <td>608586</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Machinery, Equipment &amp; Supplies</td>\n",
       "      <td>Wholesale Of Lifts, Escalators And Industrial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537302</th>\n",
       "      <td>T23LL0056G</td>\n",
       "      <td>ZECRYNE LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>13-01-2023</td>\n",
       "      <td>46301</td>\n",
       "      <td>BUKIT BATOK STREET 25</td>\n",
       "      <td>658881</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Food, Beverages &amp; Tobacco</td>\n",
       "      <td>Wholesale Of Fruits And Vegetables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537313</th>\n",
       "      <td>T24LL0528K</td>\n",
       "      <td>ZOHMH LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>07-05-2024</td>\n",
       "      <td>46303</td>\n",
       "      <td>WOODLANDS AVENUE 4</td>\n",
       "      <td>730844</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Food, Beverages &amp; Tobacco</td>\n",
       "      <td>Wholesale Of A General Line Of Groceries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537318</th>\n",
       "      <td>T24LL1189H</td>\n",
       "      <td>Z CONNECT LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>30-10-2024</td>\n",
       "      <td>46436</td>\n",
       "      <td>GAMBAS CRESCENT</td>\n",
       "      <td>757087</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Household Goods</td>\n",
       "      <td>Wholesale Of Audio And Video Equipment (Except...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38428 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               UEN                          ENTITY_NAME  \\\n",
       "0        00182000A                        AIK SENG HENG   \n",
       "1        00233500W                           ASIA STORE   \n",
       "3        00927000X                   A WALIMOHAMED BROS   \n",
       "12       04129500E                         AIK HOE & CO   \n",
       "14       04545400X                 AIK HUAT AND COMPANY   \n",
       "...            ...                                  ...   \n",
       "537268  T17LP0162L     ZYA HOLDINGS LIMITED PARTNERSHIP   \n",
       "537298  T22LL0564C        ZEN ENGINEERING & TRADING LLP   \n",
       "537302  T23LL0056G                          ZECRYNE LLP   \n",
       "537313  T24LL0528K  ZOHMH LIMITED LIABILITY PARTNERSHIP   \n",
       "537318  T24LL1189H                        Z CONNECT LLP   \n",
       "\n",
       "       BUSINESS_CONSTITUTION_DESCRIPTION           ENTITY_TYPE_DESCRIPTION  \\\n",
       "0                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "1                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "3                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "12                       SOLE-PROPRIETOR  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "14                           PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "...                                  ...                               ...   \n",
       "537268                              <NA>               LIMITED PARTNERSHIP   \n",
       "537298                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "537302                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "537313                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "537318                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "\n",
       "       ENTITY_STATUS_DESCRIPTION REGISTRATION_INCORPORATION_DATE  \\\n",
       "0                           LIVE                      07-02-1975   \n",
       "1                           LIVE                      28-10-1974   \n",
       "3                           LIVE                      12-11-1974   \n",
       "12                          LIVE                      23-01-1975   \n",
       "14                          LIVE                      17-01-1975   \n",
       "...                          ...                             ...   \n",
       "537268                      LIVE                      21-10-2017   \n",
       "537298                      LIVE                      31-05-2022   \n",
       "537302                      LIVE                      13-01-2023   \n",
       "537313                      LIVE                      07-05-2024   \n",
       "537318                      LIVE                      30-10-2024   \n",
       "\n",
       "        PRIMARY_SSIC_CODE            STREET_NAME POSTAL_CODE PARENT_INDUSTRY  \\\n",
       "0                   46302      FISHERY PORT ROAD      619742          Others   \n",
       "1                   46411            SIMS AVENUE      387509          Others   \n",
       "3                   46411          JELLICOE ROAD      208767          Others   \n",
       "12                  46551          KELANTAN ROAD      200028          Others   \n",
       "14                  46441    KAKI BUKIT AVENUE 1      417943          Others   \n",
       "...                   ...                    ...         ...             ...   \n",
       "537268              46100            NATHAN ROAD      248728          Others   \n",
       "537298              46543     TOH GUAN ROAD EAST      608586          Others   \n",
       "537302              46301  BUKIT BATOK STREET 25      658881          Others   \n",
       "537313              46303     WOODLANDS AVENUE 4      730844          Others   \n",
       "537318              46436        GAMBAS CRESCENT      757087          Others   \n",
       "\n",
       "          INDUSTRY_TYPE                     SUB_INDUSTRY  \\\n",
       "0       Wholesale Trade        Food, Beverages & Tobacco   \n",
       "1       Wholesale Trade                  Household Goods   \n",
       "3       Wholesale Trade                  Household Goods   \n",
       "12      Wholesale Trade  Machinery, Equipment & Supplies   \n",
       "14      Wholesale Trade                  Household Goods   \n",
       "...                 ...                              ...   \n",
       "537268  Wholesale Trade      Other Specialised Wholesale   \n",
       "537298  Wholesale Trade  Machinery, Equipment & Supplies   \n",
       "537302  Wholesale Trade        Food, Beverages & Tobacco   \n",
       "537313  Wholesale Trade        Food, Beverages & Tobacco   \n",
       "537318  Wholesale Trade                  Household Goods   \n",
       "\n",
       "                                              DESCRIPTION  \n",
       "0       Wholesale Of Livestock, Meat, Poultry, Eggs An...  \n",
       "1                      Wholesale Of Textiles And Leathers  \n",
       "3                      Wholesale Of Textiles And Leathers  \n",
       "12          Wholesale Of Marine Equipment And Accessories  \n",
       "14              Wholesale Of Sporting Goods And Equipment  \n",
       "...                                                   ...  \n",
       "537268             Wholesale On A Fee Or Commission Basis  \n",
       "537298  Wholesale Of Lifts, Escalators And Industrial ...  \n",
       "537302                 Wholesale Of Fruits And Vegetables  \n",
       "537313           Wholesale Of A General Line Of Groceries  \n",
       "537318  Wholesale Of Audio And Video Equipment (Except...  \n",
       "\n",
       "[38428 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wholesale data\n",
    "ssic_codes = [\n",
    "    \"46\", \"461\", \"4610\", \"46100\", \"462\", \"4621\", \"46211\", \"46212\", \"46213\", \"46219\",\n",
    "    \"4622\", \"46221\", \"46222\", \"46223\", \"46224\", \"46225\", \"46229\", \"463\", \"4630\", \"46301\",\n",
    "    \"46302\", \"46303\", \"46304\", \"46305\", \"46306\", \"46307\", \"46308\", \"46309\", \"464\", \"4641\",\n",
    "    \"46411\", \"46412\", \"46413\", \"46414\", \"46415\", \"46416\", \"4642\", \"46421\", \"46422\", \"46423\",\n",
    "    \"46424\", \"46429\", \"4643\", \"46431\", \"46432\", \"46433\", \"46434\", \"46435\", \"46436\", \"46439\",\n",
    "    \"4644\", \"46441\", \"46442\", \"46443\", \"46444\", \"46445\", \"46449\", \"4645\", \"46451\", \"46452\",\n",
    "    \"46453\", \"46459\", \"4646\", \"46461\", \"46462\", \"4647\", \"46471\", \"46472\", \"46473\", \"46474\",\n",
    "    \"46479\", \"4649\", \"46491\", \"46492\", \"46499\", \"465\", \"4651\", \"46511\", \"46512\", \"46513\",\n",
    "    \"46514\", \"4652\", \"46521\", \"46522\", \"46523\", \"4653\", \"46530\", \"4654\", \"46541\", \"46542\",\n",
    "    \"46543\", \"46544\", \"46549\", \"4655\", \"46551\", \"46552\", \"46559\", \"4656\", \"46561\", \"46562\",\n",
    "    \"46563\", \"4659\", \"46591\", \"46592\", \"46593\", \"46594\", \"46595\", \"46599\", \"466\", \"4661\",\n",
    "    \"46610\", \"4662\", \"46620\", \"4663\", \"46631\", \"46632\", \"46633\", \"46634\", \"46635\", \"46639\",\n",
    "    \"4664\", \"46641\", \"46642\", \"46643\", \"46649\", \"4665\", \"46651\", \"46659\", \"4666\", \"46661\",\n",
    "    \"46662\", \"469\", \"4690\", \"46900\"\n",
    "]\n",
    "\n",
    "\n",
    "acra_data_filtered_by_industry = acra_data_filtered[\n",
    "    (\n",
    "        (acra_data_filtered[\"ENTITY_STATUS_DESCRIPTION\"].str.lower() == \"live\") |\n",
    "        (acra_data_filtered[\"ENTITY_STATUS_DESCRIPTION\"].str.lower() == \"live company\")\n",
    "    )\n",
    "    &\n",
    "    (acra_data_filtered[\"PRIMARY_SSIC_CODE\"].astype(str).isin(ssic_codes))\n",
    "]\n",
    "\n",
    "\n",
    "acra_data_filtered_by_industry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6edf012",
   "metadata": {},
   "source": [
    "### Filter with Fresh Leads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87dc3716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UEN</th>\n",
       "      <th>ENTITY_NAME</th>\n",
       "      <th>BUSINESS_CONSTITUTION_DESCRIPTION</th>\n",
       "      <th>ENTITY_TYPE_DESCRIPTION</th>\n",
       "      <th>ENTITY_STATUS_DESCRIPTION</th>\n",
       "      <th>REGISTRATION_INCORPORATION_DATE</th>\n",
       "      <th>PRIMARY_SSIC_CODE</th>\n",
       "      <th>STREET_NAME</th>\n",
       "      <th>POSTAL_CODE</th>\n",
       "      <th>PARENT_INDUSTRY</th>\n",
       "      <th>INDUSTRY_TYPE</th>\n",
       "      <th>SUB_INDUSTRY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53480073D</td>\n",
       "      <td>HUMBLE BREWS</td>\n",
       "      <td>SOLE-PROPRIETOR</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>26-01-2024</td>\n",
       "      <td>46223</td>\n",
       "      <td>TOH YI DRIVE</td>\n",
       "      <td>590006</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Agricultural Raw Materials &amp; Live Animals</td>\n",
       "      <td>Wholesale Of Coffee, Cocoa And Tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202303828W</td>\n",
       "      <td>WINE &amp; BUBBLES PTE. LTD.</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LOCAL COMPANY</td>\n",
       "      <td>LIVE COMPANY</td>\n",
       "      <td>02-02-2023</td>\n",
       "      <td>46307</td>\n",
       "      <td>STURDEE ROAD</td>\n",
       "      <td>207855</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Food, Beverages &amp; Tobacco</td>\n",
       "      <td>Wholesale Of Liquor, Soft Drinks And Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202542730M</td>\n",
       "      <td>NUVIAA PTE. LTD.</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LOCAL COMPANY</td>\n",
       "      <td>LIVE COMPANY</td>\n",
       "      <td>24-09-2025</td>\n",
       "      <td>46413</td>\n",
       "      <td>YISHUN INDUSTRIAL STREET 1</td>\n",
       "      <td>768162</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Household Goods</td>\n",
       "      <td>Wholesale Of Children And Infants' Clothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201828332D</td>\n",
       "      <td>DE MAJESTIC VINES PTE. LTD.</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LOCAL COMPANY</td>\n",
       "      <td>LIVE COMPANY</td>\n",
       "      <td>17-08-2018</td>\n",
       "      <td>46307</td>\n",
       "      <td>ANSON ROAD</td>\n",
       "      <td>79903</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Food, Beverages &amp; Tobacco</td>\n",
       "      <td>Wholesale Of Liquor, Soft Drinks And Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201813214E</td>\n",
       "      <td>CARDE DESIGN PTE. LTD.</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LOCAL COMPANY</td>\n",
       "      <td>LIVE COMPANY</td>\n",
       "      <td>18-04-2018</td>\n",
       "      <td>46431</td>\n",
       "      <td>UPPER CROSS STREET</td>\n",
       "      <td>58357</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Household Goods</td>\n",
       "      <td>Wholesale Of Furniture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>199400661M</td>\n",
       "      <td>AIRPORT EQUIPMENT SERVICES PTE LTD</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LOCAL COMPANY</td>\n",
       "      <td>LIVE COMPANY</td>\n",
       "      <td>28-01-1994</td>\n",
       "      <td>46552</td>\n",
       "      <td>UBI CRESCENT</td>\n",
       "      <td>408564</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Machinery, Equipment &amp; Supplies</td>\n",
       "      <td>Wholesale Of Aircraft Equipment And Supplies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>201838013D</td>\n",
       "      <td>MITA MEDTECH PTE. LTD.</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LOCAL COMPANY</td>\n",
       "      <td>LIVE COMPANY</td>\n",
       "      <td>08-11-2018</td>\n",
       "      <td>46592</td>\n",
       "      <td>ORCHARD BOULEVARD</td>\n",
       "      <td>248649</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Machinery, Equipment &amp; Supplies</td>\n",
       "      <td>Wholesale Of Medical, Professional, Scientific...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>53162832M</td>\n",
       "      <td>WAHANA DISTRIBUTOR</td>\n",
       "      <td>SOLE-PROPRIETOR</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>07-03-2010</td>\n",
       "      <td>46593</td>\n",
       "      <td>BUKIT BATOK CRESCENT</td>\n",
       "      <td>658065</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Machinery, Equipment &amp; Supplies</td>\n",
       "      <td>Wholesale Of Commercial Food Service Equipment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>202540737R</td>\n",
       "      <td>ALTIVEX HOLDINGS PTE. LTD.</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LOCAL COMPANY</td>\n",
       "      <td>LIVE COMPANY</td>\n",
       "      <td>11-09-2025</td>\n",
       "      <td>46634</td>\n",
       "      <td>YISHUN INDUSTRIAL STREET 1</td>\n",
       "      <td>768162</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Other Specialised Wholesale</td>\n",
       "      <td>Wholesale Of Paints</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200301636R</td>\n",
       "      <td>NPRIME INTERNATIONAL PTE. LTD.</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LOCAL COMPANY</td>\n",
       "      <td>LIVE COMPANY</td>\n",
       "      <td>25-02-2003</td>\n",
       "      <td>46599</td>\n",
       "      <td>KALLANG AVENUE</td>\n",
       "      <td>339416</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Machinery, Equipment &amp; Supplies</td>\n",
       "      <td>Wholesale Of Other Machinery And Equipment N.E.C.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          UEN                         ENTITY_NAME  \\\n",
       "0   53480073D                        HUMBLE BREWS   \n",
       "1  202303828W            WINE & BUBBLES PTE. LTD.   \n",
       "2  202542730M                    NUVIAA PTE. LTD.   \n",
       "3  201828332D         DE MAJESTIC VINES PTE. LTD.   \n",
       "4  201813214E              CARDE DESIGN PTE. LTD.   \n",
       "5  199400661M  AIRPORT EQUIPMENT SERVICES PTE LTD   \n",
       "6  201838013D              MITA MEDTECH PTE. LTD.   \n",
       "7   53162832M                  WAHANA DISTRIBUTOR   \n",
       "8  202540737R          ALTIVEX HOLDINGS PTE. LTD.   \n",
       "9  200301636R      NPRIME INTERNATIONAL PTE. LTD.   \n",
       "\n",
       "  BUSINESS_CONSTITUTION_DESCRIPTION           ENTITY_TYPE_DESCRIPTION  \\\n",
       "0                   SOLE-PROPRIETOR  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "1                              <NA>                     LOCAL COMPANY   \n",
       "2                              <NA>                     LOCAL COMPANY   \n",
       "3                              <NA>                     LOCAL COMPANY   \n",
       "4                              <NA>                     LOCAL COMPANY   \n",
       "5                              <NA>                     LOCAL COMPANY   \n",
       "6                              <NA>                     LOCAL COMPANY   \n",
       "7                   SOLE-PROPRIETOR  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "8                              <NA>                     LOCAL COMPANY   \n",
       "9                              <NA>                     LOCAL COMPANY   \n",
       "\n",
       "  ENTITY_STATUS_DESCRIPTION REGISTRATION_INCORPORATION_DATE  \\\n",
       "0                      LIVE                      26-01-2024   \n",
       "1              LIVE COMPANY                      02-02-2023   \n",
       "2              LIVE COMPANY                      24-09-2025   \n",
       "3              LIVE COMPANY                      17-08-2018   \n",
       "4              LIVE COMPANY                      18-04-2018   \n",
       "5              LIVE COMPANY                      28-01-1994   \n",
       "6              LIVE COMPANY                      08-11-2018   \n",
       "7                      LIVE                      07-03-2010   \n",
       "8              LIVE COMPANY                      11-09-2025   \n",
       "9              LIVE COMPANY                      25-02-2003   \n",
       "\n",
       "   PRIMARY_SSIC_CODE                 STREET_NAME POSTAL_CODE PARENT_INDUSTRY  \\\n",
       "0              46223                TOH YI DRIVE      590006          Others   \n",
       "1              46307                STURDEE ROAD      207855          Others   \n",
       "2              46413  YISHUN INDUSTRIAL STREET 1      768162          Others   \n",
       "3              46307                  ANSON ROAD       79903          Others   \n",
       "4              46431          UPPER CROSS STREET       58357          Others   \n",
       "5              46552                UBI CRESCENT      408564          Others   \n",
       "6              46592           ORCHARD BOULEVARD      248649          Others   \n",
       "7              46593        BUKIT BATOK CRESCENT      658065          Others   \n",
       "8              46634  YISHUN INDUSTRIAL STREET 1      768162          Others   \n",
       "9              46599              KALLANG AVENUE      339416          Others   \n",
       "\n",
       "     INDUSTRY_TYPE                               SUB_INDUSTRY  \\\n",
       "0  Wholesale Trade  Agricultural Raw Materials & Live Animals   \n",
       "1  Wholesale Trade                  Food, Beverages & Tobacco   \n",
       "2  Wholesale Trade                            Household Goods   \n",
       "3  Wholesale Trade                  Food, Beverages & Tobacco   \n",
       "4  Wholesale Trade                            Household Goods   \n",
       "5  Wholesale Trade            Machinery, Equipment & Supplies   \n",
       "6  Wholesale Trade            Machinery, Equipment & Supplies   \n",
       "7  Wholesale Trade            Machinery, Equipment & Supplies   \n",
       "8  Wholesale Trade                Other Specialised Wholesale   \n",
       "9  Wholesale Trade            Machinery, Equipment & Supplies   \n",
       "\n",
       "                                         DESCRIPTION  \n",
       "0                 Wholesale Of Coffee, Cocoa And Tea  \n",
       "1     Wholesale Of Liquor, Soft Drinks And Beverages  \n",
       "2        Wholesale Of Children And Infants' Clothing  \n",
       "3     Wholesale Of Liquor, Soft Drinks And Beverages  \n",
       "4                             Wholesale Of Furniture  \n",
       "5       Wholesale Of Aircraft Equipment And Supplies  \n",
       "6  Wholesale Of Medical, Professional, Scientific...  \n",
       "7     Wholesale Of Commercial Food Service Equipment  \n",
       "8                                Wholesale Of Paints  \n",
       "9  Wholesale Of Other Machinery And Equipment N.E.C.  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Copy to avoid SettingWithCopyWarning ---\n",
    "acra_data_filtered_wholesale = acra_data_filtered_by_industry.copy()\n",
    "\n",
    "# --- UPDATE HERE: Remove rows if UEN exists in recordowl_results.xlsx ---\n",
    "Fresh_Leads_Wholesale = pd.read_excel(\"Fresh_Leads_Wholesale.xlsx\")\n",
    "\n",
    "\n",
    "if \"UEN\" in Fresh_Leads_Wholesale.columns and \"UEN\" in acra_data_filtered_wholesale.columns:\n",
    "    filtered = acra_data_filtered_wholesale[~acra_data_filtered_wholesale[\"UEN\"].isin(Fresh_Leads_Wholesale[\"UEN\"])]\n",
    "else:\n",
    "    raise ValueError(\"Column 'UEN' not found in one of the dataframes.\")\n",
    "\n",
    "# sample data \n",
    "acra_data_filtered_wholesale = filtered.sample(n=50, random_state=42).reset_index(drop=True)\n",
    "\n",
    "acra_data_filtered_wholesale.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b530343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# acra_data_filtered_wholesale = pd.DataFrame({\n",
    "#     \"UEN\": [\"201625008K\"]\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b44bc0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Process_data_RecordOwl_df = acra_data_filtered_wholesale.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3040bb97",
   "metadata": {},
   "source": [
    "### Get Data from RecordOwl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3cd46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Processing 53480073D (1/50)\n",
      "  📡 Starting Apify run for 53480073D (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:29:58.041Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:29:58.043Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:29:58.573Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:29:59.006Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:30:00.006Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:30:00.131Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:30:01.022Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:30:01.191Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:30:27.242Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 53480073D\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:30:27.250Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Search input found\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:30:27.251Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Waiting for page to stabilize...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:30:30.533Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m UEN typed successfully: 53480073D\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:30:30.537Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Clicking submit button...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:30:30.591Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Submit button clicked\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:30:36.493Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Page stabilized after submit\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:30:36.494Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Verifying company links are present...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:30:36.498Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Company links confirmed\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:30:36.499Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Found company link: https://recordowl.com/company/humble-brews\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:30:36.500Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Navigating to company page...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:30:53.895Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Company page loaded\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:30:58.895Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Waiting for page content...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:30:58.904Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Content loaded\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:31:01.191Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":60254,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:31:01.266Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0.059},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0.191},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:31:01.928Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Successfully extracted HTML content (133005 chars)\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:31:05.283Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:31:07.548Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":62773,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":62773,\"requestsTotal\":1,\"crawlerRuntimeMillis\":66611}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:31:07.549Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> Status: RUNNING, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8fRbCIPD4GW9aeWcO]\u001b[0m -> 2025-11-06T09:31:07.673Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 53480073D (133005 chars of HTML)\n",
      "  🗑️ Removed 73 hidden elements from HTML\n",
      "  ✅ Targeting Overview tab only (excluding officer/director data)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 10 visible dt tags (filtered from 10 total)\n",
      "  🔎 No phones found yet, searching entire visible content...\n",
      "  ⚠️ WARNING: No phone numbers found for 53480073D\n",
      "  📄 Showing first 500 chars of parent HTML for debugging:\n",
      "<div aria-labelledby=\"overview-tab\" class=\"block\" id=\"overview\" role=\"tabpanel\" style=\"height: auto !important;\">\n",
      " <div class=\"bg-white shadow overflow-hidden sm:rounded-lg mb-6\">\n",
      "  <div class=\"px-4 py-5 sm:px-6 flex justify-between items-center\">\n",
      "   <div>\n",
      "    <h2 class=\"text-lg leading-6 font-medium text-gray-900\">\n",
      "     General Information\n",
      "    </h2>\n",
      "    <p class=\"mt-1 max-w-2xl text-sm text-gray-500\">\n",
      "     Official company information and location\n",
      "    </p>\n",
      "   </div>\n",
      "  </div>\n",
      "  <div class=\"borde...\n",
      "  ✅ Processed 53480073D: 0 emails, 0 phones\n",
      "  💤 Sleeping for 26s before next request...\n",
      "\n",
      "🔎 Processing 202303828W (2/50)\n",
      "  📡 Starting Apify run for 202303828W (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:31:49.887Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:31:49.889Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:31:49.934Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:31:50.133Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:31:51.600Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:31:51.838Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:31:53.067Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:31:53.346Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:00.560Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:00.561Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:07.157Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:07.157Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":2}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:19.842Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 202303828W\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:19.848Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Search input found\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:19.848Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Waiting for page to stabilize...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:23.445Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m UEN typed successfully: 202303828W\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:23.453Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Clicking submit button...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:23.533Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Submit button clicked\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:28.052Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Page stabilized after submit\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:28.058Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Verifying company links are present...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:28.059Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Company links confirmed\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:28.059Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Found company link: https://recordowl.com/company/wine-bubbles-pte-ltd\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:28.059Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Navigating to company page...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:39.871Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Company page loaded\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:44.872Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Waiting for page content...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:44.879Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Content loaded\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:47.931Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Successfully extracted HTML content (1146793 chars)\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:48.797Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:49.349Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[null,null,1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":40854,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":40854,\"requestsTotal\":1,\"crawlerRuntimeMillis\":56360}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:49.350Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> Status: RUNNING, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:40ZhvxRbw1Tsb0crf]\u001b[0m -> 2025-11-06T09:32:49.397Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 202303828W (1146793 chars of HTML)\n",
      "  🗑️ Removed 72 hidden elements from HTML\n",
      "  ✅ Targeting Overview tab only (excluding officer/director data)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 11 visible dt tags (filtered from 11 total)\n",
      "  📝 [GENERAL] Field 'contact number': +65 88162548\n",
      "  🔢 Extracted digits: 6588162548\n",
      "  ✅ Added from dt/dd (10 digits): +6588162548\n",
      "  ✅ Total phones found: ['+6588162548']\n",
      "  ✅ Processed 202303828W: 1 emails, 1 phones\n",
      "  💤 Sleeping for 27s before next request...\n",
      "\n",
      "🔎 Processing 202542730M (3/50)\n",
      "  📡 Starting Apify run for 202542730M (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> 2025-11-06T09:33:32.204Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> 2025-11-06T09:33:32.206Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> 2025-11-06T09:33:32.275Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> 2025-11-06T09:33:32.615Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> 2025-11-06T09:33:33.338Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> 2025-11-06T09:33:33.718Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> 2025-11-06T09:33:34.528Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> 2025-11-06T09:33:34.645Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> 2025-11-06T09:33:51.897Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 202542730M\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> 2025-11-06T09:33:51.903Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Search input found\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> 2025-11-06T09:33:51.904Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Waiting for page to stabilize...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> 2025-11-06T09:33:55.259Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m UEN typed successfully: 202542730M\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> 2025-11-06T09:33:55.260Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Clicking submit button...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> 2025-11-06T09:33:55.359Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Submit button clicked\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> 2025-11-06T09:33:57.368Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Page stabilized after submit\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> 2025-11-06T09:33:57.368Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Verifying company links are present...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> 2025-11-06T09:34:07.385Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m No company links found, might be not found\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> 2025-11-06T09:34:08.170Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> 2025-11-06T09:34:09.048Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":32566,\"requestsFinishedPerMinute\":2,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":32566,\"requestsTotal\":1,\"crawlerRuntimeMillis\":34586}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> 2025-11-06T09:34:09.049Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> 2025-11-06T09:34:09.119Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:SiEJvBKax92bURmtb]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ⚠️ Company not found for UEN 202542730M\n",
      "  ❌ Company not found on RecordOwl\n",
      "\n",
      "🔎 Processing 201828332D (4/50)\n",
      "  📡 Starting Apify run for 201828332D (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:34:29.572Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:34:29.574Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:34:29.639Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:34:29.847Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:34:30.669Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:34:30.812Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:34:31.461Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:34:31.592Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:34:41.768Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:34:41.775Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:34:49.228Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:34:49.231Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":2}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:04.319Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Detected a session error, rotating session...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:04.321Z net::ERR_TUNNEL_CONNECTION_FAILED at https://recordowl.com/\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:04.323Z\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:04.325Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":3}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:27.772Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 201828332D\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:27.778Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Search input found\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:27.780Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Waiting for page to stabilize...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:31.153Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m UEN typed successfully: 201828332D\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:31.155Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Clicking submit button...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:31.192Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Submit button clicked\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:31.593Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":60207,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:31.622Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0.035},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:34.962Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Page stabilized after submit\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:34.965Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Verifying company links are present...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:34.970Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Company links confirmed\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:34.972Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Found company link: https://recordowl.com/company/de-majestic-vines-pte-ltd\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:34.973Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Navigating to company page...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:46.489Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Company page loaded\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:51.488Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Waiting for page content...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:51.496Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Content loaded\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:54.571Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Successfully extracted HTML content (1488274 chars)\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:54.883Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:55.303Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[null,null,null,1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":50173,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":50173,\"requestsTotal\":1,\"crawlerRuntimeMillis\":83917}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:55.305Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> 2025-11-06T09:35:55.321Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:WDQ5kmraEFcpgT8V0]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 201828332D (1488274 chars of HTML)\n",
      "  🗑️ Removed 73 hidden elements from HTML\n",
      "  ✅ Targeting Overview tab only (excluding officer/director data)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 13 visible dt tags (filtered from 13 total)\n",
      "  📝 [GENERAL] Field 'contact number': 6980 7200\n",
      "  🔢 Extracted digits: 69807200\n",
      "  ✅ Added from dt/dd (8 digits): +6569807200\n",
      "  ✅ Total phones found: ['+6569807200']\n",
      "  ✅ Processed 201828332D: 1 emails, 1 phones\n",
      "  💤 Sleeping for 29s before next request...\n",
      "\n",
      "🔎 Processing 201813214E (5/50)\n",
      "  📡 Starting Apify run for 201813214E (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:36:40.616Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:36:40.618Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:36:40.671Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:36:40.929Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:36:41.989Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:36:42.721Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:36:43.705Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:36:44.042Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:36:58.013Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 201813214E\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:36:58.020Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Search input found\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:36:58.020Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Waiting for page to stabilize...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:37:01.385Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m UEN typed successfully: 201813214E\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:37:01.386Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Clicking submit button...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:37:01.415Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Submit button clicked\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:37:05.488Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Page stabilized after submit\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:37:05.489Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Verifying company links are present...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:37:05.494Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Company links confirmed\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:37:05.495Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Found company link: https://recordowl.com/company/carde-design-pte-ltd\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:37:05.496Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Navigating to company page...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:37:17.380Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Company page loaded\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:37:22.380Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Waiting for page content...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:37:22.388Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Content loaded\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:37:25.410Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Successfully extracted HTML content (467882 chars)\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:37:25.991Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:37:27.318Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":41129,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":41129,\"requestsTotal\":1,\"crawlerRuntimeMillis\":43686}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:37:27.319Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> 2025-11-06T09:37:27.332Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:iRhsZ8hdalK7LaTBL]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 201813214E (467882 chars of HTML)\n",
      "  🗑️ Removed 71 hidden elements from HTML\n",
      "  ✅ Targeting Overview tab only (excluding officer/director data)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 10 visible dt tags (filtered from 10 total)\n",
      "  🔎 No phones found yet, searching entire visible content...\n",
      "  ⚠️ WARNING: No phone numbers found for 201813214E\n",
      "  📄 Showing first 500 chars of parent HTML for debugging:\n",
      "<div aria-labelledby=\"overview-tab\" class=\"block\" id=\"overview\" role=\"tabpanel\" style=\"height: auto !important;\">\n",
      " <div class=\"bg-white shadow overflow-hidden sm:rounded-lg mb-6\">\n",
      "  <div class=\"px-4 py-5 sm:px-6 flex justify-between items-center\">\n",
      "   <div>\n",
      "    <h2 class=\"text-lg leading-6 font-medium text-gray-900\">\n",
      "     General Information\n",
      "    </h2>\n",
      "    <p class=\"mt-1 max-w-2xl text-sm text-gray-500\">\n",
      "     Official company information and location\n",
      "    </p>\n",
      "   </div>\n",
      "  </div>\n",
      "  <div class=\"borde...\n",
      "  ✅ Processed 201813214E: 0 emails, 0 phones\n",
      "  💤 Sleeping for 30s before next request...\n",
      "  🛑 Checkpoint pause: waiting extra 30s...\n",
      "\n",
      "🔎 Processing 199400661M (6/50)\n",
      "  📡 Starting Apify run for 199400661M (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:38:42.442Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:38:42.449Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:38:42.500Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:38:42.722Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:38:43.419Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:38:43.547Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:38:44.272Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:38:44.500Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:38:52.854Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:38:52.855Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:39:05.322Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 199400661M\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:39:05.360Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Search input found\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:39:05.361Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Waiting for page to stabilize...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:39:08.757Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m UEN typed successfully: 199400661M\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:39:08.758Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Clicking submit button...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:39:08.862Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Submit button clicked\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:39:13.257Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Page stabilized after submit\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:39:13.260Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Verifying company links are present...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:39:13.558Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Company links confirmed\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:39:13.561Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Found company link: https://recordowl.com/company/airport-equipment-services-pte-ltd\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:39:13.562Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Navigating to company page...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:39:23.764Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Company page loaded\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:39:28.764Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Waiting for page content...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:39:28.772Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Content loaded\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:39:31.846Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Successfully extracted HTML content (815312 chars)\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:39:32.211Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:39:32.707Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[null,1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":38708,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":38708,\"requestsTotal\":1,\"crawlerRuntimeMillis\":48523}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:39:32.708Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> 2025-11-06T09:39:32.723Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3Ec1KuFlV2WoPd2yH]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 199400661M (815312 chars of HTML)\n",
      "  🗑️ Removed 71 hidden elements from HTML\n",
      "  ✅ Targeting Overview tab only (excluding officer/director data)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 12 visible dt tags (filtered from 12 total)\n",
      "  📝 [GENERAL] Field 'contact number': (65) 6542 1160\n",
      "  🔢 Extracted digits: 6565421160\n",
      "  ✅ Added from dt/dd (10 digits): +6565421160\n",
      "  ✅ Total phones found: ['+6565421160']\n",
      "  ✅ Processed 199400661M: 0 emails, 1 phones\n",
      "  💤 Sleeping for 31s before next request...\n",
      "\n",
      "🔎 Processing 201838013D (7/50)\n",
      "  📡 Starting Apify run for 201838013D (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:19.987Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:19.989Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:20.036Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:20.207Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:20.880Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:21.032Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:24.224Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:24.363Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:27.099Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Detected a session error, rotating session...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:27.101Z net::ERR_TUNNEL_CONNECTION_FAILED at https://recordowl.com/\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:27.104Z\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:27.106Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:45.276Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 201838013D\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:45.304Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Search input found\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:45.306Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Waiting for page to stabilize...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:48.955Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m UEN typed successfully: 201838013D\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:48.957Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Clicking submit button...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:48.973Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Submit button clicked\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:51.155Z \u001b[31mERROR\u001b[39m\u001b[33m PuppeteerCrawler:\u001b[39m Error during submit: log.warn is not a function\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:52.073Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Results found despite submit error\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:52.075Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Verifying company links are present...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:52.081Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Company links confirmed\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:52.084Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Found company link: https://recordowl.com/company/mita-medtech-pte-ltd\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:40:52.085Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Navigating to company page...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:41:05.629Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Company page loaded\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:41:10.641Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Waiting for page content...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:41:10.643Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Content loaded\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:41:13.750Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Successfully extracted HTML content (490435 chars)\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> Status: RUNNING, Message: Crawled 1/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:41:14.772Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:41:15.811Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[null,1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":45391,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":45391,\"requestsTotal\":1,\"crawlerRuntimeMillis\":51653}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:41:15.815Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> 2025-11-06T09:41:15.986Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:uvXGUoIgeYLjeaqpz]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 201838013D (490435 chars of HTML)\n",
      "  🗑️ Removed 72 hidden elements from HTML\n",
      "  ✅ Targeting Overview tab only (excluding officer/director data)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 11 visible dt tags (filtered from 11 total)\n",
      "  🔎 No phones found yet, searching entire visible content...\n",
      "  ⚠️ WARNING: No phone numbers found for 201838013D\n",
      "  📄 Showing first 500 chars of parent HTML for debugging:\n",
      "<div aria-labelledby=\"overview-tab\" class=\"block\" id=\"overview\" role=\"tabpanel\" style=\"height: auto !important;\">\n",
      " <div class=\"bg-white shadow overflow-hidden sm:rounded-lg mb-6\">\n",
      "  <div class=\"px-4 py-5 sm:px-6 flex justify-between items-center\">\n",
      "   <div>\n",
      "    <h2 class=\"text-lg leading-6 font-medium text-gray-900\">\n",
      "     General Information\n",
      "    </h2>\n",
      "    <p class=\"mt-1 max-w-2xl text-sm text-gray-500\">\n",
      "     Official company information and location\n",
      "    </p>\n",
      "   </div>\n",
      "  </div>\n",
      "  <div class=\"borde...\n",
      "  ✅ Processed 201838013D: 0 emails, 0 phones\n",
      "  💤 Sleeping for 32s before next request...\n",
      "\n",
      "🔎 Processing 53162832M (8/50)\n",
      "  📡 Starting Apify run for 53162832M (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:42:03.081Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:42:03.082Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:42:03.125Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:42:03.290Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:42:03.904Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:42:04.029Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:42:04.542Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:42:04.619Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:42:11.722Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:42:11.724Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:42:37.928Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. net::ERR_CONNECTION_CLOSED at https://recordowl.com/\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:42:37.929Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":2}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:04.619Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":60141,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:04.638Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:15.868Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:15.868Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":3}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:31.338Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 53162832M\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:31.344Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Search input found\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:31.345Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Waiting for page to stabilize...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:34.592Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m UEN typed successfully: 53162832M\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:34.593Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Clicking submit button...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:34.618Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Submit button clicked\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:39.169Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Page stabilized after submit\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:39.170Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Verifying company links are present...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:39.172Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Company links confirmed\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:39.173Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Found company link: https://recordowl.com/company/wahana-distributor\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:39.174Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Navigating to company page...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:41.092Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Company page loaded\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:46.092Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Waiting for page content...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:46.100Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Content loaded\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:49.109Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Successfully extracted HTML content (128058 chars)\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:49.444Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:49.748Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[null,null,null,1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":33264,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":33264,\"requestsTotal\":1,\"crawlerRuntimeMillis\":105271}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:49.749Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> 2025-11-06T09:43:49.766Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8AhG5LxIM9GgpmwgP]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 53162832M (128058 chars of HTML)\n",
      "  🗑️ Removed 73 hidden elements from HTML\n",
      "  ✅ Targeting Overview tab only (excluding officer/director data)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 8 visible dt tags (filtered from 8 total)\n",
      "  🔎 No phones found yet, searching entire visible content...\n",
      "  ⚠️ WARNING: No phone numbers found for 53162832M\n",
      "  📄 Showing first 500 chars of parent HTML for debugging:\n",
      "<div aria-labelledby=\"overview-tab\" class=\"block\" id=\"overview\" role=\"tabpanel\" style=\"height: auto !important;\">\n",
      " <div class=\"bg-white shadow overflow-hidden sm:rounded-lg mb-6\">\n",
      "  <div class=\"px-4 py-5 sm:px-6 flex justify-between items-center\">\n",
      "   <div>\n",
      "    <h2 class=\"text-lg leading-6 font-medium text-gray-900\">\n",
      "     General Information\n",
      "    </h2>\n",
      "    <p class=\"mt-1 max-w-2xl text-sm text-gray-500\">\n",
      "     Official company information and location\n",
      "    </p>\n",
      "   </div>\n",
      "  </div>\n",
      "  <div class=\"borde...\n",
      "  ✅ Processed 53162832M: 0 emails, 0 phones\n",
      "  💤 Sleeping for 33s before next request...\n",
      "\n",
      "🔎 Processing 202540737R (9/50)\n",
      "  📡 Starting Apify run for 202540737R (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:44:38.500Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:44:38.507Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:44:38.569Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:44:38.764Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:44:39.705Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:44:40.410Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:44:42.129Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:44:42.470Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:44:46.701Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:44:46.706Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:44:57.693Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 202540737R\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:44:57.704Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Search input found\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:44:57.704Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Waiting for page to stabilize...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:45:01.124Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m UEN typed successfully: 202540737R\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:45:01.125Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Clicking submit button...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:45:01.155Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Submit button clicked\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:45:12.305Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Page stabilized after submit\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:45:12.306Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Verifying company links are present...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:45:22.306Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m No company links found, might be not found\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:45:22.838Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:45:23.523Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[null,1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":34979,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":34979,\"requestsTotal\":1,\"crawlerRuntimeMillis\":41518}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:45:23.524Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> 2025-11-06T09:45:23.571Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ZKrjwPXyEVCcGIzLJ]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ⚠️ Company not found for UEN 202540737R\n",
      "  ❌ Company not found on RecordOwl\n",
      "\n",
      "🔎 Processing 200301636R (10/50)\n",
      "  📡 Starting Apify run for 200301636R (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:45:44.123Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:45:44.125Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:45:44.204Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:45:44.378Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:45:45.010Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:45:45.164Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:45:46.269Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:45:46.369Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:46:46.369Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":60170,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:46:46.398Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:47:16.790Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Navigation timed out after 90 seconds.\u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:47:46.369Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":120170,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:47:46.403Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:47:53.682Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 200301636R\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:47:53.689Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Search input found\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:47:53.691Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Waiting for page to stabilize...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:47:57.045Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m UEN typed successfully: 200301636R\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:47:57.048Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Clicking submit button...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:47:57.072Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Submit button clicked\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:48:04.436Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Page stabilized after submit\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:48:04.438Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Verifying company links are present...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:48:04.441Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Company links confirmed\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:48:04.443Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Found company link: https://recordowl.com/company/nprime-international-pte-ltd\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:48:04.445Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Navigating to company page...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:48:27.073Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Navigation did not occur (may be client-side routing)\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:48:46.370Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":180171,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:48:46.405Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0.02},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:49:04.443Z \u001b[31mERROR\u001b[39m\u001b[33m PuppeteerCrawler:\u001b[39m Error navigating to company page: Navigation timeout of 60000 ms exceeded\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:49:04.776Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:49:05.741Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[null,1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":107496,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":107496,\"requestsTotal\":1,\"crawlerRuntimeMillis\":199542}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:49:05.743Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> 2025-11-06T09:49:05.795Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:MdfMoWPxpga5p0bzt]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ❌ Error for 200301636R: Failed to load company page: Navigation timeout of 60000 ms exceeded\n",
      "  ❌ Scraping error: Failed to load company page: Navigation timeout of 60000 ms exceeded\n",
      "\n",
      "🔎 Processing 201700187C (11/50)\n",
      "  📡 Starting Apify run for 201700187C (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:QfYOrqWcCN3G9bjPF]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:QfYOrqWcCN3G9bjPF]\u001b[0m -> 2025-11-06T09:49:25.917Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:QfYOrqWcCN3G9bjPF]\u001b[0m -> 2025-11-06T09:49:25.918Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:QfYOrqWcCN3G9bjPF]\u001b[0m -> 2025-11-06T09:49:25.989Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:QfYOrqWcCN3G9bjPF]\u001b[0m -> 2025-11-06T09:49:26.146Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:QfYOrqWcCN3G9bjPF]\u001b[0m -> 2025-11-06T09:49:26.762Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:QfYOrqWcCN3G9bjPF]\u001b[0m -> 2025-11-06T09:49:26.921Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:QfYOrqWcCN3G9bjPF]\u001b[0m -> 2025-11-06T09:49:27.516Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:QfYOrqWcCN3G9bjPF]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:QfYOrqWcCN3G9bjPF]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:QfYOrqWcCN3G9bjPF]\u001b[0m -> 2025-11-06T09:49:27.596Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:QfYOrqWcCN3G9bjPF]\u001b[0m -> 2025-11-06T09:49:58.113Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. net::ERR_TIMED_OUT at https://recordowl.com/\n",
      "\u001b[36m[apify.puppeteer-scraper runId:QfYOrqWcCN3G9bjPF]\u001b[0m -> 2025-11-06T09:49:58.115Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:QfYOrqWcCN3G9bjPF]\u001b[0m -> 2025-11-06T09:50:27.596Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":60148,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:QfYOrqWcCN3G9bjPF]\u001b[0m -> 2025-11-06T09:50:27.628Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:QfYOrqWcCN3G9bjPF]\u001b[0m -> 2025-11-06T09:50:28.841Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. net::ERR_TIMED_OUT at https://recordowl.com/\n",
      "\u001b[36m[apify.puppeteer-scraper runId:QfYOrqWcCN3G9bjPF]\u001b[0m -> 2025-11-06T09:50:28.844Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":2}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:QfYOrqWcCN3G9bjPF]\u001b[0m -> 2025-11-06T09:50:59.751Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. net::ERR_TIMED_OUT at https://recordowl.com/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = ApifyClient(\"apify_api_ZCE4JkWSigwKnhksXuw2Cf6V30zTpK1kXyk2\")\n",
    "\n",
    "SOCIAL_MEDIA_DOMAINS = [\n",
    "    \"facebook.com\", \"linkedin.com\", \"instagram.com\", \"youtube.com\",\n",
    "    \"tiktok.com\", \"twitter.com\", \"x.com\", \"pinterest.com\"\n",
    "]\n",
    "\n",
    "def fetch_dataset_items_safe(dataset_client, max_retries=5, initial_wait=3):\n",
    "    \"\"\"Safely fetch dataset items with multiple retry strategies.\"\"\"\n",
    "    dataset_items = []\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Strategy 1: Try using iterate_items() (streaming)\n",
    "            try:\n",
    "                dataset_items = list(dataset_client.iterate_items())\n",
    "                if dataset_items:\n",
    "                    return dataset_items\n",
    "            except (HTTPError, ConnectionError, ProtocolError, Exception) as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = initial_wait * (2 ** attempt)  # Exponential backoff\n",
    "                    print(f\"  ⚠️ Iteration method failed (attempt {attempt + 1}/{max_retries}), trying direct fetch in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"  ⚠️ Iteration method failed after all retries, trying direct fetch...\")\n",
    "            \n",
    "            # Strategy 2: Try using list_items() (direct pagination)\n",
    "            try:\n",
    "                offset = 0\n",
    "                limit = 100\n",
    "                while True:\n",
    "                    page = dataset_client.list_items(offset=offset, limit=limit, clean=True)\n",
    "                    if not page.items:\n",
    "                        break\n",
    "                    dataset_items.extend(page.items)\n",
    "                    if len(page.items) < limit:\n",
    "                        break\n",
    "                    offset += limit\n",
    "                \n",
    "                if dataset_items:\n",
    "                    return dataset_items\n",
    "            except (HTTPError, ConnectionError, ProtocolError, Exception) as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = initial_wait * (2 ** attempt)\n",
    "                    print(f\"  ⚠️ Direct fetch failed (attempt {attempt + 1}/{max_retries}), retrying in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"  ❌ All fetch methods failed: {e}\")\n",
    "                    return []\n",
    "                    \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = initial_wait * (2 ** attempt)\n",
    "                print(f\"  ⚠️ Unexpected error (attempt {attempt + 1}/{max_retries}), retrying in {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"  ❌ Failed after all retries: {e}\")\n",
    "                return []\n",
    "    \n",
    "    return dataset_items\n",
    "\n",
    "def run_apify_with_retry(client, run_input, uen, max_retries=3):\n",
    "    \"\"\"Run Apify with exponential backoff on 403 errors AND verify dataset has items.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"  📡 Starting Apify run for {uen} (attempt {attempt + 1}/{max_retries})...\")\n",
    "            run = client.actor(\"apify/puppeteer-scraper\").call(run_input=run_input)\n",
    "            \n",
    "            print(f\"  ⏳ Waiting for run to complete...\")\n",
    "            run_client = client.run(run[\"id\"])\n",
    "            run_info = run_client.wait_for_finish()\n",
    "            \n",
    "            # CRITICAL FIX: Check if run actually scraped pages, not just if it \"succeeded\"\n",
    "            if run_info and \"status\" in run_info:\n",
    "                status = run_info.get(\"status\")\n",
    "                \n",
    "                # Even if status is \"SUCCEEDED\", verify dataset actually has items\n",
    "                if status == \"SUCCEEDED\" and \"defaultDatasetId\" in run:\n",
    "                    # Quick check if dataset has any items\n",
    "                    try:\n",
    "                        dataset_check = client.dataset(run[\"defaultDatasetId\"])\n",
    "                        time.sleep(2)  # Brief wait for dataset to be ready\n",
    "                        test_items = dataset_check.list_items(limit=1, clean=True)\n",
    "                        \n",
    "                        if test_items.items and len(test_items.items) > 0:\n",
    "                            # Dataset has items - true success!\n",
    "                            print(f\"  ✅ Run succeeded with data\")\n",
    "                            return run, None\n",
    "                        else:\n",
    "                            # Status says \"SUCCEEDED\" but dataset is EMPTY - this is a failure!\n",
    "                            print(f\"  ⚠️ Run completed but dataset is empty (likely 403 block)\")\n",
    "                            # Treat as 403 and retry\n",
    "                            if attempt < max_retries - 1:\n",
    "                                wait_time = 30 * (2 ** attempt)\n",
    "                                print(f\"  🔄 Retrying in {wait_time}s...\")\n",
    "                                time.sleep(wait_time)\n",
    "                                continue\n",
    "                            else:\n",
    "                                return None, \"Dataset empty after all retries (403 blocking)\"\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ⚠️ Could not verify dataset: {e}\")\n",
    "                        # If we can't check dataset, try to use the run anyway\n",
    "                        return run, None\n",
    "                \n",
    "                elif status != \"SUCCEEDED\":\n",
    "                    # Check error message for 403\n",
    "                    error_msg = str(run_info)\n",
    "                    if \"403\" in error_msg or \"blocked\" in error_msg.lower():\n",
    "                        if attempt < max_retries - 1:\n",
    "                            wait_time = 30 * (2 ** attempt)  # 30s, 60s, 120s\n",
    "                            print(f\"  🚫 Request blocked (403), waiting {wait_time}s before retry...\")\n",
    "                            time.sleep(wait_time)\n",
    "                            continue\n",
    "            \n",
    "            return run, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_str = str(e)\n",
    "            if \"403\" in error_str or \"blocked\" in error_str.lower():\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 30 * (2 ** attempt)\n",
    "                    print(f\"  🚫 Request blocked (403), waiting {wait_time}s before retry...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "            return None, f\"Apify call failed: {str(e)}\"\n",
    "    \n",
    "    return None, \"Max retries exceeded due to 403 blocking\"\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for idx, (i, row) in enumerate(Process_data_RecordOwl_df.iterrows(), 1):\n",
    "    uen = str(row[\"UEN\"]).strip()\n",
    "    print(f\"\\n🔎 Processing {uen} ({idx}/{len(Process_data_RecordOwl_df)})\")\n",
    "\n",
    "    # Build pageFunction with proper escaping and improved error handling\n",
    "    page_function = f\"\"\"\n",
    "    async function pageFunction(context) {{\n",
    "        const {{ page, log, request }} = context;\n",
    "        const uen = \"{uen}\";\n",
    "        log.info(\"Visiting RecordOwl for UEN: \" + uen);\n",
    "\n",
    "        try {{\n",
    "            // Step 1: Wait for search input\n",
    "            await page.waitForSelector(\"input[placeholder='Search company name, industry, or address']\", {{ timeout: 30000 }});\n",
    "            log.info(\"Search input found\");\n",
    "            \n",
    "            // Step 2: Type UEN into search box with error handling and navigation protection\n",
    "            try {{\n",
    "                // Wait for page to be stable (no navigation happening)\n",
    "                log.info(\"Waiting for page to stabilize...\");\n",
    "                await new Promise(r => setTimeout(r, 2000)); // Wait for any auto-navigation to complete\n",
    "                \n",
    "                // Wait for input to be present and stable\n",
    "                await page.waitForSelector(\"input[placeholder='Search company name, industry, or address']\", {{ \n",
    "                    timeout: 30000,\n",
    "                    visible: true \n",
    "                }});\n",
    "                \n",
    "                // Re-find input right before typing (in case page navigated)\n",
    "                let input = await page.$(\"input[placeholder='Search company name, industry, or address']\");\n",
    "                if (!input) {{\n",
    "                    log.error(\"Input element not found after wait\");\n",
    "                    return {{ status: 'error', uen, error: 'Input element not found' }};\n",
    "                }}\n",
    "                \n",
    "                // Clear and type with retry logic\n",
    "                let typed = false;\n",
    "                for (let attempt = 0; attempt < 3; attempt++) {{\n",
    "                    try {{\n",
    "                        // Re-find input on each attempt (in case context was destroyed)\n",
    "                        input = await page.$(\"input[placeholder='Search company name, industry, or address']\");\n",
    "                        if (!input) {{\n",
    "                            throw new Error(\"Input not found on attempt \" + (attempt + 1));\n",
    "                        }}\n",
    "                        \n",
    "                        // Click to focus\n",
    "                        await input.click({{ clickCount: 3 }});\n",
    "                        await new Promise(r => setTimeout(r, 300)); // Small delay after click\n",
    "                        \n",
    "                        // Clear input first\n",
    "                        await page.evaluate((selector) => {{\n",
    "                            const el = document.querySelector(selector);\n",
    "                            if (el) el.value = '';\n",
    "                        }}, \"input[placeholder='Search company name, industry, or address']\");\n",
    "                        \n",
    "                        // Type UEN\n",
    "                        await input.type(uen, {{ delay: 100 }});\n",
    "                        typed = true;\n",
    "                        log.info(\"UEN typed successfully: \" + uen);\n",
    "                        break;\n",
    "                    }} catch (typeErr) {{\n",
    "                        if (typeErr.message.includes(\"Execution context was destroyed\") || \n",
    "                            typeErr.message.includes(\"navigation\")) {{\n",
    "                            log.warn(\"Navigation occurred during typing (attempt \" + (attempt + 1) + \"/3), retrying...\");\n",
    "                            // Wait for page to stabilize after navigation\n",
    "                            await new Promise(r => setTimeout(r, 2000));\n",
    "                            // Re-wait for input\n",
    "                            await page.waitForSelector(\"input[placeholder='Search company name, industry, or address']\", {{ \n",
    "                                timeout: 10000,\n",
    "                                visible: true \n",
    "                            }});\n",
    "                            continue;\n",
    "                        }} else {{\n",
    "                            throw typeErr;\n",
    "                        }}\n",
    "                    }}\n",
    "                }}\n",
    "                \n",
    "                if (!typed) {{\n",
    "                    log.error(\"Failed to type UEN after all retries\");\n",
    "                    return {{ status: 'error', uen, error: 'Failed to type UEN after retries' }};\n",
    "                }}\n",
    "                \n",
    "            }} catch (typeErr) {{\n",
    "                log.error(\"Error typing UEN: \" + typeErr.message);\n",
    "                return {{ status: 'error', uen, error: 'Failed to type UEN: ' + typeErr.message }};\n",
    "            }}\n",
    "\n",
    "            // Step 3: Submit search with flexible waiting strategy\n",
    "            try {{\n",
    "                log.info(\"Clicking submit button...\");\n",
    "                \n",
    "                // Click submit button first\n",
    "                await page.click(\"button[type='submit']\");\n",
    "                log.info(\"Submit button clicked\");\n",
    "                \n",
    "                // Wait for either navigation OR results to appear (more flexible)\n",
    "                // Strategy: Wait for results to appear, with navigation as optional\n",
    "                try {{\n",
    "                    // Option 1: Wait for navigation (if it happens) - non-blocking\n",
    "                    const navigationPromise = page.waitForNavigation({{ \n",
    "                        waitUntil: 'networkidle2', \n",
    "                        timeout: 30000 \n",
    "                    }}).catch(() => {{\n",
    "                        log.info(\"Navigation did not occur (may be client-side routing)\");\n",
    "                        return null;\n",
    "                    }});\n",
    "                    \n",
    "                    // Option 2: Wait for results to appear (more reliable)\n",
    "                    const resultsPromise = page.waitForSelector(\"a[href*='/company/']\", {{ \n",
    "                        timeout: 60000 \n",
    "                    }});\n",
    "                    \n",
    "                    // Wait for either navigation or results (whichever happens first)\n",
    "                    await Promise.race([\n",
    "                        navigationPromise,\n",
    "                        resultsPromise\n",
    "                    ]);\n",
    "                    \n",
    "                    // Give page time to stabilize\n",
    "                    await new Promise(r => setTimeout(r, 2000));\n",
    "                    log.info(\"Page stabilized after submit\");\n",
    "                    \n",
    "                }} catch (waitErr) {{\n",
    "                    // If both navigation and results wait failed, try one more time for results\n",
    "                    log.warn(\"Initial wait failed, trying again for results: \" + waitErr.message);\n",
    "                    try {{\n",
    "                        await page.waitForSelector(\"a[href*='/company/']\", {{ timeout: 30000 }});\n",
    "                        log.info(\"Results found on retry\");\n",
    "                    }} catch (retryErr) {{\n",
    "                        log.info(\"No company links found after submit, might be not found\");\n",
    "                        return {{ status: 'not_found', uen }};\n",
    "                    }}\n",
    "                }}\n",
    "                \n",
    "            }} catch (navErr) {{\n",
    "                log.error(\"Error during submit: \" + navErr.message);\n",
    "                // Don't fail immediately - try to check if results are already there\n",
    "                try {{\n",
    "                    const hasResults = await page.$(\"a[href*='/company/']\");\n",
    "                    if (hasResults) {{\n",
    "                        log.info(\"Results found despite submit error\");\n",
    "                    }} else {{\n",
    "                        return {{ status: 'error', uen, error: 'Submit failed: ' + navErr.message }};\n",
    "                    }}\n",
    "                }} catch (checkErr) {{\n",
    "                    return {{ status: 'error', uen, error: 'Submit failed: ' + navErr.message }};\n",
    "                }}\n",
    "            }}\n",
    "\n",
    "            // Step 4: Verify search results are present\n",
    "            log.info(\"Verifying company links are present...\");\n",
    "            try {{\n",
    "                // Double-check that results are actually there\n",
    "                await page.waitForSelector(\"a[href*='/company/']\", {{ timeout: 10000 }});\n",
    "                log.info(\"Company links confirmed\");\n",
    "            }} catch (e) {{\n",
    "                log.info(\"No company links found, might be not found\");\n",
    "                return {{ status: 'not_found', uen }};\n",
    "            }}\n",
    "\n",
    "            // Step 5: Find the correct company link (in a new execution context after navigation)\n",
    "            let companyLink;\n",
    "            try {{\n",
    "                companyLink = await page.evaluate((searchUen) => {{\n",
    "                    const links = Array.from(document.querySelectorAll(\"a[href*='/company/']\"));\n",
    "                    for (const a of links) {{\n",
    "                        const text = a.innerText || \"\";\n",
    "                        const href = a.href || \"\";\n",
    "                        if (text.includes(searchUen) || href.includes(searchUen.toLowerCase())) {{\n",
    "                            return a.href;\n",
    "                        }}\n",
    "                    }}\n",
    "                    return links.length > 0 ? links[0].href : null;\n",
    "                }}, uen);\n",
    "                \n",
    "                if (!companyLink) {{\n",
    "                    log.info(\"No matching company link found\");\n",
    "                    return {{ status: 'not_found', uen }};\n",
    "                }}\n",
    "                log.info(\"Found company link: \" + companyLink);\n",
    "            }} catch (evalErr) {{\n",
    "                log.error(\"Error finding company link: \" + evalErr.message);\n",
    "                return {{ status: 'error', uen, error: 'Failed to find company link: ' + evalErr.message }};\n",
    "            }}\n",
    "\n",
    "            // Step 6: Navigate to company page if not already there\n",
    "            if (page.url() !== companyLink) {{\n",
    "                try {{\n",
    "                    log.info(\"Navigating to company page...\");\n",
    "                    await page.goto(companyLink, {{ \n",
    "                        waitUntil: 'networkidle2', \n",
    "                        timeout: 60000 \n",
    "                    }});\n",
    "                    log.info(\"Company page loaded\");\n",
    "                    \n",
    "                    // Critical: Wait for page to fully stabilize\n",
    "                    await new Promise(r => setTimeout(r, 5000));\n",
    "                }} catch (gotoErr) {{\n",
    "                    log.error(\"Error navigating to company page: \" + gotoErr.message);\n",
    "                    return {{ status: 'error', uen, error: 'Failed to load company page: ' + gotoErr.message }};\n",
    "                }}\n",
    "            }}\n",
    "\n",
    "            // Step 7: Wait for content to load (with multiple fallback strategies)\n",
    "            log.info(\"Waiting for page content...\");\n",
    "            try {{\n",
    "                await Promise.race([\n",
    "                    page.waitForSelector('dt', {{ timeout: 15000 }}),\n",
    "                    page.waitForSelector('dl', {{ timeout: 15000 }}),\n",
    "                    page.waitForSelector('.max-w-7xl', {{ timeout: 15000 }}),\n",
    "                    new Promise(r => setTimeout(r, 10000)) // Fallback: just wait 10s\n",
    "                ]);\n",
    "                log.info(\"Content loaded\");\n",
    "            }} catch (contentErr) {{\n",
    "                log.warn(\"Content wait timeout, but continuing: \" + contentErr.message);\n",
    "            }}\n",
    "            \n",
    "            // Additional stabilization wait\n",
    "            await new Promise(r => setTimeout(r, 3000));\n",
    "            \n",
    "            // Step 8: Extract content (in stable context) - ONLY VISIBLE ELEMENTS\n",
    "            let html_content, title, url;\n",
    "            try {{\n",
    "                // Get only the visible HTML content by removing hidden elements\n",
    "                await page.evaluate(() => {{\n",
    "                    // Remove all elements that are hidden from view\n",
    "                    const allElements = document.querySelectorAll('*');\n",
    "                    allElements.forEach(el => {{\n",
    "                        const style = window.getComputedStyle(el);\n",
    "                        // Mark hidden elements with a special attribute\n",
    "                        if (style.display === 'none' || \n",
    "                            style.visibility === 'hidden' || \n",
    "                            style.opacity === '0' ||\n",
    "                            el.hidden ||\n",
    "                            el.hasAttribute('hidden')) {{\n",
    "                            el.setAttribute('data-hidden-element', 'true');\n",
    "                        }}\n",
    "                    }});\n",
    "                }});\n",
    "                \n",
    "                html_content = await page.content();\n",
    "                title = await page.title();\n",
    "                url = page.url();\n",
    "                log.info(\"Successfully extracted HTML content (\" + html_content.length + \" chars)\");\n",
    "            }} catch (extractErr) {{\n",
    "                log.error(\"Error extracting content: \" + extractErr.message);\n",
    "                return {{ status: 'error', uen, error: 'Failed to extract content: ' + extractErr.message }};\n",
    "            }}\n",
    "\n",
    "            return {{ status: 'success', uen, url, title, html_content }};\n",
    "            \n",
    "        }} catch (err) {{\n",
    "            log.error(\"Unexpected error in pageFunction: \" + err.message);\n",
    "            log.error(\"Stack: \" + err.stack);\n",
    "            return {{ status: 'error', uen, error: err.message }};\n",
    "        }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    run_input = {\n",
    "        \"startUrls\": [{\"url\": \"https://recordowl.com/\"}],\n",
    "        \"useChrome\": True,\n",
    "        \"headless\": True,\n",
    "        \"stealth\": True,\n",
    "        \"pageFunction\": page_function,\n",
    "        \"ignoreSslErrors\": False,\n",
    "        \"ignoreCorsAndCsp\": False,\n",
    "        \"maxRequestRetries\": 3,  # Increased retry attempts\n",
    "        \"maxRequestsPerCrawl\": 1,  # One page per run\n",
    "        \"maxConcurrency\": 1,  # No parallel requests\n",
    "        \"pageLoadTimeoutSecs\": 90,  # Optimized timeout\n",
    "        \"pageFunctionTimeoutSecs\": 180,  # 3 minutes for pageFunction\n",
    "        \"waitUntil\": [\"networkidle2\"],  # Wait for network to be idle\n",
    "        # OPTIMIZED: Residential proxies with recommended rotation\n",
    "        \"proxyConfiguration\": {\n",
    "            \"useApifyProxy\": True,\n",
    "            \"apifyProxyGroups\": [\"RESIDENTIAL\"],  # Residential IPs less likely to be blocked\n",
    "        },\n",
    "        \"proxyRotation\": \"RECOMMENDED\",  # Optimal proxy rotation strategy\n",
    "    }\n",
    "\n",
    "    # Use retry logic for 403 errors (5 attempts = more chances to recover)\n",
    "    run, error = run_apify_with_retry(client, run_input, uen, max_retries=5)\n",
    "\n",
    "    if error or not run:\n",
    "        print(f\"  ❌ Apify call failed for {uen}: {error}\")\n",
    "        all_results.append({\n",
    "            \"UEN\": uen,\n",
    "            \"Emails\": None,\n",
    "            \"Phones\": None,\n",
    "            \"Website\": None,\n",
    "            \"Facebook\": None,\n",
    "            \"LinkedIn\": None,\n",
    "            \"Instagram\": None,\n",
    "            \"TikTok\": None,\n",
    "            \"RecordOwl_Link\": None,\n",
    "            \"Error\": error or \"No run returned\"\n",
    "        })\n",
    "        time.sleep(10)  # Longer sleep after failure\n",
    "        continue\n",
    "\n",
    "    if not run or \"defaultDatasetId\" not in run:\n",
    "        print(f\"  ⚠️ No valid dataset returned for {uen}\")\n",
    "        all_results.append({\n",
    "            \"UEN\": uen,\n",
    "            \"Emails\": None,\n",
    "            \"Phones\": None,\n",
    "            \"Website\": None,\n",
    "            \"Facebook\": None,\n",
    "            \"LinkedIn\": None,\n",
    "            \"Instagram\": None,\n",
    "            \"TikTok\": None,\n",
    "            \"RecordOwl_Link\": None,\n",
    "            \"Error\": \"No dataset returned\"\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # Wait for dataset to be ready with progressive checking\n",
    "    print(f\"  ⏳ Waiting for dataset to be ready...\")\n",
    "    time.sleep(5)  # Initial wait\n",
    "    \n",
    "    # Try to fetch dataset with progressive waits\n",
    "    dataset_client = client.dataset(run[\"defaultDatasetId\"])\n",
    "    for check_attempt in range(3):\n",
    "        try:\n",
    "            # Quick check if dataset has items\n",
    "            test_fetch = dataset_client.list_items(limit=1, clean=True)\n",
    "            if test_fetch.items:\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if check_attempt < 2:\n",
    "            additional_wait = 3 * (check_attempt + 1)\n",
    "            print(f\"  ⏳ Dataset not ready, waiting {additional_wait}s more...\")\n",
    "            time.sleep(additional_wait)\n",
    "    \n",
    "    scraped_html, record_owl_url = None, None\n",
    "    \n",
    "    # Fetch dataset items with improved error handling\n",
    "    dataset_items = fetch_dataset_items_safe(\n",
    "        dataset_client,\n",
    "        max_retries=5,\n",
    "        initial_wait=5  # Increased from 3 to 5\n",
    "    )\n",
    "    \n",
    "    # Process items\n",
    "    if not dataset_items:\n",
    "        print(f\"  ⚠️ Dataset is empty - no items returned!\")\n",
    "    else:\n",
    "        print(f\"  📊 Dataset has {len(dataset_items)} item(s)\")\n",
    "    \n",
    "    for item in dataset_items:\n",
    "        if item.get(\"status\") == \"success\":\n",
    "            scraped_html = item.get(\"html_content\", \"\")\n",
    "            record_owl_url = item.get(\"url\")\n",
    "            if scraped_html:\n",
    "                print(f\"  ✅ Successfully scraped {uen} ({len(scraped_html)} chars of HTML)\")\n",
    "            else:\n",
    "                print(f\"  ⚠️ Status is 'success' but html_content is empty for {uen}\")\n",
    "        elif item.get(\"status\") == \"not_found\":\n",
    "            print(f\"  ⚠️ Company not found for UEN {uen}\")\n",
    "        elif item.get(\"status\") == \"error\":\n",
    "            print(f\"  ❌ Error for {uen}: {item.get('error')}\")\n",
    "        else:\n",
    "            print(f\"  ⚠️ Unknown item status for {uen}: {item.get('status')}\")\n",
    "            print(f\"  📋 Item keys: {list(item.keys())}\")\n",
    "\n",
    "    if not scraped_html:\n",
    "        # Determine the specific reason for failure\n",
    "        if not dataset_items:\n",
    "            error_reason = \"Dataset empty (likely 403 block at Apify level)\"\n",
    "            print(f\"  ❌ {error_reason}\")\n",
    "        elif any(item.get(\"status\") == \"not_found\" for item in dataset_items):\n",
    "            error_reason = \"Company not found on RecordOwl\"\n",
    "            print(f\"  ❌ {error_reason}\")\n",
    "        elif any(item.get(\"status\") == \"error\" for item in dataset_items):\n",
    "            error_details = [item.get(\"error\", \"Unknown\") for item in dataset_items if item.get(\"status\") == \"error\"]\n",
    "            error_reason = f\"Scraping error: {error_details[0] if error_details else 'Unknown'}\"\n",
    "            print(f\"  ❌ {error_reason}\")\n",
    "        else:\n",
    "            error_reason = \"No HTML content retrieved (unknown reason)\"\n",
    "            print(f\"  ⚠️ {error_reason}\")\n",
    "            # Debug: show what's in dataset items\n",
    "            if dataset_items:\n",
    "                print(f\"  🔍 DEBUG - First item: {dataset_items[0]}\")\n",
    "        \n",
    "        all_results.append({\n",
    "            \"UEN\": uen,\n",
    "            \"Emails\": None,\n",
    "            \"Phones\": None,\n",
    "            \"Website\": None,\n",
    "            \"Facebook\": None,\n",
    "            \"LinkedIn\": None,\n",
    "            \"Instagram\": None,\n",
    "            \"TikTok\": None,\n",
    "            \"RecordOwl_Link\": record_owl_url or None,\n",
    "            \"Error\": error_reason\n",
    "        })\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "\n",
    "    # Parse HTML\n",
    "    try:\n",
    "        soup = BeautifulSoup(scraped_html, \"html.parser\")\n",
    "        \n",
    "        # ========== REMOVE HIDDEN ELEMENTS ==========\n",
    "        # Remove all elements marked as hidden (not visible on the actual page)\n",
    "        hidden_elements = soup.find_all(attrs={\"data-hidden-element\": \"true\"})\n",
    "        removed_count = len(hidden_elements)\n",
    "        for elem in hidden_elements:\n",
    "            elem.decompose()\n",
    "        if removed_count > 0:\n",
    "            print(f\"  🗑️ Removed {removed_count} hidden elements from HTML\")\n",
    "        # ========== END REMOVE HIDDEN ELEMENTS ==========\n",
    "        \n",
    "        # ========== FIX: TARGET ONLY COMPANY OVERVIEW, EXCLUDE OFFICER DATA ==========\n",
    "        # First, try to find the overview/company info tab specifically\n",
    "        overview_tab = (\n",
    "            soup.select_one(\"#overview\") or \n",
    "            soup.select_one(\"[aria-labelledby*='overview']\") or\n",
    "            soup.select_one(\"div[role='tabpanel']\")\n",
    "        )\n",
    "        \n",
    "        if overview_tab:\n",
    "            parent = overview_tab\n",
    "            print(f\"  ✅ Targeting Overview tab only (excluding officer/director data)\")\n",
    "        else:\n",
    "            # Fallback: Get main container but REMOVE officer/director/shareholder sections\n",
    "            parent = soup.select_one(\"div.max-w-7xl.mx-auto.lg\\\\:py-6.sm\\\\:px-6.lg\\\\:px-8\")\n",
    "            if parent:\n",
    "                # Remove sections that contain personal contact info\n",
    "                for unwanted_section in parent.select(\n",
    "                    \"#officers, #shareholders, #appointments, \"\n",
    "                    \"[id*='officer'], [id*='shareholder'], [id*='appointment'], \"\n",
    "                    \".officer-section, .shareholder-section\"\n",
    "                ):\n",
    "                    unwanted_section.decompose()\n",
    "                print(f\"  🧹 Removed officer/shareholder/appointment sections from page\")\n",
    "        # ========== END FIX ==========\n",
    "        \n",
    "        # ========== REMOVE NON-VISIBLE CONTENT ==========\n",
    "        # Remove script, style, and other non-visible elements from parent\n",
    "        if parent:\n",
    "            for unwanted in parent.select(\"script, style, noscript, [style*='display:none'], [style*='display: none']\"):\n",
    "                unwanted.decompose()\n",
    "        # ========== END REMOVE NON-VISIBLE CONTENT ==========\n",
    "\n",
    "        emails, phones, website = [], [], None\n",
    "        facebook_links, linkedin_links, instagram_links, tiktok_links = [], [], [], []\n",
    "        \n",
    "        # Helper function to check if element is visible\n",
    "        def is_element_visible(element):\n",
    "            \"\"\"Check if a BeautifulSoup element appears to be visible (not hidden).\"\"\"\n",
    "            if element is None:\n",
    "                return False\n",
    "            # Check for hidden attribute\n",
    "            if element.has_attr('data-hidden-element'):\n",
    "                return False\n",
    "            # Check for common hidden styles\n",
    "            style = element.get('style', '')\n",
    "            if any(hidden_style in style.lower() for hidden_style in ['display:none', 'display: none', 'visibility:hidden', 'visibility: hidden']):\n",
    "                return False\n",
    "            # Check for hidden/aria-hidden attributes\n",
    "            if element.get('hidden') or element.get('aria-hidden') == 'true':\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        if parent:\n",
    "            # Extract emails\n",
    "            for a in parent.select(\"a[href^=mailto]\"):\n",
    "                email = a.get(\"href\", \"\").replace(\"mailto:\", \"\").strip()\n",
    "                if email and email not in emails and \"@\" in email:\n",
    "                    emails.append(email)\n",
    "\n",
    "            # ========== COMPREHENSIVE PHONE EXTRACTION ==========\n",
    "            # This extracts Singapore phone numbers with ANY spacing/formatting:\n",
    "            # - \"65 63 19 2960\" (spaces between digits)\n",
    "            # - \"6563192960\" (no spaces)\n",
    "            # - \"+65-6319-2960\" (dashes)\n",
    "            # - \"65 6 3 1 9 2 9 6 0\" (space between every digit)\n",
    "            # - \"(65) 6319 2960\" (with parentheses)\n",
    "            # Method: Extract ALL digits first, then validate pattern\n",
    "            print(f\"  🔍 Searching for phone numbers...\")\n",
    "            \n",
    "            # Method 1: Look for tel: links (most reliable) - ONLY VISIBLE ONES\n",
    "            tel_links = parent.select(\"a[href^='tel:'], a[href^='tel']\")\n",
    "            # Filter to only visible tel links\n",
    "            visible_tel_links = [link for link in tel_links if is_element_visible(link)]\n",
    "            if visible_tel_links:\n",
    "                print(f\"  📱 Found {len(visible_tel_links)} visible tel: links (filtered from {len(tel_links)} total)\")\n",
    "            \n",
    "            for a in visible_tel_links:\n",
    "                tel_href = a.get(\"href\", \"\").replace(\"tel:\", \"\").strip()\n",
    "                tel_text = a.get_text(strip=True)\n",
    "                print(f\"  📞 Tel link - href: '{tel_href}', text: '{tel_text}'\")\n",
    "                \n",
    "                # Extract all digits from tel link\n",
    "                digits_only = re.sub(r\"\\D\", \"\", tel_href)\n",
    "                print(f\"  🔢 Tel digits: {digits_only}\")\n",
    "                \n",
    "                # Handle different digit lengths\n",
    "                if len(digits_only) == 10 and digits_only.startswith(\"65\") and digits_only[2] in \"689\":\n",
    "                    # 10 digits starting with 65 (e.g., \"6563192960\")\n",
    "                    formatted = \"+\" + digits_only\n",
    "                    if formatted not in phones:\n",
    "                        phones.append(formatted)\n",
    "                        print(f\"  ✅ Added from tel link (10 digits): {formatted}\")\n",
    "                elif len(digits_only) == 8 and digits_only[0] in \"689\":\n",
    "                    # 8 digits starting with 6/8/9 (e.g., \"63192960\")\n",
    "                    formatted = \"+65\" + digits_only\n",
    "                    if formatted not in phones:\n",
    "                        phones.append(formatted)\n",
    "                        print(f\"  ✅ Added from tel link (8 digits): {formatted}\")\n",
    "                elif len(digits_only) > 10:\n",
    "                    # More than 10 digits, try to find valid pattern\n",
    "                    print(f\"  🔍 Searching within {len(digits_only)} digits for valid pattern...\")\n",
    "                    found = False\n",
    "                    # Look for 65 followed by 6/8/9\n",
    "                    for i in range(len(digits_only) - 9):\n",
    "                        if digits_only[i:i+2] == \"65\" and digits_only[i+2] in \"689\":\n",
    "                            formatted = \"+\" + digits_only[i:i+10]\n",
    "                            if formatted not in phones:\n",
    "                                phones.append(formatted)\n",
    "                                print(f\"  ✅ Added from tel link (extracted): {formatted}\")\n",
    "                            found = True\n",
    "                            break\n",
    "                    if not found:\n",
    "                        # Try last 8 digits if they start with 6/8/9\n",
    "                        if digits_only[-8] in \"689\":\n",
    "                            formatted = \"+65\" + digits_only[-8:]\n",
    "                            if formatted not in phones:\n",
    "                                phones.append(formatted)\n",
    "                                print(f\"  ✅ Added from tel link (last 8 digits): {formatted}\")\n",
    "            \n",
    "            # Method 2: Look in dt/dd structure with broader keywords - ONLY VISIBLE ONES\n",
    "            dt_tags = parent.select(\"dt\")\n",
    "            # Filter to only visible dt tags\n",
    "            visible_dt_tags = [dt for dt in dt_tags if is_element_visible(dt)]\n",
    "            if visible_dt_tags:\n",
    "                print(f\"  📋 Found {len(visible_dt_tags)} visible dt tags (filtered from {len(dt_tags)} total)\")\n",
    "            \n",
    "            for dt in visible_dt_tags:\n",
    "                dt_text = dt.get_text(strip=True).lower()\n",
    "                \n",
    "                # ========== IMPROVED: Stricter filtering for company-level contacts ==========\n",
    "                # Company-level keywords (preferred)\n",
    "                company_contact_keywords = [\n",
    "                    \"company contact\", \"business contact\", \"office phone\", \n",
    "                    \"main phone\", \"business phone\", \"company phone\"\n",
    "                ]\n",
    "                \n",
    "                # General contact keywords (accepted if no personal identifiers)\n",
    "                general_contact_keywords = [\"contact number\", \"phone\", \"tel\", \"mobile\", \"call\", \"contact no\"]\n",
    "                \n",
    "                # EXCLUDE personal contact fields\n",
    "                exclude_keywords = [\n",
    "                    \"officer\", \"charge\", \"employee\", \"shareholder\", \"director\", \n",
    "                    \"registration\", \"person\", \"individual\", \"member\", \"partner\",\n",
    "                    \"manager\", \"owner\", \"proprietor\", \"authorized\", \"representative\",\n",
    "                    \"appointment\", \"designation\", \"name of\", \"appointed\"\n",
    "                ]\n",
    "                \n",
    "                # Check if this is a company-level contact\n",
    "                is_company_contact = any(kw in dt_text for kw in company_contact_keywords)\n",
    "                is_general_contact = any(kw in dt_text for kw in general_contact_keywords)\n",
    "                is_excluded = any(excl in dt_text for excl in exclude_keywords)\n",
    "                \n",
    "                # Only extract if it's explicitly company contact OR general contact without exclusions\n",
    "                if (is_company_contact or (is_general_contact and not is_excluded)):\n",
    "                    dd = dt.find_next_sibling(\"dd\")\n",
    "                    # Check if dd is also visible\n",
    "                    if dd and is_element_visible(dd):\n",
    "                        number_text = dd.get_text(\" \", strip=True)\n",
    "                        \n",
    "                        # Debug: Show where this phone is coming from\n",
    "                        contact_type = \"COMPANY\" if is_company_contact else \"GENERAL\"\n",
    "                        print(f\"  📝 [{contact_type}] Field '{dt_text}': {number_text}\")\n",
    "                        \n",
    "                        # Extract all digits and check if it forms a valid phone number\n",
    "                        all_digits = re.sub(r\"\\D\", \"\", number_text)\n",
    "                        print(f\"  🔢 Extracted digits: {all_digits}\")\n",
    "                        \n",
    "                        # Check for Singapore phone patterns in the digits\n",
    "                        # Pattern 1: 10 digits starting with 65\n",
    "                        if len(all_digits) == 10 and all_digits.startswith(\"65\") and all_digits[2] in \"689\":\n",
    "                            formatted = \"+\" + all_digits\n",
    "                            if formatted not in phones:\n",
    "                                phones.append(formatted)\n",
    "                                print(f\"  ✅ Added from dt/dd (10 digits): {formatted}\")\n",
    "                        # Pattern 2: 8 digits starting with 6, 8, or 9\n",
    "                        elif len(all_digits) == 8 and all_digits[0] in \"689\":\n",
    "                            formatted = \"+65\" + all_digits\n",
    "                            if formatted not in phones:\n",
    "                                phones.append(formatted)\n",
    "                                print(f\"  ✅ Added from dt/dd (8 digits): {formatted}\")\n",
    "                        # Pattern 3: More than 10 digits, try to extract 10-digit number starting with 65\n",
    "                        elif len(all_digits) > 10:\n",
    "                            # Look for 65 followed by 6/8/9 in the digit string\n",
    "                            for i in range(len(all_digits) - 9):\n",
    "                                if all_digits[i:i+2] == \"65\" and all_digits[i+2] in \"689\":\n",
    "                                    potential_number = all_digits[i:i+10]\n",
    "                                    formatted = \"+\" + potential_number\n",
    "                                    if formatted not in phones:\n",
    "                                        phones.append(formatted)\n",
    "                                        print(f\"  ✅ Added from dt/dd (extracted): {formatted}\")\n",
    "                                    break\n",
    "            \n",
    "            # Method 3: Search entire parent for phone patterns if none found\n",
    "            # Note: This only searches visible content since hidden elements were already removed\n",
    "            if not phones:\n",
    "                print(f\"  🔎 No phones found yet, searching entire visible content...\")\n",
    "                full_text = parent.get_text()\n",
    "                \n",
    "                # Ultra-comprehensive patterns to catch ALL spacing variations\n",
    "                # These patterns allow unlimited spaces/dashes between digits\n",
    "                patterns = [\n",
    "                    # Pattern 1: +65 with any spacing (e.g., \"+65 6 3 1 9 2 9 6 0\", \"+65-6319-2960\")\n",
    "                    r\"\\+[\\s\\-]*65[\\s\\-]+[689][\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d\",\n",
    "                    # Pattern 2: (65) with any spacing\n",
    "                    r\"\\([\\s\\-]*65[\\s\\-]*\\)[\\s\\-]*[689][\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d\",\n",
    "                    # Pattern 3: 65 without + or () but with space/dash (e.g., \"65 6 3 1 9 2 9 6 0\", \"65-6319-2960\")\n",
    "                    r\"(?<!\\d)65[\\s\\-]+[689][\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d(?!\\d)\",\n",
    "                    # Pattern 4: Just 8 digits starting with 6/8/9 with any spacing\n",
    "                    r\"(?<!\\d)[689][\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d(?!\\d)\",\n",
    "                ]\n",
    "                \n",
    "                for pattern_idx, pattern in enumerate(patterns, 1):\n",
    "                    matches = re.findall(pattern, full_text)\n",
    "                    if matches:\n",
    "                        print(f\"  🔍 Pattern {pattern_idx} found {len(matches)} potential matches\")\n",
    "                    \n",
    "                    for match in matches:\n",
    "                        # Extract only digits\n",
    "                        digits = re.sub(r\"\\D\", \"\", match)\n",
    "                        print(f\"  🔢 Pattern {pattern_idx} match: '{match.strip()}' → digits: '{digits}'\")\n",
    "                        \n",
    "                        # Validate and format\n",
    "                        if len(digits) == 10 and digits.startswith(\"65\") and digits[2] in \"689\":\n",
    "                            formatted = \"+\" + digits\n",
    "                            if formatted not in phones:\n",
    "                                phones.append(formatted)\n",
    "                                print(f\"  ✅ Added from pattern {pattern_idx} (10 digits): {formatted}\")\n",
    "                        elif len(digits) == 8 and digits[0] in \"689\":\n",
    "                            formatted = \"+65\" + digits\n",
    "                            if formatted not in phones:\n",
    "                                phones.append(formatted)\n",
    "                                print(f\"  ✅ Added from pattern {pattern_idx} (8 digits): {formatted}\")\n",
    "                        elif len(digits) > 10:\n",
    "                            # Try to find a valid 10-digit number within\n",
    "                            for i in range(len(digits) - 9):\n",
    "                                if digits[i:i+2] == \"65\" and digits[i+2] in \"689\":\n",
    "                                    potential = digits[i:i+10]\n",
    "                                    formatted = \"+\" + potential\n",
    "                                    if formatted not in phones:\n",
    "                                        phones.append(formatted)\n",
    "                                        print(f\"  ✅ Added from pattern {pattern_idx} (extracted): {formatted}\")\n",
    "                                    break\n",
    "            \n",
    "            if phones:\n",
    "                print(f\"  ✅ Total phones found: {phones}\")\n",
    "            else:\n",
    "                print(f\"  ⚠️ WARNING: No phone numbers found for {uen}\")\n",
    "                print(f\"  📄 Showing first 500 chars of parent HTML for debugging:\")\n",
    "                print(parent.prettify()[:500] + \"...\")\n",
    "            # ========== END PHONE EXTRACTION ==========\n",
    "\n",
    "            # Extract website\n",
    "            valid_websites = []\n",
    "            for a in parent.select(\"a[href^=http]\"):\n",
    "                href = a.get(\"href\", \"\").strip()\n",
    "                href_lower = href.lower()\n",
    "                if not any(domain in href_lower for domain in SOCIAL_MEDIA_DOMAINS):\n",
    "                    if not any(skip in href_lower for skip in [\"recordowl\", \"apify.com\"]):\n",
    "                        if any(tld in href for tld in [\".com\", \".sg\", \".net\", \".org\", \".co\"]):\n",
    "                            valid_websites.append(href)\n",
    "            website = valid_websites[0] if valid_websites else None\n",
    "\n",
    "        # Extract social media links from entire page\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a[\"href\"].strip().lower()\n",
    "            if \"facebook.com\" in href and href not in facebook_links:\n",
    "                facebook_links.append(href)\n",
    "            elif \"linkedin.com\" in href and href not in linkedin_links:\n",
    "                linkedin_links.append(href)\n",
    "            elif \"instagram.com\" in href and href not in instagram_links:\n",
    "                instagram_links.append(href)\n",
    "            elif \"tiktok.com\" in href and href not in tiktok_links:\n",
    "                tiktok_links.append(href)\n",
    "\n",
    "        all_results.append({\n",
    "            \"UEN\": uen,\n",
    "            \"Emails\": emails if emails else None,\n",
    "            \"Phones\": phones if phones else None,\n",
    "            \"Website\": website,\n",
    "            \"Facebook\": list(set(facebook_links)) if facebook_links else None,\n",
    "            \"LinkedIn\": list(set(linkedin_links)) if linkedin_links else None,\n",
    "            \"Instagram\": list(set(instagram_links)) if instagram_links else None,\n",
    "            \"TikTok\": list(set(tiktok_links)) if tiktok_links else None,\n",
    "            \"RecordOwl_Link\": record_owl_url,\n",
    "        })\n",
    "        print(f\"  ✅ Processed {uen}: {len(emails) if emails else 0} emails, {len(phones) if phones else 0} phones\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error parsing HTML for {uen}: {e}\")\n",
    "        all_results.append({\n",
    "            \"UEN\": uen,\n",
    "            \"Emails\": None,\n",
    "            \"Phones\": None,\n",
    "            \"Website\": None,\n",
    "            \"Facebook\": None,\n",
    "            \"LinkedIn\": None,\n",
    "            \"Instagram\": None,\n",
    "            \"TikTok\": None,\n",
    "            \"RecordOwl_Link\": record_owl_url or None,\n",
    "            \"Error\": f\"HTML parsing error: {str(e)}\"\n",
    "        })\n",
    "\n",
    "    # Dynamic sleep time to avoid rate limiting and 403 blocks\n",
    "    # Longer delays reduce detection and blocking\n",
    "    base_sleep = 20  # Increased from 10\n",
    "    random_addition = (idx % 10) + 5  # 5-14 seconds random\n",
    "    sleep_time = base_sleep + random_addition  # 25-34 seconds total\n",
    "\n",
    "    print(f\"  💤 Sleeping for {sleep_time}s before next request...\")\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "    # Extra delay after every 5th request to further avoid detection\n",
    "    if idx % 5 == 0:\n",
    "        extra_wait = 30\n",
    "        print(f\"  🛑 Checkpoint pause: waiting extra {extra_wait}s...\")\n",
    "        time.sleep(extra_wait)\n",
    "\n",
    "New_Fresh_Leads = pd.DataFrame(all_results)\n",
    "print(\"\\n✅ Scraping complete!\")\n",
    "print(f\"\\n📊 Results summary:\")\n",
    "print(f\"   Total processed: {len(New_Fresh_Leads)}\")\n",
    "print(f\"   With emails: {New_Fresh_Leads['Emails'].notna().sum()}\")\n",
    "print(f\"   With phones: {New_Fresh_Leads['Phones'].notna().sum()}\")\n",
    "print(f\"   With websites: {New_Fresh_Leads['Website'].notna().sum()}\")\n",
    "\n",
    "New_Fresh_Leads.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce63aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "New_Fresh_Leads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78537b1",
   "metadata": {},
   "source": [
    "### Append and save into exel sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c4e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Load both Excel files\n",
    "# file_path_1 = \"Fresh_Leads.xlsx\"\n",
    "# Fresh_Leads = pd.read_excel(file_path_1)\n",
    "\n",
    "# # file_path_2 = \"recordowl_results_4.xlsx\"\n",
    "# # recordowl_results_4 = pd.read_excel(file_path_2)\n",
    "\n",
    "# # Append (combine) them\n",
    "# combined_df = pd.concat([Fresh_Leads, Fresh_Leads_with_phones], ignore_index=True)\n",
    "\n",
    "# # Optional: Save to a new Excel file\n",
    "# combined_df.to_excel(\"Fresh_Leads_New.xlsx\", index=False)\n",
    "\n",
    "# # Preview\n",
    "# combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257fbbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_non_nan = combined_df['Phones'].notna().sum()\n",
    "# print(count_non_nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0a4e0f",
   "metadata": {},
   "source": [
    "### Website Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ccaea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import httpx\n",
    "# import asyncio\n",
    "\n",
    "# # =====================================================\n",
    "# # Validate Website (only if no phone number)\n",
    "# # =====================================================\n",
    "# async def check_url(url: str) -> bool:\n",
    "#     \"\"\"Return True if the URL is reachable (status < 400).\"\"\"\n",
    "#     if not url:\n",
    "#         return False\n",
    "#     try:\n",
    "#         async with httpx.AsyncClient(follow_redirects=True, timeout=5) as client:\n",
    "#             response = await client.head(url)\n",
    "#             return response.status_code < 400\n",
    "#     except Exception:\n",
    "#         return False\n",
    "\n",
    "\n",
    "# async def validate_if_needed(df):\n",
    "#     \"\"\"Validate websites only if phone number is missing.\"\"\"\n",
    "#     for i, row in df.iterrows():\n",
    "#         url = row.get(\"Website\")\n",
    "#         phone = row.get(\"Phones\")\n",
    "\n",
    "#         # Skip validation if phone exists\n",
    "#         if phone:\n",
    "#             df.at[i, \"Website_Valid\"] = None\n",
    "#             continue\n",
    "\n",
    "#         # Validate website if no phone\n",
    "#         if url:\n",
    "#             is_valid = await check_url(url)\n",
    "#             df.at[i, \"Website_Valid\"] = \"valid\" if is_valid else \"invalid\"\n",
    "#         else:\n",
    "#             df.at[i, \"Website_Valid\"] = \"invalid\"\n",
    "\n",
    "#     return df\n",
    "\n",
    "\n",
    "# # =====================================================\n",
    "# # Run async validation safely inside Jupyter\n",
    "# # =====================================================\n",
    "# result_df = await validate_if_needed(result_df)\n",
    "\n",
    "# # =====================================================\n",
    "# # Final output\n",
    "# # =====================================================\n",
    "# display(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c268400",
   "metadata": {},
   "source": [
    "### If contact number is invalid, then webscrapped website to get contact number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e742795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import asyncio\n",
    "# import os\n",
    "# import time\n",
    "# from apify_client import ApifyClient\n",
    "\n",
    "# # --- Initialize Apify client ---\n",
    "# APIFY_TOKEN = os.getenv(\"APIFY_TOKEN\", \"apify_api_0HQ8fc5fw5T1aosdacxKQNQYVBAEwi3tXaJc\")\n",
    "# client = ApifyClient(APIFY_TOKEN)\n",
    "\n",
    "# # --- Async wrapper so you can run in Jupyter ---\n",
    "# async def enrich_with_contact_info(df):\n",
    "#     \"\"\"Scrape contact info for rows where Website_Valid == 'valid' and Phones is empty.\"\"\"\n",
    "#     updated_df = df.copy()\n",
    "\n",
    "#     for i, row in df.iterrows():\n",
    "#         website = row.get(\"Website\")\n",
    "#         status = row.get(\"Website_Valid\")\n",
    "#         phone = row.get(\"Phones\")\n",
    "\n",
    "#         if not website or status != \"valid\" or phone:\n",
    "#             continue  # Skip invalid or already complete rows\n",
    "\n",
    "#         print(f\"🔍 Scraping contact page for: {website}\")\n",
    "\n",
    "#         # --- CONVERTED TO PUPPETEER-SCRAPER (same as Cell 20) ---\n",
    "#         # Now using native Puppeteer syntax instead of jQuery\n",
    "#         run_input = {\n",
    "#             \"startUrls\": [{\"url\": website}],\n",
    "#             \"pageFunction\": r\"\"\"\n",
    "#                 async function pageFunction(context) {\n",
    "#                     const { page, log, request } = context;\n",
    "#                     const isContact = request.userData?.isContact || false;\n",
    "\n",
    "#                     // If not on contact page yet, try to find and navigate to it\n",
    "#                     if (!isContact) {\n",
    "#                         try {\n",
    "#                             // Wait for page to load\n",
    "#                             await page.waitForSelector('a', { timeout: 10000 }).catch(() => null);\n",
    "                            \n",
    "#                             // Find contact page link using Puppeteer\n",
    "#                             const contactUrl = await page.evaluate(() => {\n",
    "#                                 const links = Array.from(document.querySelectorAll('a[href]'));\n",
    "#                                 for (const link of links) {\n",
    "#                                     const href = link.getAttribute('href');\n",
    "#                                     if (href && href.toLowerCase().includes('contact')) {\n",
    "#                                         return href.startsWith('http') ? href : window.location.origin + href;\n",
    "#                                     }\n",
    "#                                 }\n",
    "#                                 return null;\n",
    "#                             });\n",
    "\n",
    "#                             if (contactUrl) {\n",
    "#                                 await context.enqueueRequest({ \n",
    "#                                     url: contactUrl, \n",
    "#                                     userData: { isContact: true } \n",
    "#                                 });\n",
    "#                                 log.info(`Enqueued contact page: ${contactUrl}`);\n",
    "#                             }\n",
    "#                             return null;\n",
    "#                         } catch (err) {\n",
    "#                             log.error(`Error finding contact page: ${err.message}`);\n",
    "#                             return null;\n",
    "#                         }\n",
    "#                     }\n",
    "\n",
    "#                     // We're on the contact page - extract emails and phones\n",
    "#                     try {\n",
    "#                         // Wait for content to load\n",
    "#                         await new Promise(r => setTimeout(r, 3000));\n",
    "\n",
    "#                         // Extract emails and phones using Puppeteer\n",
    "#                         const contactData = await page.evaluate(() => {\n",
    "#                             // Helper: check if element is visible\n",
    "#                             function isVisible(el) {\n",
    "#                                 return el && el.offsetParent !== null;\n",
    "#                             }\n",
    "\n",
    "#                             // Extract emails from mailto links\n",
    "#                             const emailLinks = Array.from(document.querySelectorAll('a[href^=\"mailto\"]'));\n",
    "#                             const emails = emailLinks\n",
    "#                                 .filter(el => isVisible(el))\n",
    "#                                 .map(el => el.getAttribute('href').replace('mailto:', '').trim())\n",
    "#                                 .filter(email => email.length > 0);\n",
    "\n",
    "#                             // Extract phones from tel links\n",
    "#                             const phoneLinks = Array.from(document.querySelectorAll('a[href^=\"tel\"]'));\n",
    "#                             const phones = phoneLinks\n",
    "#                                 .filter(el => isVisible(el))\n",
    "#                                 .map(el => el.getAttribute('href').replace(/[^0-9]/g, ''))\n",
    "#                                 .filter(phone => phone.length > 0);\n",
    "\n",
    "#                             return {\n",
    "#                                 emails: [...new Set(emails)],\n",
    "#                                 phones: [...new Set(phones)]\n",
    "#                             };\n",
    "#                         });\n",
    "\n",
    "#                         return {\n",
    "#                             contactUrl: request.url,\n",
    "#                             emails: contactData.emails.length ? contactData.emails : [],\n",
    "#                             phones: contactData.phones.length ? contactData.phones : []\n",
    "#                         };\n",
    "#                     } catch (err) {\n",
    "#                         log.error(`Error extracting contact data: ${err.message}`);\n",
    "#                         return {\n",
    "#                             contactUrl: request.url,\n",
    "#                             emails: [],\n",
    "#                             phones: [],\n",
    "#                             error: err.message\n",
    "#                         };\n",
    "#                     }\n",
    "#                 }\n",
    "#             \"\"\",\n",
    "#             \"useChrome\": True,\n",
    "#             \"headless\": True,\n",
    "#             \"stealth\": True,\n",
    "#             \"ignoreSslErrors\": False,\n",
    "#             \"ignoreCorsAndCsp\": False,\n",
    "#             \"maxRequestRetries\": 3,  # Increased retry attempts\n",
    "#             \"maxRequestsPerCrawl\": 0,  # No limit (will crawl main + contact pages)\n",
    "#             \"maxConcurrency\": 1,  # No parallel requests\n",
    "#             \"pageLoadTimeoutSecs\": 90,  # Optimized timeout\n",
    "#             \"pageFunctionTimeoutSecs\": 180,  # 3 minutes for pageFunction\n",
    "#             \"waitUntil\": [\"networkidle2\"],  # Wait for network to be idle\n",
    "#             # OPTIMIZED: Residential proxies with recommended rotation\n",
    "#             \"proxyConfiguration\": {\n",
    "#                 \"useApifyProxy\": True,\n",
    "#                 \"apifyProxyGroups\": [\"RESIDENTIAL\"],  # Residential IPs less likely to be blocked\n",
    "#             },\n",
    "#             \"proxyRotation\": \"RECOMMENDED\",  # Optimal proxy rotation strategy\n",
    "#         }\n",
    "\n",
    "#         # --- Run the Apify scraper (NOW USING PUPPETEER-SCRAPER) ---\n",
    "#         try:\n",
    "#             print(f\"  📡 Starting Apify puppeteer-scraper...\")\n",
    "#             run = client.actor(\"apify/puppeteer-scraper\").call(run_input=run_input)\n",
    "            \n",
    "#             # Wait for dataset to be ready\n",
    "#             time.sleep(3)\n",
    "            \n",
    "#             dataset = client.dataset(run[\"defaultDatasetId\"])\n",
    "#             results = list(dataset.iterate_items())\n",
    "#             contact_results = [r for r in results if r and (r.get(\"emails\") or r.get(\"phones\"))]\n",
    "\n",
    "#             if contact_results:\n",
    "#                 scraped = contact_results[0]\n",
    "#                 updated_df.at[i, \"Emails\"] = scraped.get(\"emails\", None)\n",
    "#                 updated_df.at[i, \"Phones\"] = scraped.get(\"phones\", None)\n",
    "#                 updated_df.at[i, \"Contact_Page\"] = scraped.get(\"contactUrl\", None)\n",
    "#                 print(f\"  ✅ Found: {scraped.get('phones', [])} / {scraped.get('emails', [])}\")\n",
    "#             else:\n",
    "#                 print(\"  ⚠️ No contact data found.\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"  ❌ Error scraping {website}: {e}\")\n",
    "        \n",
    "#         # Add delay to avoid rate limiting\n",
    "#         time.sleep(5)\n",
    "\n",
    "#     return updated_df\n",
    "\n",
    "\n",
    "# # --- Run the scraper for valid websites ---\n",
    "# result_df = await enrich_with_contact_info(result_df)\n",
    "\n",
    "# # --- Display updated results ---\n",
    "# display(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad033e9f",
   "metadata": {},
   "source": [
    "### Facebook Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the ApifyClient with your API token\n",
    "# client = ApifyClient(\"apify_api_yNR85etaHpLtBzPoVozVVXUsCZe54u2Ffog1\")\n",
    "\n",
    "# # Function to validate Singapore phone numbers (MUST have country code)\n",
    "# def validate_singapore_number(phone):\n",
    "#     if not phone:\n",
    "#         return None\n",
    "    \n",
    "#     # Remove all spaces, dashes, parentheses\n",
    "#     cleaned = re.sub(r'[\\s\\-\\(\\)]', '', str(phone))\n",
    "    \n",
    "#     # MUST have country code: +65XXXXXXXX or 65XXXXXXXX\n",
    "#     # First digit after country code must be 6, 8, or 9\n",
    "#     # Total of 8 digits after country code\n",
    "#     if re.match(r'^\\+?65[689]\\d{7}$', cleaned):\n",
    "#         return phone  # Return original format\n",
    "    \n",
    "#     # Not a valid Singapore number with country code\n",
    "#     return None\n",
    "\n",
    "# # Prepare the Actor input\n",
    "# run_input = {\n",
    "#     \"pages\": [\n",
    "#         \"https://www.facebook.com/KPECTHub/\",\n",
    "#     ],\n",
    "#     \"language\": \"en-US\",\n",
    "# }\n",
    "\n",
    "# # Run the Actor and wait for it to finish\n",
    "# run = client.actor(\"oJ48ceKNY7ueGPGL0\").call(run_input=run_input)\n",
    "\n",
    "# # Collect results\n",
    "# results = []\n",
    "# for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "#     # Extract phone from multiple possible fields\n",
    "#     raw_phone = item.get('phone', None) or item.get('wa_number', None)\n",
    "    \n",
    "#     # Validate it's a Singapore number WITH country code\n",
    "#     phone = validate_singapore_number(raw_phone)\n",
    "    \n",
    "#     # Extract email\n",
    "#     email = item.get('email', None)\n",
    "    \n",
    "#     # Extract website from the websites list (take first non-Google Maps link if available)\n",
    "#     websites = item.get('websites', [])\n",
    "#     website = None\n",
    "#     if websites:\n",
    "#         # Filter out Google Maps links and take the first real website\n",
    "#         real_websites = [w for w in websites if 'maps.google.com' not in w]\n",
    "#         website = real_websites[0] if real_websites else websites[0]\n",
    "    \n",
    "#     results.append({\n",
    "#         'facebook_url': item.get('facebookUrl', None),\n",
    "#         'page_name': item.get('pageName', None),\n",
    "#         'phone': phone,  # Only Singapore numbers WITH country code or None\n",
    "#         'email': email,\n",
    "#         'website': website,\n",
    "#         'address': item.get('address', None)\n",
    "#     })\n",
    "\n",
    "# # Create DataFrame\n",
    "# df = pd.DataFrame(results)\n",
    "\n",
    "# df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
