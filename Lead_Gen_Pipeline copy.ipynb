{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "328e86f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from apify_client import ApifyClient\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from requests.exceptions import HTTPError, ConnectionError\n",
    "from urllib3.exceptions import ProtocolError\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767bf86c",
   "metadata": {},
   "source": [
    "### Getting Master DB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa7f0eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UEN</th>\n",
       "      <th>ACRA_REGISTERED_NAME</th>\n",
       "      <th>BRAND_NAME</th>\n",
       "      <th>SSIC_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04799400B</td>\n",
       "      <td>AIK BEE TEXTILE CO</td>\n",
       "      <td>AIK BEE TEXTILE CO</td>\n",
       "      <td>46411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03376200K</td>\n",
       "      <td>SERANGOON GARDEN CLINIC AND DISPENSARY</td>\n",
       "      <td>GARDEN CLINIC</td>\n",
       "      <td>550263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06239600E</td>\n",
       "      <td>SALON DE BENZIMEN</td>\n",
       "      <td>SALON DE BENZIMEN</td>\n",
       "      <td>96021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06952000C</td>\n",
       "      <td>SU LAN LADIES FASHION</td>\n",
       "      <td>SU LAN LADIES FASHION</td>\n",
       "      <td>14103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10381600C</td>\n",
       "      <td>SIN HAI PRINTING SERVICE</td>\n",
       "      <td>SIN HAI PRINTING SERVICE</td>\n",
       "      <td>18113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7444</th>\n",
       "      <td>201734006N</td>\n",
       "      <td>MISTER MOBILE HOUGANG PTE. LTD.</td>\n",
       "      <td>MISTER MOBILE (HOUGANG)</td>\n",
       "      <td>95120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7445</th>\n",
       "      <td>202210879W</td>\n",
       "      <td>MISTER MOBILE CHINATOWN PTE. LTD.</td>\n",
       "      <td>MISTER MOBILE (CHINATOWN)</td>\n",
       "      <td>47411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7446</th>\n",
       "      <td>202205507G</td>\n",
       "      <td>MISTER MOBILE PTE. LTD.</td>\n",
       "      <td>MISTER MOBILE HQ</td>\n",
       "      <td>64202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7454</th>\n",
       "      <td>53473046M</td>\n",
       "      <td>BLOONIES</td>\n",
       "      <td>BLOONIES</td>\n",
       "      <td>47742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7455</th>\n",
       "      <td>53478373B</td>\n",
       "      <td>BLOOMSNBALLOONS</td>\n",
       "      <td>BLOOMS AND BALLOONS</td>\n",
       "      <td>47742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6734 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             UEN                    ACRA_REGISTERED_NAME  \\\n",
       "0      04799400B                      AIK BEE TEXTILE CO   \n",
       "1      03376200K  SERANGOON GARDEN CLINIC AND DISPENSARY   \n",
       "2      06239600E                       SALON DE BENZIMEN   \n",
       "3      06952000C                   SU LAN LADIES FASHION   \n",
       "4      10381600C                SIN HAI PRINTING SERVICE   \n",
       "...          ...                                     ...   \n",
       "7444  201734006N         MISTER MOBILE HOUGANG PTE. LTD.   \n",
       "7445  202210879W       MISTER MOBILE CHINATOWN PTE. LTD.   \n",
       "7446  202205507G                 MISTER MOBILE PTE. LTD.   \n",
       "7454   53473046M                                BLOONIES   \n",
       "7455   53478373B                         BLOOMSNBALLOONS   \n",
       "\n",
       "                     BRAND_NAME  SSIC_CODE  \n",
       "0            AIK BEE TEXTILE CO      46411  \n",
       "1                 GARDEN CLINIC     550263  \n",
       "2             SALON DE BENZIMEN      96021  \n",
       "3         SU LAN LADIES FASHION      14103  \n",
       "4      SIN HAI PRINTING SERVICE      18113  \n",
       "...                         ...        ...  \n",
       "7444    MISTER MOBILE (HOUGANG)      95120  \n",
       "7445  MISTER MOBILE (CHINATOWN)      47411  \n",
       "7446           MISTER MOBILE HQ      64202  \n",
       "7454                   BLOONIES      47742  \n",
       "7455        BLOOMS AND BALLOONS      47742  \n",
       "\n",
       "[6734 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- CONFIG ---\n",
    "file_path = \"./Master DB/Master_DB_oct22.xlsx\"\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "def clean_uen(u: str) -> str | None:\n",
    "    if pd.isna(u):\n",
    "        return None\n",
    "    return re.sub(r\"[^A-Z0-9]\", \"\", str(u).upper().strip())\n",
    "\n",
    "def clean_text(text: str) -> str | None:\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    text = str(text).strip().upper()\n",
    "    return None if text == \"NAN\" else text\n",
    "\n",
    "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert all column names to uppercase, replace non-alphanumeric with single underscore, remove trailing underscores.\"\"\"\n",
    "    new_cols = []\n",
    "    for col in df.columns:\n",
    "        col_std = re.sub(r\"[^A-Z0-9]\", \"_\", col.upper().strip())\n",
    "        col_std = re.sub(r\"_+\", \"_\", col_std)  # Replace multiple underscores with single\n",
    "        col_std = col_std.strip(\"_\")  # Remove leading/trailing underscores\n",
    "        new_cols.append(col_std)\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "# --- LOAD DATA ---\n",
    "master_db_df = pd.read_excel(file_path)\n",
    "\n",
    "# --- SELECT RELEVANT COLUMNS ---\n",
    "columns_to_keep = [\n",
    "    \"Company Registration Number (UEN)\",\n",
    "    \"ACRA REGISTERED NAME\",\n",
    "    \"Brand/Deal Name/Business Name\",\n",
    "    \"Primary SSIC Code\",\n",
    "    \"PIC NAME 1 Contact Number\",\n",
    "    \"PIC 1 email address\",\n",
    "    \"Website URL\",\n",
    "    \"Parent Industry Type\",\n",
    "    \"Sub Industry\"\n",
    "]\n",
    "master_db_df = master_db_df[columns_to_keep].copy()\n",
    "\n",
    "# --- STANDARDIZE COLUMN NAMES ---\n",
    "master_db_df = standardize_columns(master_db_df)\n",
    "\n",
    "# --- CLEANING & RENAME SPECIFIC COLUMNS ---\n",
    "# Dynamically find the UEN column (first column containing 'UEN')\n",
    "uen_col = [c for c in master_db_df.columns if \"UEN\" in c][0]\n",
    "master_db_df[\"UEN\"] = master_db_df[uen_col].apply(clean_uen)\n",
    "master_db_df = master_db_df.drop(columns=[uen_col])\n",
    "\n",
    "# Rename other columns consistently\n",
    "rename_map = {\n",
    "    \"BRAND_DEAL_NAME_BUSINESS_NAME\": \"BRAND_NAME\",\n",
    "    \"PRIMARY_SSIC_CODE\": \"SSIC_CODE\",\n",
    "    \"ACRA_REGISTERED_NAME\": \"ACRA_REGISTERED_NAME\"\n",
    "}\n",
    "master_db_df = master_db_df.rename(columns={k: v for k, v in rename_map.items() if k in master_db_df.columns})\n",
    "\n",
    "# Clean text columns\n",
    "for col in [\"ACRA_REGISTERED_NAME\", \"BRAND_NAME\"]:\n",
    "    if col in master_db_df.columns:\n",
    "        master_db_df[col] = master_db_df[col].apply(clean_text)\n",
    "\n",
    "# Convert SSIC_CODE to integer if exists\n",
    "if \"SSIC_CODE\" in master_db_df.columns:\n",
    "    master_db_df[\"SSIC_CODE\"] = master_db_df[\"SSIC_CODE\"].astype(\"Int64\")\n",
    "\n",
    "# Keep only required columns if they exist\n",
    "required_cols = [\"UEN\", \"ACRA_REGISTERED_NAME\", \"BRAND_NAME\", \"SSIC_CODE\"]\n",
    "master_db_df = master_db_df[[c for c in required_cols if c in master_db_df.columns]]\n",
    "\n",
    "# Filter out rows with missing or empty UEN\n",
    "master_db_df = master_db_df[master_db_df[\"UEN\"].notna() & (master_db_df[\"UEN\"].str.strip() != \"\")]\n",
    "\n",
    "master_db_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e98cff",
   "metadata": {},
   "source": [
    "### Getting ACRA Data (Filter by Live, Live Company only & non relevant ssic code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e678fbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Folder containing your CSVs\n",
    "# -------------------------------------------------------------\n",
    "folder_path = \"Acra_Data\"\n",
    "\n",
    "# Get all CSV file paths inside the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "# Read and combine all CSVs\n",
    "# Using low_memory=False to avoid DtypeWarning for mixed types\n",
    "df = pd.concat((pd.read_csv(f, low_memory=False) for f in csv_files), ignore_index=True)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Convert all column names to uppercase\n",
    "# -------------------------------------------------------------\n",
    "df.columns = df.columns.str.upper()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Select relevant columns (now in uppercase)\n",
    "# -------------------------------------------------------------\n",
    "acra_data = df[[\n",
    "    \"UEN\",\n",
    "    \"ENTITY_NAME\",\n",
    "    \"BUSINESS_CONSTITUTION_DESCRIPTION\",\n",
    "    \"ENTITY_TYPE_DESCRIPTION\",\n",
    "    \"ENTITY_STATUS_DESCRIPTION\",\n",
    "    \"REGISTRATION_INCORPORATION_DATE\",\n",
    "    \"PRIMARY_SSIC_CODE\",\n",
    "    \"STREET_NAME\",\n",
    "    \"POSTAL_CODE\"\n",
    "]].copy()\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Convert to proper data types\n",
    "# -------------------------------------------------------------\n",
    "acra_data['UEN'] = acra_data['UEN'].astype('string')\n",
    "acra_data['ENTITY_NAME'] = acra_data['ENTITY_NAME'].astype('string')\n",
    "acra_data['BUSINESS_CONSTITUTION_DESCRIPTION'] = acra_data['BUSINESS_CONSTITUTION_DESCRIPTION'].astype('string')\n",
    "acra_data['ENTITY_TYPE_DESCRIPTION'] = acra_data['ENTITY_TYPE_DESCRIPTION'].astype('string')\n",
    "acra_data['ENTITY_STATUS_DESCRIPTION'] = acra_data['ENTITY_STATUS_DESCRIPTION'].astype('string')\n",
    "acra_data['REGISTRATION_INCORPORATION_DATE'] = pd.to_datetime(acra_data['REGISTRATION_INCORPORATION_DATE'], errors='coerce')\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Clean string columns — trim, remove extra spaces, uppercase\n",
    "# -------------------------------------------------------------\n",
    "for col in [\n",
    "    'UEN',\n",
    "    'ENTITY_NAME',\n",
    "    'BUSINESS_CONSTITUTION_DESCRIPTION',\n",
    "    'ENTITY_TYPE_DESCRIPTION',\n",
    "    'ENTITY_STATUS_DESCRIPTION',\n",
    "    'STREET_NAME',\n",
    "    'POSTAL_CODE'\n",
    "]:\n",
    "    acra_data[col] = (\n",
    "        acra_data[col]\n",
    "        .fillna('')\n",
    "        .str.strip()\n",
    "        .str.replace(r'\\s+', ' ', regex=True)\n",
    "        .str.upper()\n",
    "    )\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Replace placeholders with NaN for standardization\n",
    "# -------------------------------------------------------------\n",
    "acra_data.replace(['NA', 'N/A', '-', ''], np.nan, inplace=True)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Convert registration date to dd-mm-yyyy string (optional)\n",
    "# -------------------------------------------------------------\n",
    "acra_data['REGISTRATION_INCORPORATION_DATE'] = acra_data['REGISTRATION_INCORPORATION_DATE'].dt.strftime('%d-%m-%Y')\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Filter only live entities (LIVE COMPANY or LIVE)\n",
    "# -------------------------------------------------------------\n",
    "acra_data = acra_data[\n",
    "    acra_data['ENTITY_STATUS_DESCRIPTION'].isin(['LIVE COMPANY', 'LIVE'])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Exclude specific PRIMARY_SSIC_CODE values (supposedly the data would be 600k plus but when we exclude this would lessen)\n",
    "# -------------------------------------------------------------\n",
    "exclude_codes = [\n",
    "    46900, 47719, 47749, 47539, 47536, 56123,\n",
    "    10711, 10712, 10719, 10732, 10733, 93209\n",
    "]\n",
    "\n",
    "acra_data = acra_data[~acra_data['PRIMARY_SSIC_CODE'].isin(exclude_codes)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37b264bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UEN</th>\n",
       "      <th>ENTITY_NAME</th>\n",
       "      <th>BUSINESS_CONSTITUTION_DESCRIPTION</th>\n",
       "      <th>ENTITY_TYPE_DESCRIPTION</th>\n",
       "      <th>ENTITY_STATUS_DESCRIPTION</th>\n",
       "      <th>REGISTRATION_INCORPORATION_DATE</th>\n",
       "      <th>PRIMARY_SSIC_CODE</th>\n",
       "      <th>STREET_NAME</th>\n",
       "      <th>POSTAL_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00182000A</td>\n",
       "      <td>AIK SENG HENG</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>07-02-1975</td>\n",
       "      <td>46302</td>\n",
       "      <td>FISHERY PORT ROAD</td>\n",
       "      <td>619742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00233500W</td>\n",
       "      <td>ASIA STORE</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>28-10-1974</td>\n",
       "      <td>46411</td>\n",
       "      <td>SIMS AVENUE</td>\n",
       "      <td>387509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00733000J</td>\n",
       "      <td>AIK CHE HIONG</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>02-11-1974</td>\n",
       "      <td>32909</td>\n",
       "      <td>ANG MO KIO INDUSTRIAL PARK 2A</td>\n",
       "      <td>568049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00927000X</td>\n",
       "      <td>A WALIMOHAMED BROS</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>12-11-1974</td>\n",
       "      <td>46411</td>\n",
       "      <td>JELLICOE ROAD</td>\n",
       "      <td>208767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01173000E</td>\n",
       "      <td>ANG TECK MOH DEPARTMENT STORE</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>30-10-1974</td>\n",
       "      <td>47711</td>\n",
       "      <td>WOODLANDS STREET 12</td>\n",
       "      <td>738623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537323</th>\n",
       "      <td>T25LL0518K</td>\n",
       "      <td>ZEUS BARBERS LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>16-05-2025</td>\n",
       "      <td>96021</td>\n",
       "      <td>KELANTAN LANE</td>\n",
       "      <td>200031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537324</th>\n",
       "      <td>T25LL0858C</td>\n",
       "      <td>ZENSE SPACE LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>01-08-2025</td>\n",
       "      <td>43301</td>\n",
       "      <td>YISHUN INDUSTRIAL STREET 1</td>\n",
       "      <td>768161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537325</th>\n",
       "      <td>T25LL0870A</td>\n",
       "      <td>ZIQZEQ PROCUREMENT LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>04-08-2025</td>\n",
       "      <td>70209</td>\n",
       "      <td>SIN MING LANE</td>\n",
       "      <td>573969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537326</th>\n",
       "      <td>T25LL1049B</td>\n",
       "      <td>ZHONG XIN TRAVEL LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>08-09-2025</td>\n",
       "      <td>79102</td>\n",
       "      <td>JALAN BAHAGIA</td>\n",
       "      <td>320034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537327</th>\n",
       "      <td>T25LL1066B</td>\n",
       "      <td>ZDT DRIVES LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>14-09-2025</td>\n",
       "      <td>47533</td>\n",
       "      <td>FERNVALE ROAD</td>\n",
       "      <td>792466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>537328 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               UEN                    ENTITY_NAME  \\\n",
       "0        00182000A                  AIK SENG HENG   \n",
       "1        00233500W                     ASIA STORE   \n",
       "2        00733000J                  AIK CHE HIONG   \n",
       "3        00927000X             A WALIMOHAMED BROS   \n",
       "4        01173000E  ANG TECK MOH DEPARTMENT STORE   \n",
       "...            ...                            ...   \n",
       "537323  T25LL0518K               ZEUS BARBERS LLP   \n",
       "537324  T25LL0858C                ZENSE SPACE LLP   \n",
       "537325  T25LL0870A         ZIQZEQ PROCUREMENT LLP   \n",
       "537326  T25LL1049B           ZHONG XIN TRAVEL LLP   \n",
       "537327  T25LL1066B                 ZDT DRIVES LLP   \n",
       "\n",
       "       BUSINESS_CONSTITUTION_DESCRIPTION           ENTITY_TYPE_DESCRIPTION  \\\n",
       "0                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "1                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "2                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "3                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "4                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "...                                  ...                               ...   \n",
       "537323                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "537324                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "537325                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "537326                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "537327                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "\n",
       "       ENTITY_STATUS_DESCRIPTION REGISTRATION_INCORPORATION_DATE  \\\n",
       "0                           LIVE                      07-02-1975   \n",
       "1                           LIVE                      28-10-1974   \n",
       "2                           LIVE                      02-11-1974   \n",
       "3                           LIVE                      12-11-1974   \n",
       "4                           LIVE                      30-10-1974   \n",
       "...                          ...                             ...   \n",
       "537323                      LIVE                      16-05-2025   \n",
       "537324                      LIVE                      01-08-2025   \n",
       "537325                      LIVE                      04-08-2025   \n",
       "537326                      LIVE                      08-09-2025   \n",
       "537327                      LIVE                      14-09-2025   \n",
       "\n",
       "        PRIMARY_SSIC_CODE                    STREET_NAME POSTAL_CODE  \n",
       "0                   46302              FISHERY PORT ROAD      619742  \n",
       "1                   46411                    SIMS AVENUE      387509  \n",
       "2                   32909  ANG MO KIO INDUSTRIAL PARK 2A      568049  \n",
       "3                   46411                  JELLICOE ROAD      208767  \n",
       "4                   47711            WOODLANDS STREET 12      738623  \n",
       "...                   ...                            ...         ...  \n",
       "537323              96021                  KELANTAN LANE      200031  \n",
       "537324              43301     YISHUN INDUSTRIAL STREET 1      768161  \n",
       "537325              70209                  SIN MING LANE      573969  \n",
       "537326              79102                  JALAN BAHAGIA      320034  \n",
       "537327              47533                  FERNVALE ROAD      792466  \n",
       "\n",
       "[537328 rows x 9 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acra_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec969fc",
   "metadata": {},
   "source": [
    "### Getting SSIC Industry code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32f4bc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PARENT_INDUSTRY</th>\n",
       "      <th>INDUSTRY_TYPE</th>\n",
       "      <th>SUB_INDUSTRY</th>\n",
       "      <th>SSIC_CODES</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Retail</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Fashion &amp; Apparel</td>\n",
       "      <td>47711</td>\n",
       "      <td>Retail Sale Of Clothing For Adults</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Retail</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Fashion &amp; Apparel</td>\n",
       "      <td>47712</td>\n",
       "      <td>Retail Sale Of Children And Infants' Clothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Retail</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Fashion &amp; Apparel</td>\n",
       "      <td>47715</td>\n",
       "      <td>Retail Sale Of Sewing And Clothing Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Retail</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Fashion &amp; Apparel</td>\n",
       "      <td>47719</td>\n",
       "      <td>Retail Sale Of Clothing, Footwear And Leather ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Retail</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Fashion &amp; Apparel</td>\n",
       "      <td>47510</td>\n",
       "      <td>Retail Sale Of Textiles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PARENT_INDUSTRY INDUSTRY_TYPE       SUB_INDUSTRY  SSIC_CODES  \\\n",
       "0          Retail        Retail  Fashion & Apparel       47711   \n",
       "1          Retail        Retail  Fashion & Apparel       47712   \n",
       "2          Retail        Retail  Fashion & Apparel       47715   \n",
       "3          Retail        Retail  Fashion & Apparel       47719   \n",
       "4          Retail        Retail  Fashion & Apparel       47510   \n",
       "\n",
       "                                         DESCRIPTION  \n",
       "0                 Retail Sale Of Clothing For Adults  \n",
       "1      Retail Sale Of Children And Infants' Clothing  \n",
       "2     Retail Sale Of Sewing And Clothing Accessories  \n",
       "3  Retail Sale Of Clothing, Footwear And Leather ...  \n",
       "4                            Retail Sale Of Textiles  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- CONFIG ---\n",
    "file_path = \"./SSIC_Code/mapped_ssic_code.xlsx\"\n",
    "\n",
    "# --- LOAD DATA ---\n",
    "mapped_ssic_code = pd.read_excel(file_path)\n",
    "\n",
    "# --- STANDARDIZE COLUMN NAMES ---\n",
    "# Uppercase, strip spaces, replace spaces with underscores\n",
    "mapped_ssic_code.columns = (\n",
    "    mapped_ssic_code.columns\n",
    "    .str.strip()\n",
    "    .str.upper()\n",
    "    .str.replace(\" \", \"_\")\n",
    ")\n",
    "\n",
    "# --- KEEP ONLY DESIRED COLUMNS ---\n",
    "columns_to_keep = [\"PARENT_INDUSTRY\", \"INDUSTRY_TYPE\", \"SUB_INDUSTRY\", \"SSIC_CODES\", \"DESCRIPTION\"]\n",
    "mapped_ssic_code = mapped_ssic_code[columns_to_keep].copy()\n",
    "\n",
    "# --- CLEAN SSIC_CODES COLUMN ---\n",
    "mapped_ssic_code[\"SSIC_CODES\"] = (\n",
    "    pd.to_numeric(mapped_ssic_code[\"SSIC_CODES\"], errors=\"coerce\")  # safely convert to numeric\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# --- CLEAN TEXT COLUMNS ---\n",
    "text_cols = [\"PARENT_INDUSTRY\", \"INDUSTRY_TYPE\", \"SUB_INDUSTRY\", \"DESCRIPTION\"]\n",
    "mapped_ssic_code[text_cols] = mapped_ssic_code[text_cols].apply(\n",
    "    lambda col: col.astype(str).str.strip().str.title()\n",
    ")\n",
    "\n",
    "# --- REMOVE DUPLICATES & RESET INDEX ---\n",
    "mapped_ssic_code = mapped_ssic_code.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "mapped_ssic_code.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac763a73",
   "metadata": {},
   "source": [
    "### Merge ACRA data with SSIC code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e62740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PRIMARY_SSIC_CODE to int\n",
    "acra_data[\"PRIMARY_SSIC_CODE\"] = (\n",
    "    pd.to_numeric(acra_data[\"PRIMARY_SSIC_CODE\"], errors=\"coerce\")\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "# Merge based on SSIC code\n",
    "acra_data_filtered = acra_data.merge(\n",
    "    mapped_ssic_code,\n",
    "    how=\"left\",\n",
    "    left_on=\"PRIMARY_SSIC_CODE\",\n",
    "    right_on=\"SSIC_CODES\"\n",
    ")\n",
    "\n",
    "# Optional: drop the duplicate 'SSIC CODES' column (keep only PRIMARY_SSIC_CODE)\n",
    "acra_data_filtered = acra_data_filtered.drop(columns=[\"SSIC_CODES\"], errors=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ede8f881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UEN</th>\n",
       "      <th>ENTITY_NAME</th>\n",
       "      <th>BUSINESS_CONSTITUTION_DESCRIPTION</th>\n",
       "      <th>ENTITY_TYPE_DESCRIPTION</th>\n",
       "      <th>ENTITY_STATUS_DESCRIPTION</th>\n",
       "      <th>REGISTRATION_INCORPORATION_DATE</th>\n",
       "      <th>PRIMARY_SSIC_CODE</th>\n",
       "      <th>STREET_NAME</th>\n",
       "      <th>POSTAL_CODE</th>\n",
       "      <th>PARENT_INDUSTRY</th>\n",
       "      <th>INDUSTRY_TYPE</th>\n",
       "      <th>SUB_INDUSTRY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00182000A</td>\n",
       "      <td>AIK SENG HENG</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>07-02-1975</td>\n",
       "      <td>46302</td>\n",
       "      <td>FISHERY PORT ROAD</td>\n",
       "      <td>619742</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Food, Beverages &amp; Tobacco</td>\n",
       "      <td>Wholesale Of Livestock, Meat, Poultry, Eggs An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00233500W</td>\n",
       "      <td>ASIA STORE</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>28-10-1974</td>\n",
       "      <td>46411</td>\n",
       "      <td>SIMS AVENUE</td>\n",
       "      <td>387509</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Household Goods</td>\n",
       "      <td>Wholesale Of Textiles And Leathers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00733000J</td>\n",
       "      <td>AIK CHE HIONG</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>02-11-1974</td>\n",
       "      <td>32909</td>\n",
       "      <td>ANG MO KIO INDUSTRIAL PARK 2A</td>\n",
       "      <td>568049</td>\n",
       "      <td>Others</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>Other Specialised Manufacturing &amp; Distribution</td>\n",
       "      <td>Other Manufacturing Industries N.E.C.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00927000X</td>\n",
       "      <td>A WALIMOHAMED BROS</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>12-11-1974</td>\n",
       "      <td>46411</td>\n",
       "      <td>JELLICOE ROAD</td>\n",
       "      <td>208767</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Household Goods</td>\n",
       "      <td>Wholesale Of Textiles And Leathers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01173000E</td>\n",
       "      <td>ANG TECK MOH DEPARTMENT STORE</td>\n",
       "      <td>PARTNERSHIP</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>30-10-1974</td>\n",
       "      <td>47711</td>\n",
       "      <td>WOODLANDS STREET 12</td>\n",
       "      <td>738623</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Retail</td>\n",
       "      <td>Fashion &amp; Apparel</td>\n",
       "      <td>Retail Sale Of Clothing For Adults</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537323</th>\n",
       "      <td>T25LL0518K</td>\n",
       "      <td>ZEUS BARBERS LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>16-05-2025</td>\n",
       "      <td>96021</td>\n",
       "      <td>KELANTAN LANE</td>\n",
       "      <td>200031</td>\n",
       "      <td>Services</td>\n",
       "      <td>Services</td>\n",
       "      <td>Hair Salons &amp; Barbershops</td>\n",
       "      <td>Hairdressing Salons/Shops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537324</th>\n",
       "      <td>T25LL0858C</td>\n",
       "      <td>ZENSE SPACE LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>01-08-2025</td>\n",
       "      <td>43301</td>\n",
       "      <td>YISHUN INDUSTRIAL STREET 1</td>\n",
       "      <td>768161</td>\n",
       "      <td>Others</td>\n",
       "      <td>Built Environment &amp; Infrastructure</td>\n",
       "      <td>Construction</td>\n",
       "      <td>Renovation Contractors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537325</th>\n",
       "      <td>T25LL0870A</td>\n",
       "      <td>ZIQZEQ PROCUREMENT LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>04-08-2025</td>\n",
       "      <td>70209</td>\n",
       "      <td>SIN MING LANE</td>\n",
       "      <td>573969</td>\n",
       "      <td>Others</td>\n",
       "      <td>Finance, Legal &amp; Real Estate</td>\n",
       "      <td>Legal, Accounting &amp; Consultancy Activities</td>\n",
       "      <td>Management Consultancy Services N.E.C.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537326</th>\n",
       "      <td>T25LL1049B</td>\n",
       "      <td>ZHONG XIN TRAVEL LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>08-09-2025</td>\n",
       "      <td>79102</td>\n",
       "      <td>JALAN BAHAGIA</td>\n",
       "      <td>320034</td>\n",
       "      <td>Others</td>\n",
       "      <td>Tourism, Agency</td>\n",
       "      <td>Travel Agencies &amp; Tour Operators</td>\n",
       "      <td>Travel Agencies And Tour Operators (Mainly Out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537327</th>\n",
       "      <td>T25LL1066B</td>\n",
       "      <td>ZDT DRIVES LLP</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LIMITED LIABILITY PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>14-09-2025</td>\n",
       "      <td>47533</td>\n",
       "      <td>FERNVALE ROAD</td>\n",
       "      <td>792466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>537328 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               UEN                    ENTITY_NAME  \\\n",
       "0        00182000A                  AIK SENG HENG   \n",
       "1        00233500W                     ASIA STORE   \n",
       "2        00733000J                  AIK CHE HIONG   \n",
       "3        00927000X             A WALIMOHAMED BROS   \n",
       "4        01173000E  ANG TECK MOH DEPARTMENT STORE   \n",
       "...            ...                            ...   \n",
       "537323  T25LL0518K               ZEUS BARBERS LLP   \n",
       "537324  T25LL0858C                ZENSE SPACE LLP   \n",
       "537325  T25LL0870A         ZIQZEQ PROCUREMENT LLP   \n",
       "537326  T25LL1049B           ZHONG XIN TRAVEL LLP   \n",
       "537327  T25LL1066B                 ZDT DRIVES LLP   \n",
       "\n",
       "       BUSINESS_CONSTITUTION_DESCRIPTION           ENTITY_TYPE_DESCRIPTION  \\\n",
       "0                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "1                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "2                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "3                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "4                            PARTNERSHIP  SOLE PROPRIETORSHIP/ PARTNERSHIP   \n",
       "...                                  ...                               ...   \n",
       "537323                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "537324                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "537325                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "537326                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "537327                              <NA>     LIMITED LIABILITY PARTNERSHIP   \n",
       "\n",
       "       ENTITY_STATUS_DESCRIPTION REGISTRATION_INCORPORATION_DATE  \\\n",
       "0                           LIVE                      07-02-1975   \n",
       "1                           LIVE                      28-10-1974   \n",
       "2                           LIVE                      02-11-1974   \n",
       "3                           LIVE                      12-11-1974   \n",
       "4                           LIVE                      30-10-1974   \n",
       "...                          ...                             ...   \n",
       "537323                      LIVE                      16-05-2025   \n",
       "537324                      LIVE                      01-08-2025   \n",
       "537325                      LIVE                      04-08-2025   \n",
       "537326                      LIVE                      08-09-2025   \n",
       "537327                      LIVE                      14-09-2025   \n",
       "\n",
       "        PRIMARY_SSIC_CODE                    STREET_NAME POSTAL_CODE  \\\n",
       "0                   46302              FISHERY PORT ROAD      619742   \n",
       "1                   46411                    SIMS AVENUE      387509   \n",
       "2                   32909  ANG MO KIO INDUSTRIAL PARK 2A      568049   \n",
       "3                   46411                  JELLICOE ROAD      208767   \n",
       "4                   47711            WOODLANDS STREET 12      738623   \n",
       "...                   ...                            ...         ...   \n",
       "537323              96021                  KELANTAN LANE      200031   \n",
       "537324              43301     YISHUN INDUSTRIAL STREET 1      768161   \n",
       "537325              70209                  SIN MING LANE      573969   \n",
       "537326              79102                  JALAN BAHAGIA      320034   \n",
       "537327              47533                  FERNVALE ROAD      792466   \n",
       "\n",
       "       PARENT_INDUSTRY                       INDUSTRY_TYPE  \\\n",
       "0               Others                     Wholesale Trade   \n",
       "1               Others                     Wholesale Trade   \n",
       "2               Others                       Manufacturing   \n",
       "3               Others                     Wholesale Trade   \n",
       "4               Retail                              Retail   \n",
       "...                ...                                 ...   \n",
       "537323        Services                            Services   \n",
       "537324          Others  Built Environment & Infrastructure   \n",
       "537325          Others        Finance, Legal & Real Estate   \n",
       "537326          Others                     Tourism, Agency   \n",
       "537327             NaN                                 NaN   \n",
       "\n",
       "                                          SUB_INDUSTRY  \\\n",
       "0                            Food, Beverages & Tobacco   \n",
       "1                                      Household Goods   \n",
       "2       Other Specialised Manufacturing & Distribution   \n",
       "3                                      Household Goods   \n",
       "4                                    Fashion & Apparel   \n",
       "...                                                ...   \n",
       "537323                       Hair Salons & Barbershops   \n",
       "537324                                    Construction   \n",
       "537325      Legal, Accounting & Consultancy Activities   \n",
       "537326                Travel Agencies & Tour Operators   \n",
       "537327                                             NaN   \n",
       "\n",
       "                                              DESCRIPTION  \n",
       "0       Wholesale Of Livestock, Meat, Poultry, Eggs An...  \n",
       "1                      Wholesale Of Textiles And Leathers  \n",
       "2                   Other Manufacturing Industries N.E.C.  \n",
       "3                      Wholesale Of Textiles And Leathers  \n",
       "4                      Retail Sale Of Clothing For Adults  \n",
       "...                                                   ...  \n",
       "537323                          Hairdressing Salons/Shops  \n",
       "537324                             Renovation Contractors  \n",
       "537325             Management Consultancy Services N.E.C.  \n",
       "537326  Travel Agencies And Tour Operators (Mainly Out...  \n",
       "537327                                                NaN  \n",
       "\n",
       "[537328 rows x 13 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acra_data_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9289e8b3",
   "metadata": {},
   "source": [
    "### FIlter Acra data with Master DB to get list of companies havent been researched  by MR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c28478d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(533824, 13)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Ensure both UEN columns are strings for accurate matching\n",
    "acra_data_filtered['UEN'] = acra_data_filtered['UEN'].astype(str).str.strip().str.upper()\n",
    "master_db_df['UEN'] = master_db_df['UEN'].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Filter out rows in acra_data_filtered whose UEN is already in master_db_df\n",
    "acra_data_filtered = acra_data_filtered[~acra_data_filtered['UEN'].isin(master_db_df['UEN'])]\n",
    "\n",
    "acra_data_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bd24ab",
   "metadata": {},
   "source": [
    "### Filter by  Industry (Wholesale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa058f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UEN</th>\n",
       "      <th>ENTITY_NAME</th>\n",
       "      <th>BUSINESS_CONSTITUTION_DESCRIPTION</th>\n",
       "      <th>ENTITY_TYPE_DESCRIPTION</th>\n",
       "      <th>ENTITY_STATUS_DESCRIPTION</th>\n",
       "      <th>REGISTRATION_INCORPORATION_DATE</th>\n",
       "      <th>PRIMARY_SSIC_CODE</th>\n",
       "      <th>STREET_NAME</th>\n",
       "      <th>POSTAL_CODE</th>\n",
       "      <th>PARENT_INDUSTRY</th>\n",
       "      <th>INDUSTRY_TYPE</th>\n",
       "      <th>SUB_INDUSTRY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [UEN, ENTITY_NAME, BUSINESS_CONSTITUTION_DESCRIPTION, ENTITY_TYPE_DESCRIPTION, ENTITY_STATUS_DESCRIPTION, REGISTRATION_INCORPORATION_DATE, PRIMARY_SSIC_CODE, STREET_NAME, POSTAL_CODE, PARENT_INDUSTRY, INDUSTRY_TYPE, SUB_INDUSTRY, DESCRIPTION]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wholesale data\n",
    "ssic_codes = [\n",
    "    \"8550\", \"856\"\n",
    "]\n",
    "\n",
    "\n",
    "acra_data_filtered_wholesale = acra_data_filtered[\n",
    "    (\n",
    "        (acra_data_filtered[\"ENTITY_STATUS_DESCRIPTION\"].str.lower() == \"live\") |\n",
    "        (acra_data_filtered[\"ENTITY_STATUS_DESCRIPTION\"].str.lower() == \"live company\")\n",
    "    )\n",
    "    &\n",
    "    (acra_data_filtered[\"PRIMARY_SSIC_CODE\"].astype(str).isin(ssic_codes))\n",
    "]\n",
    "\n",
    "\n",
    "acra_data_filtered_wholesale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d89c23d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recordowl_results = pd.read_excel(\"Fresh_Leads.xlsx\")\n",
    "# is_unique = recordowl_results['UEN'].is_unique\n",
    "# print(\"Is UEN unique?:\", is_unique)\n",
    "\n",
    "# recordowl_results.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6edf012",
   "metadata": {},
   "source": [
    "### Filter with Fresh Leads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87dc3716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UEN</th>\n",
       "      <th>ENTITY_NAME</th>\n",
       "      <th>BUSINESS_CONSTITUTION_DESCRIPTION</th>\n",
       "      <th>ENTITY_TYPE_DESCRIPTION</th>\n",
       "      <th>ENTITY_STATUS_DESCRIPTION</th>\n",
       "      <th>REGISTRATION_INCORPORATION_DATE</th>\n",
       "      <th>PRIMARY_SSIC_CODE</th>\n",
       "      <th>STREET_NAME</th>\n",
       "      <th>POSTAL_CODE</th>\n",
       "      <th>PARENT_INDUSTRY</th>\n",
       "      <th>INDUSTRY_TYPE</th>\n",
       "      <th>SUB_INDUSTRY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53480073D</td>\n",
       "      <td>HUMBLE BREWS</td>\n",
       "      <td>SOLE-PROPRIETOR</td>\n",
       "      <td>SOLE PROPRIETORSHIP/ PARTNERSHIP</td>\n",
       "      <td>LIVE</td>\n",
       "      <td>26-01-2024</td>\n",
       "      <td>46223</td>\n",
       "      <td>TOH YI DRIVE</td>\n",
       "      <td>590006</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Agricultural Raw Materials &amp; Live Animals</td>\n",
       "      <td>Wholesale Of Coffee, Cocoa And Tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202303828W</td>\n",
       "      <td>WINE &amp; BUBBLES PTE. LTD.</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LOCAL COMPANY</td>\n",
       "      <td>LIVE COMPANY</td>\n",
       "      <td>02-02-2023</td>\n",
       "      <td>46307</td>\n",
       "      <td>STURDEE ROAD</td>\n",
       "      <td>207855</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Food, Beverages &amp; Tobacco</td>\n",
       "      <td>Wholesale Of Liquor, Soft Drinks And Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202542730M</td>\n",
       "      <td>NUVIAA PTE. LTD.</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LOCAL COMPANY</td>\n",
       "      <td>LIVE COMPANY</td>\n",
       "      <td>24-09-2025</td>\n",
       "      <td>46413</td>\n",
       "      <td>YISHUN INDUSTRIAL STREET 1</td>\n",
       "      <td>768162</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Household Goods</td>\n",
       "      <td>Wholesale Of Children And Infants' Clothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201828332D</td>\n",
       "      <td>DE MAJESTIC VINES PTE. LTD.</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LOCAL COMPANY</td>\n",
       "      <td>LIVE COMPANY</td>\n",
       "      <td>17-08-2018</td>\n",
       "      <td>46307</td>\n",
       "      <td>ANSON ROAD</td>\n",
       "      <td>79903</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Food, Beverages &amp; Tobacco</td>\n",
       "      <td>Wholesale Of Liquor, Soft Drinks And Beverages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201813214E</td>\n",
       "      <td>CARDE DESIGN PTE. LTD.</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>LOCAL COMPANY</td>\n",
       "      <td>LIVE COMPANY</td>\n",
       "      <td>18-04-2018</td>\n",
       "      <td>46431</td>\n",
       "      <td>UPPER CROSS STREET</td>\n",
       "      <td>58357</td>\n",
       "      <td>Others</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>Household Goods</td>\n",
       "      <td>Wholesale Of Furniture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          UEN                  ENTITY_NAME BUSINESS_CONSTITUTION_DESCRIPTION  \\\n",
       "0   53480073D                 HUMBLE BREWS                   SOLE-PROPRIETOR   \n",
       "1  202303828W     WINE & BUBBLES PTE. LTD.                              <NA>   \n",
       "2  202542730M             NUVIAA PTE. LTD.                              <NA>   \n",
       "3  201828332D  DE MAJESTIC VINES PTE. LTD.                              <NA>   \n",
       "4  201813214E       CARDE DESIGN PTE. LTD.                              <NA>   \n",
       "\n",
       "            ENTITY_TYPE_DESCRIPTION ENTITY_STATUS_DESCRIPTION  \\\n",
       "0  SOLE PROPRIETORSHIP/ PARTNERSHIP                      LIVE   \n",
       "1                     LOCAL COMPANY              LIVE COMPANY   \n",
       "2                     LOCAL COMPANY              LIVE COMPANY   \n",
       "3                     LOCAL COMPANY              LIVE COMPANY   \n",
       "4                     LOCAL COMPANY              LIVE COMPANY   \n",
       "\n",
       "  REGISTRATION_INCORPORATION_DATE  PRIMARY_SSIC_CODE  \\\n",
       "0                      26-01-2024              46223   \n",
       "1                      02-02-2023              46307   \n",
       "2                      24-09-2025              46413   \n",
       "3                      17-08-2018              46307   \n",
       "4                      18-04-2018              46431   \n",
       "\n",
       "                  STREET_NAME POSTAL_CODE PARENT_INDUSTRY    INDUSTRY_TYPE  \\\n",
       "0                TOH YI DRIVE      590006          Others  Wholesale Trade   \n",
       "1                STURDEE ROAD      207855          Others  Wholesale Trade   \n",
       "2  YISHUN INDUSTRIAL STREET 1      768162          Others  Wholesale Trade   \n",
       "3                  ANSON ROAD       79903          Others  Wholesale Trade   \n",
       "4          UPPER CROSS STREET       58357          Others  Wholesale Trade   \n",
       "\n",
       "                                SUB_INDUSTRY  \\\n",
       "0  Agricultural Raw Materials & Live Animals   \n",
       "1                  Food, Beverages & Tobacco   \n",
       "2                            Household Goods   \n",
       "3                  Food, Beverages & Tobacco   \n",
       "4                            Household Goods   \n",
       "\n",
       "                                      DESCRIPTION  \n",
       "0              Wholesale Of Coffee, Cocoa And Tea  \n",
       "1  Wholesale Of Liquor, Soft Drinks And Beverages  \n",
       "2     Wholesale Of Children And Infants' Clothing  \n",
       "3  Wholesale Of Liquor, Soft Drinks And Beverages  \n",
       "4                          Wholesale Of Furniture  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Copy to avoid SettingWithCopyWarning ---\n",
    "acra_data_filtered_wholesale = acra_data_filtered_wholesale.copy()\n",
    "\n",
    "# --- UPDATE HERE: Remove rows if UEN exists in recordowl_results.xlsx ---\n",
    "recordowl_results = pd.read_excel(\"Fresh_Leads.xlsx\")\n",
    "# Ensure both dataframes have a 'UEN' column\n",
    "if \"UEN\" in recordowl_results.columns and \"UEN\" in acra_data_filtered_wholesale.columns:\n",
    "    filtered = acra_data_filtered_wholesale[~acra_data_filtered_wholesale[\"UEN\"].isin(recordowl_results[\"UEN\"])]\n",
    "else:\n",
    "    raise ValueError(\"Column 'UEN' not found in one of the dataframes.\")\n",
    "\n",
    "# sample data \n",
    "acra_data_filtered_wholesale = filtered.sample(n=50, random_state=42).reset_index(drop=True)\n",
    "\n",
    "acra_data_filtered_wholesale.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b530343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# acra_data_filtered_wholesale = pd.DataFrame({\n",
    "#     \"UEN\": [\"201625008K\"]\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3040bb97",
   "metadata": {},
   "source": [
    "### Get Data from RecordOwl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b3cd46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Processing 53480073D (1/50)\n",
      "  📡 Starting Apify run for 53480073D (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:1Gwe6bUPGAoyTjiWP]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:1Gwe6bUPGAoyTjiWP]\u001b[0m -> 2025-11-05T07:28:11.301Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:1Gwe6bUPGAoyTjiWP]\u001b[0m -> 2025-11-05T07:28:11.303Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:1Gwe6bUPGAoyTjiWP]\u001b[0m -> 2025-11-05T07:28:11.346Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:1Gwe6bUPGAoyTjiWP]\u001b[0m -> 2025-11-05T07:28:11.551Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:1Gwe6bUPGAoyTjiWP]\u001b[0m -> 2025-11-05T07:28:13.087Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:1Gwe6bUPGAoyTjiWP]\u001b[0m -> 2025-11-05T07:28:13.219Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:1Gwe6bUPGAoyTjiWP]\u001b[0m -> 2025-11-05T07:28:14.585Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:1Gwe6bUPGAoyTjiWP]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:1Gwe6bUPGAoyTjiWP]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:1Gwe6bUPGAoyTjiWP]\u001b[0m -> 2025-11-05T07:28:14.716Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:1Gwe6bUPGAoyTjiWP]\u001b[0m -> 2025-11-05T07:28:30.480Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 53480073D\n",
      "\u001b[36m[apify.puppeteer-scraper runId:1Gwe6bUPGAoyTjiWP]\u001b[0m -> 2025-11-05T07:28:44.011Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:1Gwe6bUPGAoyTjiWP]\u001b[0m -> 2025-11-05T07:28:44.397Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":28860,\"requestsFinishedPerMinute\":2,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":28860,\"requestsTotal\":1,\"crawlerRuntimeMillis\":29885}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:1Gwe6bUPGAoyTjiWP]\u001b[0m -> 2025-11-05T07:28:44.398Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:1Gwe6bUPGAoyTjiWP]\u001b[0m -> Status: RUNNING, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:1Gwe6bUPGAoyTjiWP]\u001b[0m -> 2025-11-05T07:28:44.415Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 53480073D (114818 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 18 dt tags\n",
      "  🔎 No phones found yet, searching entire content...\n",
      "  ⚠️ WARNING: No phone numbers found for 53480073D\n",
      "  📄 Showing first 500 chars of parent HTML for debugging:\n",
      "<div class=\"max-w-7xl mx-auto lg:py-6 sm:px-6 lg:px-8\" style=\"height: auto !important;\">\n",
      " <div class=\"flex flex-col lg:flex-row\" style=\"height: auto !important;\">\n",
      "  <div class=\"w-full lg:w-2/3 lg:pr-8\" style=\"height: auto !important;\">\n",
      "   <div class=\"lg:mb-4 border-b border-gray-200\">\n",
      "    <ul class=\"flex flex-nowrap overflow-x-auto -mb-px text-sm font-medium text-center scrollbar-hide\" id=\"companyTabs\" role=\"tablist\">\n",
      "     <li class=\"mr-2\" role=\"presentation\">\n",
      "      <button aria-controls=\"overvi...\n",
      "  ✅ Processed 53480073D: 0 emails, 0 phones\n",
      "  💤 Sleeping for 26s before next request...\n",
      "\n",
      "🔎 Processing 202303828W (2/50)\n",
      "  📡 Starting Apify run for 202303828W (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:oPKP3nUCSTAZdspWW]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oPKP3nUCSTAZdspWW]\u001b[0m -> 2025-11-05T07:29:26.816Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oPKP3nUCSTAZdspWW]\u001b[0m -> 2025-11-05T07:29:26.821Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oPKP3nUCSTAZdspWW]\u001b[0m -> 2025-11-05T07:29:26.902Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oPKP3nUCSTAZdspWW]\u001b[0m -> 2025-11-05T07:29:27.103Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oPKP3nUCSTAZdspWW]\u001b[0m -> 2025-11-05T07:29:27.902Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oPKP3nUCSTAZdspWW]\u001b[0m -> 2025-11-05T07:29:28.014Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oPKP3nUCSTAZdspWW]\u001b[0m -> 2025-11-05T07:29:28.712Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oPKP3nUCSTAZdspWW]\u001b[0m -> 2025-11-05T07:29:28.866Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oPKP3nUCSTAZdspWW]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oPKP3nUCSTAZdspWW]\u001b[0m -> 2025-11-05T07:29:38.304Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 202303828W\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oPKP3nUCSTAZdspWW]\u001b[0m -> 2025-11-05T07:29:51.886Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oPKP3nUCSTAZdspWW]\u001b[0m -> 2025-11-05T07:29:52.405Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":22576,\"requestsFinishedPerMinute\":3,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":22576,\"requestsTotal\":1,\"crawlerRuntimeMillis\":23775}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oPKP3nUCSTAZdspWW]\u001b[0m -> 2025-11-05T07:29:52.406Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oPKP3nUCSTAZdspWW]\u001b[0m -> 2025-11-05T07:29:52.425Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oPKP3nUCSTAZdspWW]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 202303828W (472374 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 19 dt tags\n",
      "  📝 Field 'contact number': +65 88162548\n",
      "  🔢 Extracted digits: 6588162548\n",
      "  ✅ Added from dt/dd (10 digits): +6588162548\n",
      "  ✅ Total phones found: ['+6588162548']\n",
      "  ✅ Processed 202303828W: 1 emails, 1 phones\n",
      "  💤 Sleeping for 27s before next request...\n",
      "\n",
      "🔎 Processing 202542730M (3/50)\n",
      "  📡 Starting Apify run for 202542730M (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:FApjBC6dIeF8VOgc4]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:FApjBC6dIeF8VOgc4]\u001b[0m -> 2025-11-05T07:30:36.574Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:FApjBC6dIeF8VOgc4]\u001b[0m -> 2025-11-05T07:30:37.328Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:FApjBC6dIeF8VOgc4]\u001b[0m -> 2025-11-05T07:30:37.433Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:FApjBC6dIeF8VOgc4]\u001b[0m -> 2025-11-05T07:30:37.617Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:FApjBC6dIeF8VOgc4]\u001b[0m -> 2025-11-05T07:30:38.983Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:FApjBC6dIeF8VOgc4]\u001b[0m -> 2025-11-05T07:30:39.125Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:FApjBC6dIeF8VOgc4]\u001b[0m -> 2025-11-05T07:30:40.054Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:FApjBC6dIeF8VOgc4]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:FApjBC6dIeF8VOgc4]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:FApjBC6dIeF8VOgc4]\u001b[0m -> 2025-11-05T07:30:40.167Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:FApjBC6dIeF8VOgc4]\u001b[0m -> 2025-11-05T07:30:56.250Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 202542730M\n",
      "\u001b[36m[apify.puppeteer-scraper runId:FApjBC6dIeF8VOgc4]\u001b[0m -> 2025-11-05T07:31:40.168Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":60192,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:FApjBC6dIeF8VOgc4]\u001b[0m -> 2025-11-05T07:31:40.200Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:FApjBC6dIeF8VOgc4]\u001b[0m -> 2025-11-05T07:31:57.578Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m No company links found, might be not found\n",
      "\u001b[36m[apify.puppeteer-scraper runId:FApjBC6dIeF8VOgc4]\u001b[0m -> 2025-11-05T07:31:58.405Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:FApjBC6dIeF8VOgc4]\u001b[0m -> 2025-11-05T07:31:59.211Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":77202,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":77202,\"requestsTotal\":1,\"crawlerRuntimeMillis\":79236}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:FApjBC6dIeF8VOgc4]\u001b[0m -> 2025-11-05T07:31:59.213Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:FApjBC6dIeF8VOgc4]\u001b[0m -> Status: RUNNING, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:FApjBC6dIeF8VOgc4]\u001b[0m -> 2025-11-05T07:31:59.266Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ⚠️ Company not found for UEN 202542730M\n",
      "  ❌ Company not found on RecordOwl\n",
      "\n",
      "🔎 Processing 201828332D (4/50)\n",
      "  📡 Starting Apify run for 201828332D (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:Y4ec0wCph8iewJUBC]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:Y4ec0wCph8iewJUBC]\u001b[0m -> 2025-11-05T07:32:19.246Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Y4ec0wCph8iewJUBC]\u001b[0m -> 2025-11-05T07:32:19.247Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Y4ec0wCph8iewJUBC]\u001b[0m -> 2025-11-05T07:32:19.395Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Y4ec0wCph8iewJUBC]\u001b[0m -> 2025-11-05T07:32:19.574Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Y4ec0wCph8iewJUBC]\u001b[0m -> 2025-11-05T07:32:20.843Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Y4ec0wCph8iewJUBC]\u001b[0m -> 2025-11-05T07:32:20.945Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Y4ec0wCph8iewJUBC]\u001b[0m -> 2025-11-05T07:32:21.668Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Y4ec0wCph8iewJUBC]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Y4ec0wCph8iewJUBC]\u001b[0m -> 2025-11-05T07:32:21.755Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Y4ec0wCph8iewJUBC]\u001b[0m -> 2025-11-05T07:32:28.433Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Y4ec0wCph8iewJUBC]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Y4ec0wCph8iewJUBC]\u001b[0m -> 2025-11-05T07:32:28.435Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Y4ec0wCph8iewJUBC]\u001b[0m -> 2025-11-05T07:32:33.999Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Y4ec0wCph8iewJUBC]\u001b[0m -> 2025-11-05T07:32:34.002Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":2}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Y4ec0wCph8iewJUBC]\u001b[0m -> 2025-11-05T07:32:48.682Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 201828332D\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Y4ec0wCph8iewJUBC]\u001b[0m -> 2025-11-05T07:33:07.150Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Y4ec0wCph8iewJUBC]\u001b[0m -> 2025-11-05T07:33:07.559Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[null,null,1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":32429,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":32429,\"requestsTotal\":1,\"crawlerRuntimeMillis\":45966}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Y4ec0wCph8iewJUBC]\u001b[0m -> 2025-11-05T07:33:07.561Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Y4ec0wCph8iewJUBC]\u001b[0m -> 2025-11-05T07:33:07.594Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Y4ec0wCph8iewJUBC]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 201828332D (1162685 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 21 dt tags\n",
      "  📝 Field 'contact number': 6980 7200\n",
      "  🔢 Extracted digits: 69807200\n",
      "  ✅ Added from dt/dd (8 digits): +6569807200\n",
      "  ✅ Total phones found: ['+6569807200']\n",
      "  ✅ Processed 201828332D: 1 emails, 1 phones\n",
      "  💤 Sleeping for 29s before next request...\n",
      "\n",
      "🔎 Processing 201813214E (5/50)\n",
      "  📡 Starting Apify run for 201813214E (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:8m5cTfaWwthNPF7p2]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:8m5cTfaWwthNPF7p2]\u001b[0m -> 2025-11-05T07:33:54.311Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8m5cTfaWwthNPF7p2]\u001b[0m -> 2025-11-05T07:33:54.313Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8m5cTfaWwthNPF7p2]\u001b[0m -> 2025-11-05T07:33:54.465Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8m5cTfaWwthNPF7p2]\u001b[0m -> 2025-11-05T07:33:54.668Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8m5cTfaWwthNPF7p2]\u001b[0m -> 2025-11-05T07:33:55.409Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8m5cTfaWwthNPF7p2]\u001b[0m -> 2025-11-05T07:33:55.652Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8m5cTfaWwthNPF7p2]\u001b[0m -> 2025-11-05T07:33:56.686Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8m5cTfaWwthNPF7p2]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8m5cTfaWwthNPF7p2]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8m5cTfaWwthNPF7p2]\u001b[0m -> 2025-11-05T07:33:56.776Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8m5cTfaWwthNPF7p2]\u001b[0m -> 2025-11-05T07:34:15.141Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 201813214E\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8m5cTfaWwthNPF7p2]\u001b[0m -> 2025-11-05T07:34:56.776Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":60175,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8m5cTfaWwthNPF7p2]\u001b[0m -> 2025-11-05T07:34:56.803Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8m5cTfaWwthNPF7p2]\u001b[0m -> Status: RUNNING, Message: Crawled 1/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8m5cTfaWwthNPF7p2]\u001b[0m -> 2025-11-05T07:35:26.634Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8m5cTfaWwthNPF7p2]\u001b[0m -> 2025-11-05T07:35:27.224Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":88847,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":88847,\"requestsTotal\":1,\"crawlerRuntimeMillis\":90624}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8m5cTfaWwthNPF7p2]\u001b[0m -> 2025-11-05T07:35:27.226Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8m5cTfaWwthNPF7p2]\u001b[0m -> 2025-11-05T07:35:27.246Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:8m5cTfaWwthNPF7p2]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 201813214E (463465 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 18 dt tags\n",
      "  🔎 No phones found yet, searching entire content...\n",
      "  ⚠️ WARNING: No phone numbers found for 201813214E\n",
      "  📄 Showing first 500 chars of parent HTML for debugging:\n",
      "<div class=\"max-w-7xl mx-auto lg:py-6 sm:px-6 lg:px-8\" style=\"height: auto !important;\">\n",
      " <div class=\"flex flex-col lg:flex-row\" style=\"height: auto !important;\">\n",
      "  <div class=\"w-full lg:w-2/3 lg:pr-8\" style=\"height: auto !important;\">\n",
      "   <div class=\"lg:mb-4 border-b border-gray-200\">\n",
      "    <ul class=\"flex flex-nowrap overflow-x-auto -mb-px text-sm font-medium text-center scrollbar-hide\" id=\"companyTabs\" role=\"tablist\">\n",
      "     <li class=\"mr-2\" role=\"presentation\">\n",
      "      <button aria-controls=\"overvi...\n",
      "  ✅ Processed 201813214E: 0 emails, 0 phones\n",
      "  💤 Sleeping for 30s before next request...\n",
      "  🛑 Checkpoint pause: waiting extra 30s...\n",
      "\n",
      "🔎 Processing 199400661M (6/50)\n",
      "  📡 Starting Apify run for 199400661M (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> 2025-11-05T07:36:51.194Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> 2025-11-05T07:36:51.195Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> 2025-11-05T07:36:51.248Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> 2025-11-05T07:36:51.442Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> 2025-11-05T07:36:52.111Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> 2025-11-05T07:36:52.279Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> 2025-11-05T07:36:52.940Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> 2025-11-05T07:36:53.063Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> 2025-11-05T07:37:03.587Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> 2025-11-05T07:37:03.592Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> 2025-11-05T07:37:50.530Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> 2025-11-05T07:37:50.531Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":2}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> 2025-11-05T07:37:53.063Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":60208,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> 2025-11-05T07:37:53.097Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> 2025-11-05T07:38:19.899Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 199400661M\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> 2025-11-05T07:38:53.062Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":120207,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> 2025-11-05T07:38:53.164Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> 2025-11-05T07:39:26.054Z \u001b[31mERROR\u001b[39m\u001b[33m PuppeteerCrawler:\u001b[39m Error in pageFunction: Navigation timeout of 60000 ms exceeded\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> 2025-11-05T07:39:26.599Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> 2025-11-05T07:39:27.041Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[null,null,1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":95521,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":95521,\"requestsTotal\":1,\"crawlerRuntimeMillis\":154186}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> Status: RUNNING, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> 2025-11-05T07:39:27.042Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6i0CvxwzkyrJ8Xnkj]\u001b[0m -> 2025-11-05T07:39:27.097Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ❌ Error for 199400661M: Navigation timeout of 60000 ms exceeded\n",
      "  ❌ Scraping error: Navigation timeout of 60000 ms exceeded\n",
      "\n",
      "🔎 Processing 201838013D (7/50)\n",
      "  📡 Starting Apify run for 201838013D (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:39:47.606Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:39:47.608Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:39:47.652Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:39:47.852Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:39:48.565Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:39:48.659Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:39:49.223Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:39:49.313Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:39:56.909Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:39:56.910Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:40:49.313Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":60165,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:40:49.335Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:40:58.542Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. net::ERR_CONNECTION_CLOSED at https://recordowl.com/\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:40:58.544Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":2}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:41:49.313Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":120165,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:41:49.336Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:42:14.042Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 201838013D\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:42:49.313Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":180165,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:42:49.338Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:43:49.314Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":240165,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:43:49.340Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:44:00.814Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m No company links found, might be not found\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:44:00.966Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:44:01.277Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[null,null,1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":182080,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":182080,\"requestsTotal\":1,\"crawlerRuntimeMillis\":252129}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:44:01.279Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> 2025-11-05T07:44:01.290Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:jHhVnxG4X4g6RZ6LC]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ⚠️ Company not found for UEN 201838013D\n",
      "  ❌ Company not found on RecordOwl\n",
      "\n",
      "🔎 Processing 53162832M (8/50)\n",
      "  📡 Starting Apify run for 53162832M (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:fs2zu2X8e8ZaoCFqZ]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:fs2zu2X8e8ZaoCFqZ]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:fs2zu2X8e8ZaoCFqZ]\u001b[0m -> 2025-11-05T07:44:22.973Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:fs2zu2X8e8ZaoCFqZ]\u001b[0m -> 2025-11-05T07:44:22.975Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:fs2zu2X8e8ZaoCFqZ]\u001b[0m -> 2025-11-05T07:44:23.150Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:fs2zu2X8e8ZaoCFqZ]\u001b[0m -> 2025-11-05T07:44:23.347Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:fs2zu2X8e8ZaoCFqZ]\u001b[0m -> 2025-11-05T07:44:24.008Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:fs2zu2X8e8ZaoCFqZ]\u001b[0m -> 2025-11-05T07:44:24.104Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:fs2zu2X8e8ZaoCFqZ]\u001b[0m -> 2025-11-05T07:44:24.781Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:fs2zu2X8e8ZaoCFqZ]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:fs2zu2X8e8ZaoCFqZ]\u001b[0m -> 2025-11-05T07:44:24.874Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:fs2zu2X8e8ZaoCFqZ]\u001b[0m -> 2025-11-05T07:45:24.875Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":60183,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:fs2zu2X8e8ZaoCFqZ]\u001b[0m -> 2025-11-05T07:45:24.959Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0.036},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:fs2zu2X8e8ZaoCFqZ]\u001b[0m -> 2025-11-05T07:45:26.314Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 53162832M\n",
      "\u001b[36m[apify.puppeteer-scraper runId:fs2zu2X8e8ZaoCFqZ]\u001b[0m -> 2025-11-05T07:45:44.847Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:fs2zu2X8e8ZaoCFqZ]\u001b[0m -> 2025-11-05T07:45:45.126Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":79569,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":79569,\"requestsTotal\":1,\"crawlerRuntimeMillis\":80434}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:fs2zu2X8e8ZaoCFqZ]\u001b[0m -> 2025-11-05T07:45:45.128Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:fs2zu2X8e8ZaoCFqZ]\u001b[0m -> Status: RUNNING, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:fs2zu2X8e8ZaoCFqZ]\u001b[0m -> 2025-11-05T07:45:45.146Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 53162832M (1457378 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 16 dt tags\n",
      "  🔎 No phones found yet, searching entire content...\n",
      "  ⚠️ WARNING: No phone numbers found for 53162832M\n",
      "  📄 Showing first 500 chars of parent HTML for debugging:\n",
      "<div class=\"max-w-7xl mx-auto lg:py-6 sm:px-6 lg:px-8\" style=\"height: auto !important;\">\n",
      " <div class=\"flex flex-col lg:flex-row\" style=\"height: auto !important;\">\n",
      "  <div class=\"w-full lg:w-2/3 lg:pr-8\" style=\"height: auto !important;\">\n",
      "   <div class=\"lg:mb-4 border-b border-gray-200\">\n",
      "    <ul class=\"flex flex-nowrap overflow-x-auto -mb-px text-sm font-medium text-center scrollbar-hide\" id=\"companyTabs\" role=\"tablist\">\n",
      "     <li class=\"mr-2\" role=\"presentation\">\n",
      "      <button aria-controls=\"overvi...\n",
      "  ✅ Processed 53162832M: 0 emails, 0 phones\n",
      "  💤 Sleeping for 33s before next request...\n",
      "\n",
      "🔎 Processing 202540737R (9/50)\n",
      "  📡 Starting Apify run for 202540737R (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:qZgnDg7Onewrss7L3]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:qZgnDg7Onewrss7L3]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qZgnDg7Onewrss7L3]\u001b[0m -> 2025-11-05T07:46:46.938Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qZgnDg7Onewrss7L3]\u001b[0m -> 2025-11-05T07:46:46.940Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qZgnDg7Onewrss7L3]\u001b[0m -> 2025-11-05T07:46:47.000Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qZgnDg7Onewrss7L3]\u001b[0m -> 2025-11-05T07:46:47.161Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qZgnDg7Onewrss7L3]\u001b[0m -> 2025-11-05T07:46:48.466Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qZgnDg7Onewrss7L3]\u001b[0m -> 2025-11-05T07:46:48.705Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qZgnDg7Onewrss7L3]\u001b[0m -> 2025-11-05T07:46:49.536Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qZgnDg7Onewrss7L3]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qZgnDg7Onewrss7L3]\u001b[0m -> 2025-11-05T07:46:49.641Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qZgnDg7Onewrss7L3]\u001b[0m -> 2025-11-05T07:47:00.057Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 202540737R\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qZgnDg7Onewrss7L3]\u001b[0m -> 2025-11-05T07:47:49.640Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":60173,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qZgnDg7Onewrss7L3]\u001b[0m -> 2025-11-05T07:47:49.674Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qZgnDg7Onewrss7L3]\u001b[0m -> 2025-11-05T07:47:53.383Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m No company links found, might be not found\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qZgnDg7Onewrss7L3]\u001b[0m -> 2025-11-05T07:47:53.764Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qZgnDg7Onewrss7L3]\u001b[0m -> 2025-11-05T07:47:54.141Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":63524,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":63524,\"requestsTotal\":1,\"crawlerRuntimeMillis\":64674}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qZgnDg7Onewrss7L3]\u001b[0m -> Status: RUNNING, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qZgnDg7Onewrss7L3]\u001b[0m -> 2025-11-05T07:47:54.142Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qZgnDg7Onewrss7L3]\u001b[0m -> 2025-11-05T07:47:54.159Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ⚠️ Company not found for UEN 202540737R\n",
      "  ❌ Company not found on RecordOwl\n",
      "\n",
      "🔎 Processing 200301636R (10/50)\n",
      "  📡 Starting Apify run for 200301636R (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:4hhu1hfOYMdKKGoBJ]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4hhu1hfOYMdKKGoBJ]\u001b[0m -> 2025-11-05T07:48:14.213Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4hhu1hfOYMdKKGoBJ]\u001b[0m -> 2025-11-05T07:48:14.214Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4hhu1hfOYMdKKGoBJ]\u001b[0m -> 2025-11-05T07:48:14.262Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4hhu1hfOYMdKKGoBJ]\u001b[0m -> 2025-11-05T07:48:14.452Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4hhu1hfOYMdKKGoBJ]\u001b[0m -> 2025-11-05T07:48:15.226Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4hhu1hfOYMdKKGoBJ]\u001b[0m -> 2025-11-05T07:48:15.338Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4hhu1hfOYMdKKGoBJ]\u001b[0m -> 2025-11-05T07:48:15.887Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4hhu1hfOYMdKKGoBJ]\u001b[0m -> 2025-11-05T07:48:15.963Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4hhu1hfOYMdKKGoBJ]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4hhu1hfOYMdKKGoBJ]\u001b[0m -> 2025-11-05T07:48:25.902Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 200301636R\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4hhu1hfOYMdKKGoBJ]\u001b[0m -> 2025-11-05T07:48:47.502Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4hhu1hfOYMdKKGoBJ]\u001b[0m -> 2025-11-05T07:48:47.998Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":30795,\"requestsFinishedPerMinute\":2,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":30795,\"requestsTotal\":1,\"crawlerRuntimeMillis\":32181}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4hhu1hfOYMdKKGoBJ]\u001b[0m -> 2025-11-05T07:48:47.999Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4hhu1hfOYMdKKGoBJ]\u001b[0m -> 2025-11-05T07:48:48.014Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4hhu1hfOYMdKKGoBJ]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 200301636R (484734 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 18 dt tags\n",
      "  📝 Field 'contact number': 6226 1178\n",
      "  🔢 Extracted digits: 62261178\n",
      "  ✅ Added from dt/dd (8 digits): +6562261178\n",
      "  ✅ Total phones found: ['+6562261178']\n",
      "  ✅ Processed 200301636R: 0 emails, 1 phones\n",
      "  💤 Sleeping for 25s before next request...\n",
      "  🛑 Checkpoint pause: waiting extra 30s...\n",
      "\n",
      "🔎 Processing 201700187C (11/50)\n",
      "  📡 Starting Apify run for 201700187C (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:oCF6riFEceY62vSfI]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:oCF6riFEceY62vSfI]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oCF6riFEceY62vSfI]\u001b[0m -> 2025-11-05T07:50:01.615Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oCF6riFEceY62vSfI]\u001b[0m -> 2025-11-05T07:50:01.617Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oCF6riFEceY62vSfI]\u001b[0m -> 2025-11-05T07:50:01.810Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oCF6riFEceY62vSfI]\u001b[0m -> 2025-11-05T07:50:02.011Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oCF6riFEceY62vSfI]\u001b[0m -> 2025-11-05T07:50:02.639Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oCF6riFEceY62vSfI]\u001b[0m -> 2025-11-05T07:50:02.767Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oCF6riFEceY62vSfI]\u001b[0m -> 2025-11-05T07:50:03.485Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oCF6riFEceY62vSfI]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oCF6riFEceY62vSfI]\u001b[0m -> 2025-11-05T07:50:03.574Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oCF6riFEceY62vSfI]\u001b[0m -> 2025-11-05T07:50:34.037Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. net::ERR_TIMED_OUT at https://recordowl.com/\n",
      "Exception in thread Thread-23 (_log_changed_status_message):\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"c:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\threading.py\"\u001b[0m, line \u001b[35m1043\u001b[0m, in \u001b[35m_bootstrap_inner\u001b[0m\n",
      "    \u001b[31mself.run\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"c:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\ipykernel\\ipkernel.py\"\u001b[0m, line \u001b[35m772\u001b[0m, in \u001b[35mrun_closure\u001b[0m\n",
      "    \u001b[31m_threading_Thread_run\u001b[0m\u001b[1;31m(self)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"c:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\threading.py\"\u001b[0m, line \u001b[35m994\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31mself._target\u001b[0m\u001b[1;31m(*self._args, **self._kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"c:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\apify_client\\clients\\resource_clients\\log.py\"\u001b[0m, line \u001b[35m534\u001b[0m, in \u001b[35m_log_changed_status_message\u001b[0m\n",
      "    if not self._log_run_data(\u001b[31mself._run_client.get\u001b[0m\u001b[1;31m()\u001b[0m):\n",
      "                              \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"c:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\apify_client\\_logging.py\"\u001b[0m, line \u001b[35m86\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    return fun(resource_client, *args, **kwargs)\n",
      "  File \u001b[35m\"c:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\apify_client\\clients\\resource_clients\\run.py\"\u001b[0m, line \u001b[35m55\u001b[0m, in \u001b[35mget\u001b[0m\n",
      "    return \u001b[31mself._get\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"c:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\apify_client\\clients\\base\\resource_client.py\"\u001b[0m, line \u001b[35m15\u001b[0m, in \u001b[35m_get\u001b[0m\n",
      "    response = self.http_client.call(\n",
      "        url=self.url,\n",
      "    ...<2 lines>...\n",
      "        timeout_secs=timeout_secs,\n",
      "    )\n",
      "  File \u001b[35m\"c:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\apify_client\\_http_client.py\"\u001b[0m, line \u001b[35m210\u001b[0m, in \u001b[35mcall\u001b[0m\n",
      "    return retry_with_exp_backoff(\n",
      "        _make_request,\n",
      "    ...<3 lines>...\n",
      "        random_factor=DEFAULT_BACKOFF_RANDOM_FACTOR,\n",
      "    )\n",
      "  File \u001b[35m\"c:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\apify_client\\_utils.py\"\u001b[0m, line \u001b[35m168\u001b[0m, in \u001b[35mretry_with_exp_backoff\u001b[0m\n",
      "    return func(stop_retrying, attempt)\n",
      "  File \u001b[35m\"c:\\Users\\azlie\\miniconda3\\envs\\firstenv\\Lib\\site-packages\\apify_client\\_http_client.py\"\u001b[0m, line \u001b[35m174\u001b[0m, in \u001b[35m_make_request\u001b[0m\n",
      "    response = impit_client.request(\n",
      "        method=method,\n",
      "    ...<4 lines>...\n",
      "        stream=stream or False,\n",
      "    )\n",
      "\u001b[1;35mimpit.HTTPError\u001b[0m: \u001b[35mThe internal HTTP library has thrown an error:\n",
      "reqwest::Error {\n",
      "    kind: Request,\n",
      "    url: \"https://api.apify.com/v2/actor-runs/oCF6riFEceY62vSfI\",\n",
      "    source: hyper_util::client::legacy::Error(\n",
      "        SendRequest,\n",
      "        hyper::Error(\n",
      "            Io,\n",
      "            Os {\n",
      "                code: 10054,\n",
      "                kind: ConnectionReset,\n",
      "                message: \"An existing connection was forcibly closed by the remote host.\",\n",
      "            },\n",
      "        ),\n",
      "    ),\n",
      "}\u001b[0m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oCF6riFEceY62vSfI]\u001b[0m -> 2025-11-05T07:50:34.039Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ❌ Apify call failed for 201700187C: Apify call failed: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "🔎 Processing 52859594J (12/50)\n",
      "  📡 Starting Apify run for 52859594J (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:353YNlddP0YfejYc2]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:353YNlddP0YfejYc2]\u001b[0m -> 2025-11-05T07:52:01.896Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:353YNlddP0YfejYc2]\u001b[0m -> 2025-11-05T07:52:01.898Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:353YNlddP0YfejYc2]\u001b[0m -> 2025-11-05T07:52:01.963Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:353YNlddP0YfejYc2]\u001b[0m -> 2025-11-05T07:52:02.174Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:353YNlddP0YfejYc2]\u001b[0m -> 2025-11-05T07:52:02.959Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:353YNlddP0YfejYc2]\u001b[0m -> 2025-11-05T07:52:03.083Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:353YNlddP0YfejYc2]\u001b[0m -> 2025-11-05T07:52:03.612Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:353YNlddP0YfejYc2]\u001b[0m -> 2025-11-05T07:52:03.725Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:353YNlddP0YfejYc2]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:353YNlddP0YfejYc2]\u001b[0m -> 2025-11-05T07:52:13.049Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 52859594J\n",
      "\u001b[36m[apify.puppeteer-scraper runId:353YNlddP0YfejYc2]\u001b[0m -> 2025-11-05T07:53:03.713Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":60166,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:353YNlddP0YfejYc2]\u001b[0m -> 2025-11-05T07:53:03.736Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:353YNlddP0YfejYc2]\u001b[0m -> 2025-11-05T07:53:24.589Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:353YNlddP0YfejYc2]\u001b[0m -> 2025-11-05T07:53:24.929Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":80631,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":80631,\"requestsTotal\":1,\"crawlerRuntimeMillis\":81382}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:353YNlddP0YfejYc2]\u001b[0m -> 2025-11-05T07:53:24.931Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:353YNlddP0YfejYc2]\u001b[0m -> 2025-11-05T07:53:24.948Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:353YNlddP0YfejYc2]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 52859594J (663957 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 25 dt tags\n",
      "  📝 Field 'contact number': +65 9852 2583\n",
      "  🔢 Extracted digits: 6598522583\n",
      "  ✅ Added from dt/dd (10 digits): +6598522583\n",
      "  ✅ Total phones found: ['+6598522583']\n",
      "  ✅ Processed 52859594J: 1 emails, 1 phones\n",
      "  💤 Sleeping for 27s before next request...\n",
      "\n",
      "🔎 Processing 201834600Z (13/50)\n",
      "  📡 Starting Apify run for 201834600Z (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:ne1XwMG2r62Y2tGT1]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ne1XwMG2r62Y2tGT1]\u001b[0m -> 2025-11-05T07:54:12.719Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ne1XwMG2r62Y2tGT1]\u001b[0m -> 2025-11-05T07:54:12.722Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ne1XwMG2r62Y2tGT1]\u001b[0m -> 2025-11-05T07:54:12.926Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ne1XwMG2r62Y2tGT1]\u001b[0m -> 2025-11-05T07:54:13.218Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ne1XwMG2r62Y2tGT1]\u001b[0m -> 2025-11-05T07:54:14.035Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ne1XwMG2r62Y2tGT1]\u001b[0m -> 2025-11-05T07:54:14.230Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ne1XwMG2r62Y2tGT1]\u001b[0m -> 2025-11-05T07:54:14.710Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ne1XwMG2r62Y2tGT1]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ne1XwMG2r62Y2tGT1]\u001b[0m -> 2025-11-05T07:54:14.796Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ne1XwMG2r62Y2tGT1]\u001b[0m -> 2025-11-05T07:54:27.617Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 201834600Z\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ne1XwMG2r62Y2tGT1]\u001b[0m -> 2025-11-05T07:54:53.810Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ne1XwMG2r62Y2tGT1]\u001b[0m -> 2025-11-05T07:54:54.218Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":38559,\"requestsFinishedPerMinute\":2,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":38559,\"requestsTotal\":1,\"crawlerRuntimeMillis\":39577}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ne1XwMG2r62Y2tGT1]\u001b[0m -> 2025-11-05T07:54:54.220Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ne1XwMG2r62Y2tGT1]\u001b[0m -> 2025-11-05T07:54:54.231Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:ne1XwMG2r62Y2tGT1]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 201834600Z (1132343 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 17 dt tags\n",
      "  📝 Field 'contact number': 6264 0718\n",
      "  🔢 Extracted digits: 62640718\n",
      "  ✅ Added from dt/dd (8 digits): +6562640718\n",
      "  ✅ Total phones found: ['+6562640718']\n",
      "  ✅ Processed 201834600Z: 1 emails, 1 phones\n",
      "  💤 Sleeping for 28s before next request...\n",
      "\n",
      "🔎 Processing 199903282W (14/50)\n",
      "  📡 Starting Apify run for 199903282W (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:OhO6aZg3dcL61QunY]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:OhO6aZg3dcL61QunY]\u001b[0m -> 2025-11-05T07:55:43.618Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:OhO6aZg3dcL61QunY]\u001b[0m -> 2025-11-05T07:55:43.620Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:OhO6aZg3dcL61QunY]\u001b[0m -> 2025-11-05T07:55:43.669Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:OhO6aZg3dcL61QunY]\u001b[0m -> 2025-11-05T07:55:43.853Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:OhO6aZg3dcL61QunY]\u001b[0m -> 2025-11-05T07:55:44.490Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:OhO6aZg3dcL61QunY]\u001b[0m -> 2025-11-05T07:55:44.637Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:OhO6aZg3dcL61QunY]\u001b[0m -> 2025-11-05T07:55:45.555Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:OhO6aZg3dcL61QunY]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:OhO6aZg3dcL61QunY]\u001b[0m -> 2025-11-05T07:55:45.692Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:OhO6aZg3dcL61QunY]\u001b[0m -> 2025-11-05T07:55:53.846Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:OhO6aZg3dcL61QunY]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:OhO6aZg3dcL61QunY]\u001b[0m -> 2025-11-05T07:55:53.856Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:OhO6aZg3dcL61QunY]\u001b[0m -> 2025-11-05T07:56:31.497Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 199903282W\n",
      "\u001b[36m[apify.puppeteer-scraper runId:OhO6aZg3dcL61QunY]\u001b[0m -> 2025-11-05T07:56:45.693Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":60209,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:OhO6aZg3dcL61QunY]\u001b[0m -> 2025-11-05T07:56:45.793Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0.019},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0.035},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:OhO6aZg3dcL61QunY]\u001b[0m -> 2025-11-05T07:57:27.980Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:OhO6aZg3dcL61QunY]\u001b[0m -> 2025-11-05T07:57:28.467Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[null,1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":93283,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":93283,\"requestsTotal\":1,\"crawlerRuntimeMillis\":102983}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:OhO6aZg3dcL61QunY]\u001b[0m -> 2025-11-05T07:57:28.469Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:OhO6aZg3dcL61QunY]\u001b[0m -> 2025-11-05T07:57:28.485Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:OhO6aZg3dcL61QunY]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 199903282W (471931 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 19 dt tags\n",
      "  📝 Field 'contact number': 6567 1808\n",
      "  🔢 Extracted digits: 65671808\n",
      "  ✅ Added from dt/dd (8 digits): +6565671808\n",
      "  ✅ Total phones found: ['+6565671808']\n",
      "  ✅ Processed 199903282W: 1 emails, 1 phones\n",
      "  💤 Sleeping for 29s before next request...\n",
      "\n",
      "🔎 Processing 200415577K (15/50)\n",
      "  📡 Starting Apify run for 200415577K (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> 2025-11-05T07:58:14.358Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> 2025-11-05T07:58:14.359Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> 2025-11-05T07:58:14.413Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> 2025-11-05T07:58:14.619Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> 2025-11-05T07:58:16.040Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> 2025-11-05T07:58:16.229Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> 2025-11-05T07:58:17.127Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> 2025-11-05T07:58:17.291Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> 2025-11-05T07:58:52.460Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. net::ERR_TIMED_OUT at https://recordowl.com/\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> 2025-11-05T07:58:52.462Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> 2025-11-05T07:59:17.291Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":60256,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> 2025-11-05T07:59:17.317Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> 2025-11-05T07:59:36.873Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> 2025-11-05T07:59:36.875Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":2}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> 2025-11-05T07:59:49.600Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 200415577K\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> 2025-11-05T08:00:13.354Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> 2025-11-05T08:00:13.991Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[null,null,1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":35533,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":35533,\"requestsTotal\":1,\"crawlerRuntimeMillis\":116955}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> 2025-11-05T08:00:13.994Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> 2025-11-05T08:00:14.047Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CPJWjaVV4Ql9V965S]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 200415577K (1156996 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 18 dt tags\n",
      "  📝 Field 'contact number': 6508 7400\n",
      "  🔢 Extracted digits: 65087400\n",
      "  ✅ Added from dt/dd (8 digits): +6565087400\n",
      "  ✅ Total phones found: ['+6565087400']\n",
      "  ✅ Processed 200415577K: 0 emails, 1 phones\n",
      "  💤 Sleeping for 30s before next request...\n",
      "  🛑 Checkpoint pause: waiting extra 30s...\n",
      "\n",
      "🔎 Processing 53240200J (16/50)\n",
      "  📡 Starting Apify run for 53240200J (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:yw8xudzUAVneHtVvR]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:yw8xudzUAVneHtVvR]\u001b[0m -> 2025-11-05T08:01:30.730Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:yw8xudzUAVneHtVvR]\u001b[0m -> 2025-11-05T08:01:30.732Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:yw8xudzUAVneHtVvR]\u001b[0m -> 2025-11-05T08:01:30.824Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:yw8xudzUAVneHtVvR]\u001b[0m -> 2025-11-05T08:01:31.006Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:yw8xudzUAVneHtVvR]\u001b[0m -> 2025-11-05T08:01:32.245Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:yw8xudzUAVneHtVvR]\u001b[0m -> 2025-11-05T08:01:32.443Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:yw8xudzUAVneHtVvR]\u001b[0m -> 2025-11-05T08:01:33.256Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:yw8xudzUAVneHtVvR]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:yw8xudzUAVneHtVvR]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:yw8xudzUAVneHtVvR]\u001b[0m -> 2025-11-05T08:01:33.510Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:yw8xudzUAVneHtVvR]\u001b[0m -> 2025-11-05T08:01:49.091Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 53240200J\n",
      "\u001b[36m[apify.puppeteer-scraper runId:yw8xudzUAVneHtVvR]\u001b[0m -> 2025-11-05T08:02:25.526Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:yw8xudzUAVneHtVvR]\u001b[0m -> 2025-11-05T08:02:26.036Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":51398,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":51398,\"requestsTotal\":1,\"crawlerRuntimeMillis\":52855}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:yw8xudzUAVneHtVvR]\u001b[0m -> 2025-11-05T08:02:26.038Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:yw8xudzUAVneHtVvR]\u001b[0m -> 2025-11-05T08:02:26.057Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:yw8xudzUAVneHtVvR]\u001b[0m -> Status: RUNNING, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 53240200J (478781 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 19 dt tags\n",
      "  📝 Field 'contact number': 6746 5959\n",
      "  🔢 Extracted digits: 67465959\n",
      "  ✅ Added from dt/dd (8 digits): +6567465959\n",
      "  ✅ Total phones found: ['+6567465959']\n",
      "  ✅ Processed 53240200J: 0 emails, 1 phones\n",
      "  💤 Sleeping for 31s before next request...\n",
      "\n",
      "🔎 Processing 202533515K (17/50)\n",
      "  📡 Starting Apify run for 202533515K (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:0jPJJiQvKxdNpGyH8]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:0jPJJiQvKxdNpGyH8]\u001b[0m -> 2025-11-05T08:03:18.787Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:0jPJJiQvKxdNpGyH8]\u001b[0m -> 2025-11-05T08:03:18.789Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:0jPJJiQvKxdNpGyH8]\u001b[0m -> 2025-11-05T08:03:18.833Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:0jPJJiQvKxdNpGyH8]\u001b[0m -> 2025-11-05T08:03:19.022Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:0jPJJiQvKxdNpGyH8]\u001b[0m -> 2025-11-05T08:03:19.632Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:0jPJJiQvKxdNpGyH8]\u001b[0m -> 2025-11-05T08:03:19.830Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:0jPJJiQvKxdNpGyH8]\u001b[0m -> 2025-11-05T08:03:20.588Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:0jPJJiQvKxdNpGyH8]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:0jPJJiQvKxdNpGyH8]\u001b[0m -> 2025-11-05T08:03:20.709Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:0jPJJiQvKxdNpGyH8]\u001b[0m -> 2025-11-05T08:03:35.634Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 202533515K\n",
      "\u001b[36m[apify.puppeteer-scraper runId:0jPJJiQvKxdNpGyH8]\u001b[0m -> 2025-11-05T08:04:10.108Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:0jPJJiQvKxdNpGyH8]\u001b[0m -> 2025-11-05T08:04:10.771Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":48356,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":48356,\"requestsTotal\":1,\"crawlerRuntimeMillis\":50251}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:0jPJJiQvKxdNpGyH8]\u001b[0m -> 2025-11-05T08:04:10.773Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:0jPJJiQvKxdNpGyH8]\u001b[0m -> Status: RUNNING, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:0jPJJiQvKxdNpGyH8]\u001b[0m -> 2025-11-05T08:04:10.874Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 202533515K (1116848 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 17 dt tags\n",
      "  🔎 No phones found yet, searching entire content...\n",
      "  🔍 Pattern 4 found 1 potential matches\n",
      "  🔢 Pattern 4 match: '80 121465' → digits: '80121465'\n",
      "  ✅ Added from pattern 4 (8 digits): +6580121465\n",
      "  ✅ Total phones found: ['+6580121465']\n",
      "  ✅ Processed 202533515K: 0 emails, 1 phones\n",
      "  💤 Sleeping for 32s before next request...\n",
      "\n",
      "🔎 Processing 200820584C (18/50)\n",
      "  📡 Starting Apify run for 200820584C (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:emwf0fFlY3rngAgiI]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:emwf0fFlY3rngAgiI]\u001b[0m -> 2025-11-05T08:05:00.043Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:emwf0fFlY3rngAgiI]\u001b[0m -> 2025-11-05T08:05:00.045Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:emwf0fFlY3rngAgiI]\u001b[0m -> 2025-11-05T08:05:00.089Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:emwf0fFlY3rngAgiI]\u001b[0m -> 2025-11-05T08:05:00.298Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:emwf0fFlY3rngAgiI]\u001b[0m -> 2025-11-05T08:05:01.614Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:emwf0fFlY3rngAgiI]\u001b[0m -> 2025-11-05T08:05:01.927Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:emwf0fFlY3rngAgiI]\u001b[0m -> 2025-11-05T08:05:02.880Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:emwf0fFlY3rngAgiI]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:emwf0fFlY3rngAgiI]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:emwf0fFlY3rngAgiI]\u001b[0m -> 2025-11-05T08:05:03.063Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:emwf0fFlY3rngAgiI]\u001b[0m -> 2025-11-05T08:05:19.570Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 200820584C\n",
      "\u001b[36m[apify.puppeteer-scraper runId:emwf0fFlY3rngAgiI]\u001b[0m -> 2025-11-05T08:05:36.622Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:emwf0fFlY3rngAgiI]\u001b[0m -> 2025-11-05T08:05:37.089Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":32983,\"requestsFinishedPerMinute\":2,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":32983,\"requestsTotal\":1,\"crawlerRuntimeMillis\":34278}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:emwf0fFlY3rngAgiI]\u001b[0m -> 2025-11-05T08:05:37.090Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:emwf0fFlY3rngAgiI]\u001b[0m -> 2025-11-05T08:05:37.107Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:emwf0fFlY3rngAgiI]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 200820584C (1476062 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 18 dt tags\n",
      "  📝 Field 'contact number': 6473 0048\n",
      "  🔢 Extracted digits: 64730048\n",
      "  ✅ Added from dt/dd (8 digits): +6564730048\n",
      "  ✅ Total phones found: ['+6564730048']\n",
      "  ✅ Processed 200820584C: 0 emails, 1 phones\n",
      "  💤 Sleeping for 33s before next request...\n",
      "\n",
      "🔎 Processing 202225156W (19/50)\n",
      "  📡 Starting Apify run for 202225156W (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:3My73IBeYKgqMBRJ5]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:3My73IBeYKgqMBRJ5]\u001b[0m -> 2025-11-05T08:06:25.733Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3My73IBeYKgqMBRJ5]\u001b[0m -> 2025-11-05T08:06:25.735Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3My73IBeYKgqMBRJ5]\u001b[0m -> 2025-11-05T08:06:25.814Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3My73IBeYKgqMBRJ5]\u001b[0m -> 2025-11-05T08:06:26.031Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3My73IBeYKgqMBRJ5]\u001b[0m -> 2025-11-05T08:06:26.745Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3My73IBeYKgqMBRJ5]\u001b[0m -> 2025-11-05T08:06:26.861Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3My73IBeYKgqMBRJ5]\u001b[0m -> 2025-11-05T08:06:27.437Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3My73IBeYKgqMBRJ5]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3My73IBeYKgqMBRJ5]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3My73IBeYKgqMBRJ5]\u001b[0m -> 2025-11-05T08:06:27.558Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3My73IBeYKgqMBRJ5]\u001b[0m -> 2025-11-05T08:06:46.329Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 202225156W\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3My73IBeYKgqMBRJ5]\u001b[0m -> 2025-11-05T08:07:08.119Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3My73IBeYKgqMBRJ5]\u001b[0m -> 2025-11-05T08:07:08.481Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":40240,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":40240,\"requestsTotal\":1,\"crawlerRuntimeMillis\":41109}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3My73IBeYKgqMBRJ5]\u001b[0m -> 2025-11-05T08:07:08.483Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3My73IBeYKgqMBRJ5]\u001b[0m -> 2025-11-05T08:07:08.498Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:3My73IBeYKgqMBRJ5]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ⚠️ Could not verify dataset: Expecting value: line 1 column 1 (char 0)\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 202225156W (781055 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 16 dt tags\n",
      "  🔎 No phones found yet, searching entire content...\n",
      "  ⚠️ WARNING: No phone numbers found for 202225156W\n",
      "  📄 Showing first 500 chars of parent HTML for debugging:\n",
      "<div class=\"max-w-7xl mx-auto lg:py-6 sm:px-6 lg:px-8\" style=\"height: auto !important;\">\n",
      " <div class=\"flex flex-col lg:flex-row\" style=\"height: auto !important;\">\n",
      "  <div class=\"w-full lg:w-2/3 lg:pr-8\" style=\"height: auto !important;\">\n",
      "   <div class=\"lg:mb-4 border-b border-gray-200\">\n",
      "    <ul class=\"flex flex-nowrap overflow-x-auto -mb-px text-sm font-medium text-center scrollbar-hide\" id=\"companyTabs\" role=\"tablist\">\n",
      "     <li class=\"mr-2\" role=\"presentation\">\n",
      "      <button aria-controls=\"overvi...\n",
      "  ✅ Processed 202225156W: 0 emails, 0 phones\n",
      "  💤 Sleeping for 34s before next request...\n",
      "\n",
      "🔎 Processing 202119083M (20/50)\n",
      "  📡 Starting Apify run for 202119083M (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> 2025-11-05T08:10:56.318Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> 2025-11-05T08:10:56.320Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> 2025-11-05T08:10:56.355Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> 2025-11-05T08:10:56.536Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> 2025-11-05T08:10:57.171Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> 2025-11-05T08:10:57.343Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> 2025-11-05T08:10:57.856Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> 2025-11-05T08:10:58.270Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> 2025-11-05T08:11:37.969Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. net::ERR_SOCKET_NOT_CONNECTED at https://recordowl.com/\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> 2025-11-05T08:11:37.971Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> 2025-11-05T08:11:58.270Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":60478,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> 2025-11-05T08:11:58.362Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> 2025-11-05T08:12:38.373Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 202119083M\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> 2025-11-05T08:12:58.270Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":120478,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> 2025-11-05T08:12:58.347Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0.07},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> 2025-11-05T08:13:58.270Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":180478,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> 2025-11-05T08:13:58.349Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> 2025-11-05T08:14:11.134Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> 2025-11-05T08:14:11.646Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[null,1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":152572,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":152572,\"requestsTotal\":1,\"crawlerRuntimeMillis\":193854}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> 2025-11-05T08:14:11.648Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> 2025-11-05T08:14:11.668Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:lb6Lh87tbHAqbGQZG]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 202119083M (1446523 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 16 dt tags\n",
      "  🔎 No phones found yet, searching entire content...\n",
      "  ⚠️ WARNING: No phone numbers found for 202119083M\n",
      "  📄 Showing first 500 chars of parent HTML for debugging:\n",
      "<div class=\"max-w-7xl mx-auto lg:py-6 sm:px-6 lg:px-8\" style=\"height: auto !important;\">\n",
      " <div class=\"flex flex-col lg:flex-row\" style=\"height: auto !important;\">\n",
      "  <div class=\"w-full lg:w-2/3 lg:pr-8\" style=\"height: auto !important;\">\n",
      "   <div class=\"lg:mb-4 border-b border-gray-200\">\n",
      "    <ul class=\"flex flex-nowrap overflow-x-auto -mb-px text-sm font-medium text-center scrollbar-hide\" id=\"companyTabs\" role=\"tablist\">\n",
      "     <li class=\"mr-2\" role=\"presentation\">\n",
      "      <button aria-controls=\"overvi...\n",
      "  ✅ Processed 202119083M: 0 emails, 0 phones\n",
      "  💤 Sleeping for 25s before next request...\n",
      "  🛑 Checkpoint pause: waiting extra 30s...\n",
      "\n",
      "🔎 Processing 202342689M (21/50)\n",
      "  📡 Starting Apify run for 202342689M (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:wBpTpSQjO6gQYjnKI]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:wBpTpSQjO6gQYjnKI]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:wBpTpSQjO6gQYjnKI]\u001b[0m -> 2025-11-05T08:15:39.860Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:wBpTpSQjO6gQYjnKI]\u001b[0m -> 2025-11-05T08:15:39.862Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:wBpTpSQjO6gQYjnKI]\u001b[0m -> 2025-11-05T08:15:39.900Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:wBpTpSQjO6gQYjnKI]\u001b[0m -> 2025-11-05T08:15:40.112Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:wBpTpSQjO6gQYjnKI]\u001b[0m -> 2025-11-05T08:15:40.783Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:wBpTpSQjO6gQYjnKI]\u001b[0m -> 2025-11-05T08:15:40.879Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:wBpTpSQjO6gQYjnKI]\u001b[0m -> 2025-11-05T08:15:41.443Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:wBpTpSQjO6gQYjnKI]\u001b[0m -> 2025-11-05T08:15:41.557Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:wBpTpSQjO6gQYjnKI]\u001b[0m -> 2025-11-05T08:15:59.508Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 202342689M\n",
      "\u001b[36m[apify.puppeteer-scraper runId:wBpTpSQjO6gQYjnKI]\u001b[0m -> 2025-11-05T08:16:13.225Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:wBpTpSQjO6gQYjnKI]\u001b[0m -> 2025-11-05T08:16:13.663Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":31383,\"requestsFinishedPerMinute\":2,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":31383,\"requestsTotal\":1,\"crawlerRuntimeMillis\":32296}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:wBpTpSQjO6gQYjnKI]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 202342689M (121350 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 18 dt tags\n",
      "  🔎 No phones found yet, searching entire content...\n",
      "  ⚠️ WARNING: No phone numbers found for 202342689M\n",
      "  📄 Showing first 500 chars of parent HTML for debugging:\n",
      "<div class=\"max-w-7xl mx-auto lg:py-6 sm:px-6 lg:px-8\">\n",
      " <div class=\"flex flex-col lg:flex-row\">\n",
      "  <div class=\"w-full lg:w-2/3 lg:pr-8\">\n",
      "   <div class=\"lg:mb-4 border-b border-gray-200\">\n",
      "    <ul class=\"flex flex-nowrap overflow-x-auto -mb-px text-sm font-medium text-center scrollbar-hide\" id=\"companyTabs\" role=\"tablist\">\n",
      "     <li class=\"mr-2\" role=\"presentation\">\n",
      "      <button aria-controls=\"overview\" aria-selected=\"true\" class=\"inline-block p-4 border-b-2 rounded-t-lg transition-colors duration...\n",
      "  ✅ Processed 202342689M: 0 emails, 0 phones\n",
      "  💤 Sleeping for 26s before next request...\n",
      "\n",
      "🔎 Processing 45376600E (22/50)\n",
      "  📡 Starting Apify run for 45376600E (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:6TLCM339j7Jqxq7OS]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6TLCM339j7Jqxq7OS]\u001b[0m -> 2025-11-05T08:17:00.194Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6TLCM339j7Jqxq7OS]\u001b[0m -> 2025-11-05T08:17:00.196Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6TLCM339j7Jqxq7OS]\u001b[0m -> 2025-11-05T08:17:00.301Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6TLCM339j7Jqxq7OS]\u001b[0m -> 2025-11-05T08:17:00.525Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6TLCM339j7Jqxq7OS]\u001b[0m -> 2025-11-05T08:17:01.311Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6TLCM339j7Jqxq7OS]\u001b[0m -> 2025-11-05T08:17:01.464Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6TLCM339j7Jqxq7OS]\u001b[0m -> 2025-11-05T08:17:02.230Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6TLCM339j7Jqxq7OS]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6TLCM339j7Jqxq7OS]\u001b[0m -> 2025-11-05T08:17:02.335Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6TLCM339j7Jqxq7OS]\u001b[0m -> 2025-11-05T08:17:16.221Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 45376600E\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6TLCM339j7Jqxq7OS]\u001b[0m -> 2025-11-05T08:17:31.747Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6TLCM339j7Jqxq7OS]\u001b[0m -> 2025-11-05T08:17:32.062Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":29048,\"requestsFinishedPerMinute\":2,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":29048,\"requestsTotal\":1,\"crawlerRuntimeMillis\":29909}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6TLCM339j7Jqxq7OS]\u001b[0m -> 2025-11-05T08:17:32.062Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6TLCM339j7Jqxq7OS]\u001b[0m -> 2025-11-05T08:17:32.076Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:6TLCM339j7Jqxq7OS]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 45376600E (1122702 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 20 dt tags\n",
      "  📝 Field 'contact number': 6448 4828\n",
      "  🔢 Extracted digits: 64484828\n",
      "  ✅ Added from dt/dd (8 digits): +6564484828\n",
      "  ✅ Total phones found: ['+6564484828']\n",
      "  ✅ Processed 45376600E: 1 emails, 1 phones\n",
      "  💤 Sleeping for 27s before next request...\n",
      "\n",
      "🔎 Processing 202020005Z (23/50)\n",
      "  📡 Starting Apify run for 202020005Z (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:CXAlVwdTN9icWaDth]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:CXAlVwdTN9icWaDth]\u001b[0m -> 2025-11-05T08:18:15.652Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CXAlVwdTN9icWaDth]\u001b[0m -> 2025-11-05T08:18:15.653Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CXAlVwdTN9icWaDth]\u001b[0m -> 2025-11-05T08:18:15.973Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CXAlVwdTN9icWaDth]\u001b[0m -> 2025-11-05T08:18:16.296Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CXAlVwdTN9icWaDth]\u001b[0m -> 2025-11-05T08:18:17.050Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CXAlVwdTN9icWaDth]\u001b[0m -> 2025-11-05T08:18:17.168Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CXAlVwdTN9icWaDth]\u001b[0m -> 2025-11-05T08:18:17.860Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CXAlVwdTN9icWaDth]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CXAlVwdTN9icWaDth]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CXAlVwdTN9icWaDth]\u001b[0m -> 2025-11-05T08:18:17.998Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CXAlVwdTN9icWaDth]\u001b[0m -> 2025-11-05T08:18:34.434Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 202020005Z\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CXAlVwdTN9icWaDth]\u001b[0m -> 2025-11-05T08:19:04.131Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CXAlVwdTN9icWaDth]\u001b[0m -> 2025-11-05T08:19:04.675Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":45557,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":45557,\"requestsTotal\":1,\"crawlerRuntimeMillis\":46912}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CXAlVwdTN9icWaDth]\u001b[0m -> 2025-11-05T08:19:04.676Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CXAlVwdTN9icWaDth]\u001b[0m -> Status: RUNNING, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:CXAlVwdTN9icWaDth]\u001b[0m -> 2025-11-05T08:19:04.692Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 202020005Z (791482 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 18 dt tags\n",
      "  🔎 No phones found yet, searching entire content...\n",
      "  ⚠️ WARNING: No phone numbers found for 202020005Z\n",
      "  📄 Showing first 500 chars of parent HTML for debugging:\n",
      "<div class=\"max-w-7xl mx-auto lg:py-6 sm:px-6 lg:px-8\" style=\"height: auto !important;\">\n",
      " <div class=\"flex flex-col lg:flex-row\" style=\"height: auto !important;\">\n",
      "  <div class=\"w-full lg:w-2/3 lg:pr-8\" style=\"height: auto !important;\">\n",
      "   <div class=\"lg:mb-4 border-b border-gray-200\">\n",
      "    <ul class=\"flex flex-nowrap overflow-x-auto -mb-px text-sm font-medium text-center scrollbar-hide\" id=\"companyTabs\" role=\"tablist\">\n",
      "     <li class=\"mr-2\" role=\"presentation\">\n",
      "      <button aria-controls=\"overvi...\n",
      "  ✅ Processed 202020005Z: 0 emails, 0 phones\n",
      "  💤 Sleeping for 28s before next request...\n",
      "\n",
      "🔎 Processing 202120660M (24/50)\n",
      "  📡 Starting Apify run for 202120660M (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:o9Ug0qBh2V019THJF]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:o9Ug0qBh2V019THJF]\u001b[0m -> 2025-11-05T08:19:50.228Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:o9Ug0qBh2V019THJF]\u001b[0m -> 2025-11-05T08:19:50.229Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:o9Ug0qBh2V019THJF]\u001b[0m -> 2025-11-05T08:19:50.358Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:o9Ug0qBh2V019THJF]\u001b[0m -> 2025-11-05T08:19:50.622Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:o9Ug0qBh2V019THJF]\u001b[0m -> 2025-11-05T08:19:51.440Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:o9Ug0qBh2V019THJF]\u001b[0m -> 2025-11-05T08:19:51.536Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:o9Ug0qBh2V019THJF]\u001b[0m -> 2025-11-05T08:19:52.241Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:o9Ug0qBh2V019THJF]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:o9Ug0qBh2V019THJF]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:o9Ug0qBh2V019THJF]\u001b[0m -> 2025-11-05T08:19:52.341Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:o9Ug0qBh2V019THJF]\u001b[0m -> 2025-11-05T08:20:04.528Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 202120660M\n",
      "\u001b[36m[apify.puppeteer-scraper runId:o9Ug0qBh2V019THJF]\u001b[0m -> 2025-11-05T08:20:30.109Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:o9Ug0qBh2V019THJF]\u001b[0m -> 2025-11-05T08:20:30.543Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":37261,\"requestsFinishedPerMinute\":2,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":37261,\"requestsTotal\":1,\"crawlerRuntimeMillis\":38412}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:o9Ug0qBh2V019THJF]\u001b[0m -> 2025-11-05T08:20:30.544Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:o9Ug0qBh2V019THJF]\u001b[0m -> Status: RUNNING, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:o9Ug0qBh2V019THJF]\u001b[0m -> 2025-11-05T08:20:30.561Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 202120660M (1406110 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 15 dt tags\n",
      "  🔎 No phones found yet, searching entire content...\n",
      "  ⚠️ WARNING: No phone numbers found for 202120660M\n",
      "  📄 Showing first 500 chars of parent HTML for debugging:\n",
      "<div class=\"max-w-7xl mx-auto lg:py-6 sm:px-6 lg:px-8\" style=\"height: auto !important;\">\n",
      " <div class=\"flex flex-col lg:flex-row\" style=\"height: auto !important;\">\n",
      "  <div class=\"w-full lg:w-2/3 lg:pr-8\" style=\"height: auto !important;\">\n",
      "   <div class=\"lg:mb-4 border-b border-gray-200\">\n",
      "    <ul class=\"flex flex-nowrap overflow-x-auto -mb-px text-sm font-medium text-center scrollbar-hide\" id=\"companyTabs\" role=\"tablist\">\n",
      "     <li class=\"mr-2\" role=\"presentation\">\n",
      "      <button aria-controls=\"overvi...\n",
      "  ✅ Processed 202120660M: 0 emails, 0 phones\n",
      "  💤 Sleeping for 29s before next request...\n",
      "\n",
      "🔎 Processing 52842965E (25/50)\n",
      "  📡 Starting Apify run for 52842965E (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:hddbNIbQ0s36OKXe9]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:hddbNIbQ0s36OKXe9]\u001b[0m -> 2025-11-05T08:21:15.475Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:hddbNIbQ0s36OKXe9]\u001b[0m -> 2025-11-05T08:21:15.477Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:hddbNIbQ0s36OKXe9]\u001b[0m -> 2025-11-05T08:21:15.574Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:hddbNIbQ0s36OKXe9]\u001b[0m -> 2025-11-05T08:21:15.755Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:hddbNIbQ0s36OKXe9]\u001b[0m -> 2025-11-05T08:21:16.440Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:hddbNIbQ0s36OKXe9]\u001b[0m -> 2025-11-05T08:21:16.547Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:hddbNIbQ0s36OKXe9]\u001b[0m -> 2025-11-05T08:21:17.473Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:hddbNIbQ0s36OKXe9]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:hddbNIbQ0s36OKXe9]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:hddbNIbQ0s36OKXe9]\u001b[0m -> 2025-11-05T08:21:17.591Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:hddbNIbQ0s36OKXe9]\u001b[0m -> 2025-11-05T08:21:28.791Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 52842965E\n",
      "\u001b[36m[apify.puppeteer-scraper runId:hddbNIbQ0s36OKXe9]\u001b[0m -> 2025-11-05T08:21:48.307Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:hddbNIbQ0s36OKXe9]\u001b[0m -> 2025-11-05T08:21:48.651Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":30346,\"requestsFinishedPerMinute\":2,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":30346,\"requestsTotal\":1,\"crawlerRuntimeMillis\":31248}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:hddbNIbQ0s36OKXe9]\u001b[0m -> 2025-11-05T08:21:48.653Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:hddbNIbQ0s36OKXe9]\u001b[0m -> Status: RUNNING, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:hddbNIbQ0s36OKXe9]\u001b[0m -> 2025-11-05T08:21:48.665Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 52842965E (1436212 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 17 dt tags\n",
      "  🔎 No phones found yet, searching entire content...\n",
      "  ⚠️ WARNING: No phone numbers found for 52842965E\n",
      "  📄 Showing first 500 chars of parent HTML for debugging:\n",
      "<div class=\"max-w-7xl mx-auto lg:py-6 sm:px-6 lg:px-8\" style=\"height: auto !important;\">\n",
      " <div class=\"flex flex-col lg:flex-row\" style=\"height: auto !important;\">\n",
      "  <div class=\"w-full lg:w-2/3 lg:pr-8\" style=\"height: auto !important;\">\n",
      "   <div class=\"lg:mb-4 border-b border-gray-200\">\n",
      "    <ul class=\"flex flex-nowrap overflow-x-auto -mb-px text-sm font-medium text-center scrollbar-hide\" id=\"companyTabs\" role=\"tablist\">\n",
      "     <li class=\"mr-2\" role=\"presentation\">\n",
      "      <button aria-controls=\"overvi...\n",
      "  ✅ Processed 52842965E: 0 emails, 0 phones\n",
      "  💤 Sleeping for 30s before next request...\n",
      "  🛑 Checkpoint pause: waiting extra 30s...\n",
      "\n",
      "🔎 Processing 202438145H (26/50)\n",
      "  📡 Starting Apify run for 202438145H (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> 2025-11-05T08:23:04.148Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> 2025-11-05T08:23:04.151Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> 2025-11-05T08:23:04.210Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> 2025-11-05T08:23:04.428Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> 2025-11-05T08:23:05.955Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> 2025-11-05T08:23:06.064Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> 2025-11-05T08:23:06.933Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> 2025-11-05T08:23:07.001Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> 2025-11-05T08:23:13.745Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> 2025-11-05T08:23:13.752Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> 2025-11-05T08:23:19.409Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Detected a session error, rotating session...\n",
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> 2025-11-05T08:23:19.409Z net::ERR_TUNNEL_CONNECTION_FAILED at https://recordowl.com/\n",
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> 2025-11-05T08:23:19.410Z\n",
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> 2025-11-05T08:23:19.410Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":2}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> 2025-11-05T08:23:33.474Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 202438145H\n",
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> 2025-11-05T08:23:54.994Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> 2025-11-05T08:23:55.420Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[null,null,1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":34968,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":34968,\"requestsTotal\":1,\"crawlerRuntimeMillis\":48576}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> 2025-11-05T08:23:55.428Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> 2025-11-05T08:23:55.439Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:bScDWnmoKMXeYFjG4]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 202438145H (1461683 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 18 dt tags\n",
      "  🔎 No phones found yet, searching entire content...\n",
      "  ⚠️ WARNING: No phone numbers found for 202438145H\n",
      "  📄 Showing first 500 chars of parent HTML for debugging:\n",
      "<div class=\"max-w-7xl mx-auto lg:py-6 sm:px-6 lg:px-8\" style=\"height: auto !important;\">\n",
      " <div class=\"flex flex-col lg:flex-row\" style=\"height: auto !important;\">\n",
      "  <div class=\"w-full lg:w-2/3 lg:pr-8\" style=\"height: auto !important;\">\n",
      "   <div class=\"lg:mb-4 border-b border-gray-200\">\n",
      "    <ul class=\"flex flex-nowrap overflow-x-auto -mb-px text-sm font-medium text-center scrollbar-hide\" id=\"companyTabs\" role=\"tablist\">\n",
      "     <li class=\"mr-2\" role=\"presentation\">\n",
      "      <button aria-controls=\"overvi...\n",
      "  ✅ Processed 202438145H: 0 emails, 0 phones\n",
      "  💤 Sleeping for 31s before next request...\n",
      "\n",
      "🔎 Processing 201534300E (27/50)\n",
      "  📡 Starting Apify run for 201534300E (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:4CXa2oR99jziXldrf]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:4CXa2oR99jziXldrf]\u001b[0m -> 2025-11-05T08:24:43.605Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4CXa2oR99jziXldrf]\u001b[0m -> 2025-11-05T08:24:43.607Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4CXa2oR99jziXldrf]\u001b[0m -> 2025-11-05T08:24:43.659Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4CXa2oR99jziXldrf]\u001b[0m -> 2025-11-05T08:24:43.858Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4CXa2oR99jziXldrf]\u001b[0m -> 2025-11-05T08:24:44.567Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4CXa2oR99jziXldrf]\u001b[0m -> 2025-11-05T08:24:44.703Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4CXa2oR99jziXldrf]\u001b[0m -> 2025-11-05T08:24:45.265Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4CXa2oR99jziXldrf]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4CXa2oR99jziXldrf]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4CXa2oR99jziXldrf]\u001b[0m -> 2025-11-05T08:24:45.357Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4CXa2oR99jziXldrf]\u001b[0m -> 2025-11-05T08:25:16.164Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4CXa2oR99jziXldrf]\u001b[0m -> 2025-11-05T08:25:16.166Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4CXa2oR99jziXldrf]\u001b[0m -> 2025-11-05T08:25:34.574Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 201534300E\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4CXa2oR99jziXldrf]\u001b[0m -> 2025-11-05T08:25:45.358Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":60159,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4CXa2oR99jziXldrf]\u001b[0m -> 2025-11-05T08:25:45.393Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0.071},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4CXa2oR99jziXldrf]\u001b[0m -> 2025-11-05T08:26:03.255Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4CXa2oR99jziXldrf]\u001b[0m -> 2025-11-05T08:26:03.787Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[null,1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":46387,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":46387,\"requestsTotal\":1,\"crawlerRuntimeMillis\":78588}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4CXa2oR99jziXldrf]\u001b[0m -> 2025-11-05T08:26:03.789Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4CXa2oR99jziXldrf]\u001b[0m -> 2025-11-05T08:26:03.801Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:4CXa2oR99jziXldrf]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 201534300E (147236 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 20 dt tags\n",
      "  📝 Field 'contact number': 8382 5133\n",
      "  🔢 Extracted digits: 83825133\n",
      "  ✅ Added from dt/dd (8 digits): +6583825133\n",
      "  ✅ Total phones found: ['+6583825133']\n",
      "  ✅ Processed 201534300E: 0 emails, 1 phones\n",
      "  💤 Sleeping for 32s before next request...\n",
      "\n",
      "🔎 Processing 201333430N (28/50)\n",
      "  📡 Starting Apify run for 201333430N (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:aXlTIpY77l3UETmKL]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:aXlTIpY77l3UETmKL]\u001b[0m -> 2025-11-05T08:26:51.397Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aXlTIpY77l3UETmKL]\u001b[0m -> 2025-11-05T08:26:51.399Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aXlTIpY77l3UETmKL]\u001b[0m -> 2025-11-05T08:26:51.469Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aXlTIpY77l3UETmKL]\u001b[0m -> 2025-11-05T08:26:51.712Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aXlTIpY77l3UETmKL]\u001b[0m -> 2025-11-05T08:26:52.347Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aXlTIpY77l3UETmKL]\u001b[0m -> 2025-11-05T08:26:52.467Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aXlTIpY77l3UETmKL]\u001b[0m -> 2025-11-05T08:26:53.106Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aXlTIpY77l3UETmKL]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aXlTIpY77l3UETmKL]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aXlTIpY77l3UETmKL]\u001b[0m -> 2025-11-05T08:26:53.183Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aXlTIpY77l3UETmKL]\u001b[0m -> 2025-11-05T08:27:44.456Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 201333430N\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aXlTIpY77l3UETmKL]\u001b[0m -> 2025-11-05T08:27:53.184Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":60152,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aXlTIpY77l3UETmKL]\u001b[0m -> 2025-11-05T08:27:53.210Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aXlTIpY77l3UETmKL]\u001b[0m -> 2025-11-05T08:28:11.780Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aXlTIpY77l3UETmKL]\u001b[0m -> 2025-11-05T08:28:12.360Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":78234,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":78234,\"requestsTotal\":1,\"crawlerRuntimeMillis\":79328}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aXlTIpY77l3UETmKL]\u001b[0m -> 2025-11-05T08:28:12.361Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aXlTIpY77l3UETmKL]\u001b[0m -> Status: RUNNING, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aXlTIpY77l3UETmKL]\u001b[0m -> 2025-11-05T08:28:12.375Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 201333430N (796435 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 19 dt tags\n",
      "  🔎 No phones found yet, searching entire content...\n",
      "  ⚠️ WARNING: No phone numbers found for 201333430N\n",
      "  📄 Showing first 500 chars of parent HTML for debugging:\n",
      "<div class=\"max-w-7xl mx-auto lg:py-6 sm:px-6 lg:px-8\" style=\"height: auto !important;\">\n",
      " <div class=\"flex flex-col lg:flex-row\" style=\"height: auto !important;\">\n",
      "  <div class=\"w-full lg:w-2/3 lg:pr-8\" style=\"height: auto !important;\">\n",
      "   <div class=\"lg:mb-4 border-b border-gray-200\">\n",
      "    <ul class=\"flex flex-nowrap overflow-x-auto -mb-px text-sm font-medium text-center scrollbar-hide\" id=\"companyTabs\" role=\"tablist\">\n",
      "     <li class=\"mr-2\" role=\"presentation\">\n",
      "      <button aria-controls=\"overvi...\n",
      "  ✅ Processed 201333430N: 0 emails, 0 phones\n",
      "  💤 Sleeping for 33s before next request...\n",
      "\n",
      "🔎 Processing 202532398Z (29/50)\n",
      "  📡 Starting Apify run for 202532398Z (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:BuPkePYegPkgNkjy1]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:BuPkePYegPkgNkjy1]\u001b[0m -> 2025-11-05T08:29:01.165Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:BuPkePYegPkgNkjy1]\u001b[0m -> 2025-11-05T08:29:01.167Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:BuPkePYegPkgNkjy1]\u001b[0m -> 2025-11-05T08:29:01.205Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:BuPkePYegPkgNkjy1]\u001b[0m -> 2025-11-05T08:29:01.341Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:BuPkePYegPkgNkjy1]\u001b[0m -> 2025-11-05T08:29:01.906Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:BuPkePYegPkgNkjy1]\u001b[0m -> 2025-11-05T08:29:02.029Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:BuPkePYegPkgNkjy1]\u001b[0m -> 2025-11-05T08:29:02.666Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:BuPkePYegPkgNkjy1]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:BuPkePYegPkgNkjy1]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:BuPkePYegPkgNkjy1]\u001b[0m -> 2025-11-05T08:29:02.762Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:BuPkePYegPkgNkjy1]\u001b[0m -> 2025-11-05T08:29:14.093Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 202532398Z\n",
      "\u001b[36m[apify.puppeteer-scraper runId:BuPkePYegPkgNkjy1]\u001b[0m -> 2025-11-05T08:29:37.409Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:BuPkePYegPkgNkjy1]\u001b[0m -> 2025-11-05T08:29:38.016Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":33852,\"requestsFinishedPerMinute\":2,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":33852,\"requestsTotal\":1,\"crawlerRuntimeMillis\":35412}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:BuPkePYegPkgNkjy1]\u001b[0m -> 2025-11-05T08:29:38.017Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:BuPkePYegPkgNkjy1]\u001b[0m -> 2025-11-05T08:29:38.043Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:BuPkePYegPkgNkjy1]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 202532398Z (122678 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 18 dt tags\n",
      "  🔎 No phones found yet, searching entire content...\n",
      "  ⚠️ WARNING: No phone numbers found for 202532398Z\n",
      "  📄 Showing first 500 chars of parent HTML for debugging:\n",
      "<div class=\"max-w-7xl mx-auto lg:py-6 sm:px-6 lg:px-8\" style=\"height: auto !important;\">\n",
      " <div class=\"flex flex-col lg:flex-row\" style=\"height: auto !important;\">\n",
      "  <div class=\"w-full lg:w-2/3 lg:pr-8\" style=\"height: auto !important;\">\n",
      "   <div class=\"lg:mb-4 border-b border-gray-200\">\n",
      "    <ul class=\"flex flex-nowrap overflow-x-auto -mb-px text-sm font-medium text-center scrollbar-hide\" id=\"companyTabs\" role=\"tablist\">\n",
      "     <li class=\"mr-2\" role=\"presentation\">\n",
      "      <button aria-controls=\"overvi...\n",
      "  ✅ Processed 202532398Z: 0 emails, 0 phones\n",
      "  💤 Sleeping for 34s before next request...\n",
      "\n",
      "🔎 Processing 52999664B (30/50)\n",
      "  📡 Starting Apify run for 52999664B (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:c95KfliaUx5qlQ7Vc]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:c95KfliaUx5qlQ7Vc]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:c95KfliaUx5qlQ7Vc]\u001b[0m -> 2025-11-05T08:30:28.940Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:c95KfliaUx5qlQ7Vc]\u001b[0m -> 2025-11-05T08:30:28.942Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:c95KfliaUx5qlQ7Vc]\u001b[0m -> 2025-11-05T08:30:28.995Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:c95KfliaUx5qlQ7Vc]\u001b[0m -> 2025-11-05T08:30:29.164Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:c95KfliaUx5qlQ7Vc]\u001b[0m -> 2025-11-05T08:30:30.461Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:c95KfliaUx5qlQ7Vc]\u001b[0m -> 2025-11-05T08:30:30.584Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:c95KfliaUx5qlQ7Vc]\u001b[0m -> 2025-11-05T08:30:31.447Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:c95KfliaUx5qlQ7Vc]\u001b[0m -> 2025-11-05T08:30:31.558Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:c95KfliaUx5qlQ7Vc]\u001b[0m -> 2025-11-05T08:30:37.123Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:c95KfliaUx5qlQ7Vc]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:c95KfliaUx5qlQ7Vc]\u001b[0m -> 2025-11-05T08:30:37.125Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:c95KfliaUx5qlQ7Vc]\u001b[0m -> 2025-11-05T08:30:46.288Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:c95KfliaUx5qlQ7Vc]\u001b[0m -> 2025-11-05T08:30:46.290Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":2}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:c95KfliaUx5qlQ7Vc]\u001b[0m -> 2025-11-05T08:31:04.244Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 52999664B\n",
      "\u001b[36m[apify.puppeteer-scraper runId:c95KfliaUx5qlQ7Vc]\u001b[0m -> 2025-11-05T08:31:30.686Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:c95KfliaUx5qlQ7Vc]\u001b[0m -> 2025-11-05T08:31:31.175Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[null,null,1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":43737,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":43737,\"requestsTotal\":1,\"crawlerRuntimeMillis\":59799}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:c95KfliaUx5qlQ7Vc]\u001b[0m -> 2025-11-05T08:31:31.177Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:c95KfliaUx5qlQ7Vc]\u001b[0m -> 2025-11-05T08:31:31.192Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:c95KfliaUx5qlQ7Vc]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 52999664B (1141787 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 19 dt tags\n",
      "  📝 Field 'contact number': (65) 6425 5785\n",
      "  🔢 Extracted digits: 6564255785\n",
      "  ✅ Added from dt/dd (10 digits): +6564255785\n",
      "  ✅ Total phones found: ['+6564255785']\n",
      "  ✅ Processed 52999664B: 0 emails, 1 phones\n",
      "  💤 Sleeping for 25s before next request...\n",
      "  🛑 Checkpoint pause: waiting extra 30s...\n",
      "\n",
      "🔎 Processing 201601712W (31/50)\n",
      "  📡 Starting Apify run for 201601712W (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:N1tQzt7O2hDHMXYlq]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:N1tQzt7O2hDHMXYlq]\u001b[0m -> 2025-11-05T08:32:41.377Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:N1tQzt7O2hDHMXYlq]\u001b[0m -> 2025-11-05T08:32:41.380Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:N1tQzt7O2hDHMXYlq]\u001b[0m -> 2025-11-05T08:32:41.440Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:N1tQzt7O2hDHMXYlq]\u001b[0m -> 2025-11-05T08:32:41.632Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:N1tQzt7O2hDHMXYlq]\u001b[0m -> 2025-11-05T08:32:42.336Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:N1tQzt7O2hDHMXYlq]\u001b[0m -> 2025-11-05T08:32:42.460Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:N1tQzt7O2hDHMXYlq]\u001b[0m -> 2025-11-05T08:32:43.138Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:N1tQzt7O2hDHMXYlq]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:N1tQzt7O2hDHMXYlq]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:N1tQzt7O2hDHMXYlq]\u001b[0m -> 2025-11-05T08:32:43.225Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:N1tQzt7O2hDHMXYlq]\u001b[0m -> 2025-11-05T08:32:57.071Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 201601712W\n",
      "\u001b[36m[apify.puppeteer-scraper runId:N1tQzt7O2hDHMXYlq]\u001b[0m -> 2025-11-05T08:33:24.159Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:N1tQzt7O2hDHMXYlq]\u001b[0m -> 2025-11-05T08:33:24.597Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":40512,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":40512,\"requestsTotal\":1,\"crawlerRuntimeMillis\":41541}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:N1tQzt7O2hDHMXYlq]\u001b[0m -> 2025-11-05T08:33:24.599Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:N1tQzt7O2hDHMXYlq]\u001b[0m -> 2025-11-05T08:33:24.615Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:N1tQzt7O2hDHMXYlq]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 201601712W (790415 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 20 dt tags\n",
      "  📝 Field 'contact number': 8499 9352\n",
      "  🔢 Extracted digits: 84999352\n",
      "  ✅ Added from dt/dd (8 digits): +6584999352\n",
      "  ✅ Total phones found: ['+6584999352']\n",
      "  ✅ Processed 201601712W: 0 emails, 1 phones\n",
      "  💤 Sleeping for 26s before next request...\n",
      "\n",
      "🔎 Processing 202023231M (32/50)\n",
      "  📡 Starting Apify run for 202023231M (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:oumwFHAabGnbXsft8]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:oumwFHAabGnbXsft8]\u001b[0m -> 2025-11-05T08:34:06.806Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oumwFHAabGnbXsft8]\u001b[0m -> 2025-11-05T08:34:06.808Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oumwFHAabGnbXsft8]\u001b[0m -> 2025-11-05T08:34:06.860Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oumwFHAabGnbXsft8]\u001b[0m -> 2025-11-05T08:34:07.050Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oumwFHAabGnbXsft8]\u001b[0m -> 2025-11-05T08:34:08.374Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oumwFHAabGnbXsft8]\u001b[0m -> 2025-11-05T08:34:08.533Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oumwFHAabGnbXsft8]\u001b[0m -> 2025-11-05T08:34:09.365Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oumwFHAabGnbXsft8]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oumwFHAabGnbXsft8]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oumwFHAabGnbXsft8]\u001b[0m -> 2025-11-05T08:34:09.491Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oumwFHAabGnbXsft8]\u001b[0m -> 2025-11-05T08:34:20.835Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 202023231M\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oumwFHAabGnbXsft8]\u001b[0m -> 2025-11-05T08:34:46.161Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oumwFHAabGnbXsft8]\u001b[0m -> 2025-11-05T08:34:46.606Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":36324,\"requestsFinishedPerMinute\":2,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":36324,\"requestsTotal\":1,\"crawlerRuntimeMillis\":37317}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oumwFHAabGnbXsft8]\u001b[0m -> 2025-11-05T08:34:46.608Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oumwFHAabGnbXsft8]\u001b[0m -> 2025-11-05T08:34:46.626Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:oumwFHAabGnbXsft8]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 202023231M (1472644 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 19 dt tags\n",
      "  🔎 No phones found yet, searching entire content...\n",
      "  ⚠️ WARNING: No phone numbers found for 202023231M\n",
      "  📄 Showing first 500 chars of parent HTML for debugging:\n",
      "<div class=\"max-w-7xl mx-auto lg:py-6 sm:px-6 lg:px-8\" style=\"height: auto !important;\">\n",
      " <div class=\"flex flex-col lg:flex-row\" style=\"height: auto !important;\">\n",
      "  <div class=\"w-full lg:w-2/3 lg:pr-8\" style=\"height: auto !important;\">\n",
      "   <div class=\"lg:mb-4 border-b border-gray-200\">\n",
      "    <ul class=\"flex flex-nowrap overflow-x-auto -mb-px text-sm font-medium text-center scrollbar-hide\" id=\"companyTabs\" role=\"tablist\">\n",
      "     <li class=\"mr-2\" role=\"presentation\">\n",
      "      <button aria-controls=\"overvi...\n",
      "  ✅ Processed 202023231M: 0 emails, 0 phones\n",
      "  💤 Sleeping for 27s before next request...\n",
      "\n",
      "🔎 Processing 200004369C (33/50)\n",
      "  📡 Starting Apify run for 200004369C (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:Op2Lsap6LZLzBme3Y]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:Op2Lsap6LZLzBme3Y]\u001b[0m -> 2025-11-05T08:35:28.961Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Op2Lsap6LZLzBme3Y]\u001b[0m -> 2025-11-05T08:35:28.962Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Op2Lsap6LZLzBme3Y]\u001b[0m -> 2025-11-05T08:35:30.259Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Op2Lsap6LZLzBme3Y]\u001b[0m -> 2025-11-05T08:35:30.424Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Op2Lsap6LZLzBme3Y]\u001b[0m -> 2025-11-05T08:35:31.062Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Op2Lsap6LZLzBme3Y]\u001b[0m -> 2025-11-05T08:35:31.216Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Op2Lsap6LZLzBme3Y]\u001b[0m -> 2025-11-05T08:35:31.755Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Op2Lsap6LZLzBme3Y]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Op2Lsap6LZLzBme3Y]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Op2Lsap6LZLzBme3Y]\u001b[0m -> 2025-11-05T08:35:31.893Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Op2Lsap6LZLzBme3Y]\u001b[0m -> 2025-11-05T08:35:46.373Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 200004369C\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Op2Lsap6LZLzBme3Y]\u001b[0m -> 2025-11-05T08:36:13.893Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Op2Lsap6LZLzBme3Y]\u001b[0m -> 2025-11-05T08:36:14.244Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":41415,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":41415,\"requestsTotal\":1,\"crawlerRuntimeMillis\":42556}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Op2Lsap6LZLzBme3Y]\u001b[0m -> 2025-11-05T08:36:14.245Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Op2Lsap6LZLzBme3Y]\u001b[0m -> Status: RUNNING, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:Op2Lsap6LZLzBme3Y]\u001b[0m -> 2025-11-05T08:36:14.260Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 200004369C (1463941 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 19 dt tags\n",
      "  📝 Field 'contact number': +65 6852 3141\n",
      "  🔢 Extracted digits: 6568523141\n",
      "  ✅ Added from dt/dd (10 digits): +6568523141\n",
      "  ✅ Total phones found: ['+6568523141']\n",
      "  ✅ Processed 200004369C: 0 emails, 1 phones\n",
      "  💤 Sleeping for 28s before next request...\n",
      "\n",
      "🔎 Processing 53062756J (34/50)\n",
      "  📡 Starting Apify run for 53062756J (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:O5cannoigC83pEK5z]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:O5cannoigC83pEK5z]\u001b[0m -> 2025-11-05T08:37:01.748Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:O5cannoigC83pEK5z]\u001b[0m -> 2025-11-05T08:37:01.749Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:O5cannoigC83pEK5z]\u001b[0m -> 2025-11-05T08:37:01.804Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:O5cannoigC83pEK5z]\u001b[0m -> 2025-11-05T08:37:01.997Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:O5cannoigC83pEK5z]\u001b[0m -> 2025-11-05T08:37:03.384Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:O5cannoigC83pEK5z]\u001b[0m -> 2025-11-05T08:37:03.523Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:O5cannoigC83pEK5z]\u001b[0m -> 2025-11-05T08:37:04.550Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:O5cannoigC83pEK5z]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:O5cannoigC83pEK5z]\u001b[0m -> 2025-11-05T08:37:04.920Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:O5cannoigC83pEK5z]\u001b[0m -> 2025-11-05T08:37:12.576Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:O5cannoigC83pEK5z]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:O5cannoigC83pEK5z]\u001b[0m -> 2025-11-05T08:37:12.577Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:O5cannoigC83pEK5z]\u001b[0m -> 2025-11-05T08:37:31.514Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 53062756J\n",
      "\u001b[36m[apify.puppeteer-scraper runId:O5cannoigC83pEK5z]\u001b[0m -> 2025-11-05T08:37:50.110Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:O5cannoigC83pEK5z]\u001b[0m -> 2025-11-05T08:37:50.843Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[null,1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":36644,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":36644,\"requestsTotal\":1,\"crawlerRuntimeMillis\":46378}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:O5cannoigC83pEK5z]\u001b[0m -> 2025-11-05T08:37:50.844Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:O5cannoigC83pEK5z]\u001b[0m -> 2025-11-05T08:37:50.861Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:O5cannoigC83pEK5z]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 53062756J (774638 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 18 dt tags\n",
      "  📝 Field 'contact number': 6677 7767\n",
      "  🔢 Extracted digits: 66777767\n",
      "  ✅ Added from dt/dd (8 digits): +6566777767\n",
      "  ✅ Total phones found: ['+6566777767']\n",
      "  ✅ Processed 53062756J: 0 emails, 1 phones\n",
      "  💤 Sleeping for 29s before next request...\n",
      "\n",
      "🔎 Processing 202349037N (35/50)\n",
      "  📡 Starting Apify run for 202349037N (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:cXHXib8WegcEwTkvp]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:cXHXib8WegcEwTkvp]\u001b[0m -> 2025-11-05T08:38:36.649Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:cXHXib8WegcEwTkvp]\u001b[0m -> 2025-11-05T08:38:36.651Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:cXHXib8WegcEwTkvp]\u001b[0m -> 2025-11-05T08:38:36.892Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:cXHXib8WegcEwTkvp]\u001b[0m -> 2025-11-05T08:38:37.348Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:cXHXib8WegcEwTkvp]\u001b[0m -> 2025-11-05T08:38:38.221Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:cXHXib8WegcEwTkvp]\u001b[0m -> 2025-11-05T08:38:38.698Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:cXHXib8WegcEwTkvp]\u001b[0m -> 2025-11-05T08:38:39.799Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:cXHXib8WegcEwTkvp]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:cXHXib8WegcEwTkvp]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:cXHXib8WegcEwTkvp]\u001b[0m -> 2025-11-05T08:38:40.088Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:cXHXib8WegcEwTkvp]\u001b[0m -> 2025-11-05T08:38:54.282Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 202349037N\n",
      "\u001b[36m[apify.puppeteer-scraper runId:cXHXib8WegcEwTkvp]\u001b[0m -> 2025-11-05T08:39:05.684Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:cXHXib8WegcEwTkvp]\u001b[0m -> 2025-11-05T08:39:06.541Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":24458,\"requestsFinishedPerMinute\":2,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":24458,\"requestsTotal\":1,\"crawlerRuntimeMillis\":26815}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:cXHXib8WegcEwTkvp]\u001b[0m -> 2025-11-05T08:39:06.544Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:cXHXib8WegcEwTkvp]\u001b[0m -> Status: RUNNING, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:cXHXib8WegcEwTkvp]\u001b[0m -> 2025-11-05T08:39:06.568Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 202349037N (1446997 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 17 dt tags\n",
      "  🔎 No phones found yet, searching entire content...\n",
      "  ⚠️ WARNING: No phone numbers found for 202349037N\n",
      "  📄 Showing first 500 chars of parent HTML for debugging:\n",
      "<div class=\"max-w-7xl mx-auto lg:py-6 sm:px-6 lg:px-8\" style=\"height: auto !important;\">\n",
      " <div class=\"flex flex-col lg:flex-row\" style=\"height: auto !important;\">\n",
      "  <div class=\"w-full lg:w-2/3 lg:pr-8\" style=\"height: auto !important;\">\n",
      "   <div class=\"lg:mb-4 border-b border-gray-200\">\n",
      "    <ul class=\"flex flex-nowrap overflow-x-auto -mb-px text-sm font-medium text-center scrollbar-hide\" id=\"companyTabs\" role=\"tablist\">\n",
      "     <li class=\"mr-2\" role=\"presentation\">\n",
      "      <button aria-controls=\"overvi...\n",
      "  ✅ Processed 202349037N: 0 emails, 0 phones\n",
      "  💤 Sleeping for 30s before next request...\n",
      "  🛑 Checkpoint pause: waiting extra 30s...\n",
      "\n",
      "🔎 Processing 202240464G (36/50)\n",
      "  📡 Starting Apify run for 202240464G (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:qf4COHvGZduOiB68T]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:qf4COHvGZduOiB68T]\u001b[0m -> 2025-11-05T08:40:23.579Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qf4COHvGZduOiB68T]\u001b[0m -> 2025-11-05T08:40:23.581Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qf4COHvGZduOiB68T]\u001b[0m -> 2025-11-05T08:40:23.652Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qf4COHvGZduOiB68T]\u001b[0m -> 2025-11-05T08:40:23.867Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qf4COHvGZduOiB68T]\u001b[0m -> 2025-11-05T08:40:25.240Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qf4COHvGZduOiB68T]\u001b[0m -> 2025-11-05T08:40:25.378Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qf4COHvGZduOiB68T]\u001b[0m -> 2025-11-05T08:40:26.238Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qf4COHvGZduOiB68T]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qf4COHvGZduOiB68T]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qf4COHvGZduOiB68T]\u001b[0m -> 2025-11-05T08:40:26.352Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qf4COHvGZduOiB68T]\u001b[0m -> 2025-11-05T08:41:09.146Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 202240464G\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qf4COHvGZduOiB68T]\u001b[0m -> 2025-11-05T08:41:26.352Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":60189,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qf4COHvGZduOiB68T]\u001b[0m -> 2025-11-05T08:41:26.375Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qf4COHvGZduOiB68T]\u001b[0m -> 2025-11-05T08:42:26.353Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":120189,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qf4COHvGZduOiB68T]\u001b[0m -> 2025-11-05T08:42:26.377Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qf4COHvGZduOiB68T]\u001b[0m -> 2025-11-05T08:43:04.302Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qf4COHvGZduOiB68T]\u001b[0m -> 2025-11-05T08:43:04.649Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":157599,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":157599,\"requestsTotal\":1,\"crawlerRuntimeMillis\":158486}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qf4COHvGZduOiB68T]\u001b[0m -> 2025-11-05T08:43:04.651Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qf4COHvGZduOiB68T]\u001b[0m -> 2025-11-05T08:43:04.665Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:qf4COHvGZduOiB68T]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 202240464G (154260 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 25 dt tags\n",
      "  📝 Field 'contact number': +65 9852 2583\n",
      "  🔢 Extracted digits: 6598522583\n",
      "  ✅ Added from dt/dd (10 digits): +6598522583\n",
      "  ✅ Total phones found: ['+6598522583']\n",
      "  ✅ Processed 202240464G: 1 emails, 1 phones\n",
      "  💤 Sleeping for 31s before next request...\n",
      "\n",
      "🔎 Processing 00388100L (37/50)\n",
      "  📡 Starting Apify run for 00388100L (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:PUKyYeVs1UcDAiQVZ]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:PUKyYeVs1UcDAiQVZ]\u001b[0m -> 2025-11-05T08:43:50.963Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:PUKyYeVs1UcDAiQVZ]\u001b[0m -> 2025-11-05T08:43:50.965Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:PUKyYeVs1UcDAiQVZ]\u001b[0m -> 2025-11-05T08:43:51.127Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:PUKyYeVs1UcDAiQVZ]\u001b[0m -> 2025-11-05T08:43:51.393Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:PUKyYeVs1UcDAiQVZ]\u001b[0m -> 2025-11-05T08:43:52.079Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:PUKyYeVs1UcDAiQVZ]\u001b[0m -> 2025-11-05T08:43:52.254Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:PUKyYeVs1UcDAiQVZ]\u001b[0m -> 2025-11-05T08:43:52.898Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:PUKyYeVs1UcDAiQVZ]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:PUKyYeVs1UcDAiQVZ]\u001b[0m -> 2025-11-05T08:43:53.000Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:PUKyYeVs1UcDAiQVZ]\u001b[0m -> 2025-11-05T08:43:59.031Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. Request blocked - received 403 status code.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:PUKyYeVs1UcDAiQVZ]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:PUKyYeVs1UcDAiQVZ]\u001b[0m -> 2025-11-05T08:43:59.032Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:PUKyYeVs1UcDAiQVZ]\u001b[0m -> 2025-11-05T08:44:49.535Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 00388100L\n",
      "\u001b[36m[apify.puppeteer-scraper runId:PUKyYeVs1UcDAiQVZ]\u001b[0m -> 2025-11-05T08:44:53.001Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":60169,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:PUKyYeVs1UcDAiQVZ]\u001b[0m -> 2025-11-05T08:44:53.025Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:PUKyYeVs1UcDAiQVZ]\u001b[0m -> 2025-11-05T08:45:38.848Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m No company links found, might be not found\n",
      "\u001b[36m[apify.puppeteer-scraper runId:PUKyYeVs1UcDAiQVZ]\u001b[0m -> 2025-11-05T08:45:39.018Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:PUKyYeVs1UcDAiQVZ]\u001b[0m -> 2025-11-05T08:45:39.411Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[null,1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":99692,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":99692,\"requestsTotal\":1,\"crawlerRuntimeMillis\":106579}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:PUKyYeVs1UcDAiQVZ]\u001b[0m -> 2025-11-05T08:45:39.411Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:PUKyYeVs1UcDAiQVZ]\u001b[0m -> 2025-11-05T08:45:39.423Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:PUKyYeVs1UcDAiQVZ]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ⚠️ Company not found for UEN 00388100L\n",
      "  ❌ Company not found on RecordOwl\n",
      "\n",
      "🔎 Processing 53510505C (38/50)\n",
      "  📡 Starting Apify run for 53510505C (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:7HqQE2SjmYgNzoGfH]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:7HqQE2SjmYgNzoGfH]\u001b[0m -> 2025-11-05T08:45:59.398Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:7HqQE2SjmYgNzoGfH]\u001b[0m -> 2025-11-05T08:45:59.400Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:7HqQE2SjmYgNzoGfH]\u001b[0m -> 2025-11-05T08:45:59.454Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:7HqQE2SjmYgNzoGfH]\u001b[0m -> 2025-11-05T08:45:59.641Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:7HqQE2SjmYgNzoGfH]\u001b[0m -> 2025-11-05T08:46:00.325Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:7HqQE2SjmYgNzoGfH]\u001b[0m -> 2025-11-05T08:46:00.512Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:7HqQE2SjmYgNzoGfH]\u001b[0m -> 2025-11-05T08:46:01.181Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:7HqQE2SjmYgNzoGfH]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:7HqQE2SjmYgNzoGfH]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:7HqQE2SjmYgNzoGfH]\u001b[0m -> 2025-11-05T08:46:01.283Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:7HqQE2SjmYgNzoGfH]\u001b[0m -> 2025-11-05T08:46:12.975Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 53510505C\n",
      "\u001b[36m[apify.puppeteer-scraper runId:7HqQE2SjmYgNzoGfH]\u001b[0m -> 2025-11-05T08:47:01.282Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":60179,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:7HqQE2SjmYgNzoGfH]\u001b[0m -> 2025-11-05T08:47:01.399Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:7HqQE2SjmYgNzoGfH]\u001b[0m -> 2025-11-05T08:47:11.264Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m No company links found, might be not found\n",
      "\u001b[36m[apify.puppeteer-scraper runId:7HqQE2SjmYgNzoGfH]\u001b[0m -> 2025-11-05T08:47:11.605Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:7HqQE2SjmYgNzoGfH]\u001b[0m -> 2025-11-05T08:47:12.019Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":69660,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":69660,\"requestsTotal\":1,\"crawlerRuntimeMillis\":70915}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:7HqQE2SjmYgNzoGfH]\u001b[0m -> 2025-11-05T08:47:12.020Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:7HqQE2SjmYgNzoGfH]\u001b[0m -> Status: RUNNING, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:7HqQE2SjmYgNzoGfH]\u001b[0m -> 2025-11-05T08:47:12.031Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ⚠️ Company not found for UEN 53510505C\n",
      "  ❌ Company not found on RecordOwl\n",
      "\n",
      "🔎 Processing 201303230K (39/50)\n",
      "  📡 Starting Apify run for 201303230K (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:aO4tjZQCvUTWf2E8A]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.puppeteer-scraper runId:aO4tjZQCvUTWf2E8A]\u001b[0m -> 2025-11-05T08:47:32.206Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aO4tjZQCvUTWf2E8A]\u001b[0m -> 2025-11-05T08:47:32.208Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aO4tjZQCvUTWf2E8A]\u001b[0m -> 2025-11-05T08:47:32.286Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aO4tjZQCvUTWf2E8A]\u001b[0m -> 2025-11-05T08:47:32.483Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aO4tjZQCvUTWf2E8A]\u001b[0m -> 2025-11-05T08:47:34.068Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aO4tjZQCvUTWf2E8A]\u001b[0m -> 2025-11-05T08:47:34.195Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aO4tjZQCvUTWf2E8A]\u001b[0m -> 2025-11-05T08:47:35.018Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aO4tjZQCvUTWf2E8A]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aO4tjZQCvUTWf2E8A]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aO4tjZQCvUTWf2E8A]\u001b[0m -> 2025-11-05T08:47:35.094Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aO4tjZQCvUTWf2E8A]\u001b[0m -> 2025-11-05T08:48:05.772Z \u001b[33mWARN\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Reclaiming failed request back to the list or queue. net::ERR_TIMED_OUT at https://recordowl.com/\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aO4tjZQCvUTWf2E8A]\u001b[0m -> 2025-11-05T08:48:05.774Z \u001b[90m {\"id\":\"XsOj8IqwoXyWgRx\",\"url\":\"https://recordowl.com/\",\"retryCount\":1}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aO4tjZQCvUTWf2E8A]\u001b[0m -> 2025-11-05T08:48:33.546Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 201303230K\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aO4tjZQCvUTWf2E8A]\u001b[0m -> 2025-11-05T08:48:35.095Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:Statistics:\u001b[39m PuppeteerCrawler request statistics:\u001b[90m {\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":null,\"requestsFinishedPerMinute\":0,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":0,\"requestsTotal\":0,\"crawlerRuntimeMillis\":60157,\"retryHistogram\":[]}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aO4tjZQCvUTWf2E8A]\u001b[0m -> 2025-11-05T08:48:35.116Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:AutoscaledPool:\u001b[39m state\u001b[90m {\"currentConcurrency\":1,\"desiredConcurrency\":1,\"systemStatus\":{\"isSystemIdle\":true,\"memInfo\":{\"isOverloaded\":false,\"limitRatio\":0.2,\"actualRatio\":0},\"eventLoopInfo\":{\"isOverloaded\":false,\"limitRatio\":0.6,\"actualRatio\":0},\"cpuInfo\":{\"isOverloaded\":false,\"limitRatio\":0.4,\"actualRatio\":0.036},\"clientInfo\":{\"isOverloaded\":false,\"limitRatio\":0.3,\"actualRatio\":0}}}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aO4tjZQCvUTWf2E8A]\u001b[0m -> 2025-11-05T08:49:23.146Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aO4tjZQCvUTWf2E8A]\u001b[0m -> 2025-11-05T08:49:23.547Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[null,1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":77000,\"requestsFinishedPerMinute\":1,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":77000,\"requestsTotal\":1,\"crawlerRuntimeMillis\":108609}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aO4tjZQCvUTWf2E8A]\u001b[0m -> 2025-11-05T08:49:23.549Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aO4tjZQCvUTWf2E8A]\u001b[0m -> 2025-11-05T08:49:23.560Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:aO4tjZQCvUTWf2E8A]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 201303230K (1478230 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 19 dt tags\n",
      "  📝 Field 'contact number': 6702 3707\n",
      "  🔢 Extracted digits: 67023707\n",
      "  ✅ Added from dt/dd (8 digits): +6567023707\n",
      "  ✅ Total phones found: ['+6567023707']\n",
      "  ✅ Processed 201303230K: 0 emails, 1 phones\n",
      "  💤 Sleeping for 34s before next request...\n",
      "\n",
      "🔎 Processing 53409897E (40/50)\n",
      "  📡 Starting Apify run for 53409897E (attempt 1/5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.puppeteer-scraper runId:AF9V09TZ5K2GQ4l2e]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:AF9V09TZ5K2GQ4l2e]\u001b[0m -> 2025-11-05T08:50:14.467Z ACTOR: Pulling container image of build g6G5r98rF5fM6ecm3 from registry.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:AF9V09TZ5K2GQ4l2e]\u001b[0m -> 2025-11-05T08:50:14.470Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:AF9V09TZ5K2GQ4l2e]\u001b[0m -> 2025-11-05T08:50:14.523Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:AF9V09TZ5K2GQ4l2e]\u001b[0m -> 2025-11-05T08:50:14.730Z Will run command: xvfb-run -a -s \"-ac -screen 0 1920x1080x24+32 -nolisten tcp\" /bin/sh -c ./start_xvfb_and_run_cmd.sh && npm run start:prod --silent\n",
      "\u001b[36m[apify.puppeteer-scraper runId:AF9V09TZ5K2GQ4l2e]\u001b[0m -> 2025-11-05T08:50:15.402Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.4\",\"apifyClientVersion\":\"2.16.0\",\"crawleeVersion\":\"3.14.1\",\"osType\":\"Linux\",\"nodeVersion\":\"v22.19.0\"}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:AF9V09TZ5K2GQ4l2e]\u001b[0m -> 2025-11-05T08:50:15.527Z \u001b[32mINFO\u001b[39m  Configuring Puppeteer Scraper.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:AF9V09TZ5K2GQ4l2e]\u001b[0m -> 2025-11-05T08:50:16.102Z \u001b[32mINFO\u001b[39m  Configuration completed. Starting the scrape.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:AF9V09TZ5K2GQ4l2e]\u001b[0m -> Status: RUNNING, Message: Crawled 0/1 pages, 0 failed requests, desired concurrency 1.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:AF9V09TZ5K2GQ4l2e]\u001b[0m -> 2025-11-05T08:50:16.227Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:AF9V09TZ5K2GQ4l2e]\u001b[0m -> 2025-11-05T08:50:34.035Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Visiting RecordOwl for UEN: 53409897E\n",
      "\u001b[36m[apify.puppeteer-scraper runId:AF9V09TZ5K2GQ4l2e]\u001b[0m -> 2025-11-05T08:50:45.240Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:AF9V09TZ5K2GQ4l2e]\u001b[0m -> 2025-11-05T08:50:45.609Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":28728,\"requestsFinishedPerMinute\":2,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":28728,\"requestsTotal\":1,\"crawlerRuntimeMillis\":29573}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:AF9V09TZ5K2GQ4l2e]\u001b[0m -> 2025-11-05T08:50:45.611Z \u001b[32mINFO\u001b[39m \u001b[33m PuppeteerCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.puppeteer-scraper runId:AF9V09TZ5K2GQ4l2e]\u001b[0m -> 2025-11-05T08:50:45.637Z \u001b[32mINFO\u001b[39m  Puppeteer Scraper finished.\n",
      "\u001b[36m[apify.puppeteer-scraper runId:AF9V09TZ5K2GQ4l2e]\u001b[0m -> Status: SUCCEEDED, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ⏳ Waiting for run to complete...\n",
      "  ✅ Run succeeded with data\n",
      "  ⏳ Waiting for dataset to be ready...\n",
      "  📊 Dataset has 1 item(s)\n",
      "  ✅ Successfully scraped 53409897E (1456995 chars of HTML)\n",
      "  🔍 Searching for phone numbers...\n",
      "  📋 Found 17 dt tags\n",
      "  🔎 No phones found yet, searching entire content...\n",
      "  ⚠️ WARNING: No phone numbers found for 53409897E\n",
      "  📄 Showing first 500 chars of parent HTML for debugging:\n",
      "<div class=\"max-w-7xl mx-auto lg:py-6 sm:px-6 lg:px-8\" style=\"height: auto !important;\">\n",
      " <div class=\"flex flex-col lg:flex-row\" style=\"height: auto !important;\">\n",
      "  <div class=\"w-full lg:w-2/3 lg:pr-8\" style=\"height: auto !important;\">\n",
      "   <div class=\"lg:mb-4 border-b border-gray-200\">\n",
      "    <ul class=\"flex flex-nowrap overflow-x-auto -mb-px text-sm font-medium text-center scrollbar-hide\" id=\"companyTabs\" role=\"tablist\">\n",
      "     <li class=\"mr-2\" role=\"presentation\">\n",
      "      <button aria-controls=\"overvi...\n",
      "  ✅ Processed 53409897E: 0 emails, 0 phones\n",
      "  💤 Sleeping for 25s before next request...\n",
      "  🛑 Checkpoint pause: waiting extra 30s...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 586\u001b[39m\n\u001b[32m    584\u001b[39m         extra_wait = \u001b[32m30\u001b[39m\n\u001b[32m    585\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  🛑 Checkpoint pause: waiting extra \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextra_wait\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m586\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextra_wait\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    588\u001b[39m New_Fresh_Leads = pd.DataFrame(all_results)\n\u001b[32m    589\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Scraping complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "client = ApifyClient(\"apify_api_yNR85etaHpLtBzPoVozVVXUsCZe54u2Ffog1\")\n",
    "\n",
    "SOCIAL_MEDIA_DOMAINS = [\n",
    "    \"facebook.com\", \"linkedin.com\", \"instagram.com\", \"youtube.com\",\n",
    "    \"tiktok.com\", \"twitter.com\", \"x.com\", \"pinterest.com\"\n",
    "]\n",
    "\n",
    "def fetch_dataset_items_safe(dataset_client, max_retries=5, initial_wait=3):\n",
    "    \"\"\"Safely fetch dataset items with multiple retry strategies.\"\"\"\n",
    "    dataset_items = []\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Strategy 1: Try using iterate_items() (streaming)\n",
    "            try:\n",
    "                dataset_items = list(dataset_client.iterate_items())\n",
    "                if dataset_items:\n",
    "                    return dataset_items\n",
    "            except (HTTPError, ConnectionError, ProtocolError, Exception) as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = initial_wait * (2 ** attempt)  # Exponential backoff\n",
    "                    print(f\"  ⚠️ Iteration method failed (attempt {attempt + 1}/{max_retries}), trying direct fetch in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"  ⚠️ Iteration method failed after all retries, trying direct fetch...\")\n",
    "            \n",
    "            # Strategy 2: Try using list_items() (direct pagination)\n",
    "            try:\n",
    "                offset = 0\n",
    "                limit = 100\n",
    "                while True:\n",
    "                    page = dataset_client.list_items(offset=offset, limit=limit, clean=True)\n",
    "                    if not page.items:\n",
    "                        break\n",
    "                    dataset_items.extend(page.items)\n",
    "                    if len(page.items) < limit:\n",
    "                        break\n",
    "                    offset += limit\n",
    "                \n",
    "                if dataset_items:\n",
    "                    return dataset_items\n",
    "            except (HTTPError, ConnectionError, ProtocolError, Exception) as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = initial_wait * (2 ** attempt)\n",
    "                    print(f\"  ⚠️ Direct fetch failed (attempt {attempt + 1}/{max_retries}), retrying in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"  ❌ All fetch methods failed: {e}\")\n",
    "                    return []\n",
    "                    \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = initial_wait * (2 ** attempt)\n",
    "                print(f\"  ⚠️ Unexpected error (attempt {attempt + 1}/{max_retries}), retrying in {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"  ❌ Failed after all retries: {e}\")\n",
    "                return []\n",
    "    \n",
    "    return dataset_items\n",
    "\n",
    "def run_apify_with_retry(client, run_input, uen, max_retries=3):\n",
    "    \"\"\"Run Apify with exponential backoff on 403 errors AND verify dataset has items.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"  📡 Starting Apify run for {uen} (attempt {attempt + 1}/{max_retries})...\")\n",
    "            run = client.actor(\"apify/puppeteer-scraper\").call(run_input=run_input)\n",
    "            \n",
    "            print(f\"  ⏳ Waiting for run to complete...\")\n",
    "            run_client = client.run(run[\"id\"])\n",
    "            run_info = run_client.wait_for_finish()\n",
    "            \n",
    "            # CRITICAL FIX: Check if run actually scraped pages, not just if it \"succeeded\"\n",
    "            if run_info and \"status\" in run_info:\n",
    "                status = run_info.get(\"status\")\n",
    "                \n",
    "                # Even if status is \"SUCCEEDED\", verify dataset actually has items\n",
    "                if status == \"SUCCEEDED\" and \"defaultDatasetId\" in run:\n",
    "                    # Quick check if dataset has any items\n",
    "                    try:\n",
    "                        dataset_check = client.dataset(run[\"defaultDatasetId\"])\n",
    "                        time.sleep(2)  # Brief wait for dataset to be ready\n",
    "                        test_items = dataset_check.list_items(limit=1, clean=True)\n",
    "                        \n",
    "                        if test_items.items and len(test_items.items) > 0:\n",
    "                            # Dataset has items - true success!\n",
    "                            print(f\"  ✅ Run succeeded with data\")\n",
    "                            return run, None\n",
    "                        else:\n",
    "                            # Status says \"SUCCEEDED\" but dataset is EMPTY - this is a failure!\n",
    "                            print(f\"  ⚠️ Run completed but dataset is empty (likely 403 block)\")\n",
    "                            # Treat as 403 and retry\n",
    "                            if attempt < max_retries - 1:\n",
    "                                wait_time = 30 * (2 ** attempt)\n",
    "                                print(f\"  🔄 Retrying in {wait_time}s...\")\n",
    "                                time.sleep(wait_time)\n",
    "                                continue\n",
    "                            else:\n",
    "                                return None, \"Dataset empty after all retries (403 blocking)\"\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ⚠️ Could not verify dataset: {e}\")\n",
    "                        # If we can't check dataset, try to use the run anyway\n",
    "                        return run, None\n",
    "                \n",
    "                elif status != \"SUCCEEDED\":\n",
    "                    # Check error message for 403\n",
    "                    error_msg = str(run_info)\n",
    "                    if \"403\" in error_msg or \"blocked\" in error_msg.lower():\n",
    "                        if attempt < max_retries - 1:\n",
    "                            wait_time = 30 * (2 ** attempt)  # 30s, 60s, 120s\n",
    "                            print(f\"  🚫 Request blocked (403), waiting {wait_time}s before retry...\")\n",
    "                            time.sleep(wait_time)\n",
    "                            continue\n",
    "            \n",
    "            return run, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_str = str(e)\n",
    "            if \"403\" in error_str or \"blocked\" in error_str.lower():\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait_time = 30 * (2 ** attempt)\n",
    "                    print(f\"  🚫 Request blocked (403), waiting {wait_time}s before retry...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "            return None, f\"Apify call failed: {str(e)}\"\n",
    "    \n",
    "    return None, \"Max retries exceeded due to 403 blocking\"\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for idx, (i, row) in enumerate(acra_data_filtered_wholesale.iterrows(), 1):\n",
    "    uen = str(row[\"UEN\"]).strip()\n",
    "    print(f\"\\n🔎 Processing {uen} ({idx}/{len(acra_data_filtered_wholesale)})\")\n",
    "\n",
    "    # Build pageFunction with proper escaping\n",
    "    page_function = f\"\"\"\n",
    "    async function pageFunction(context) {{\n",
    "        const {{ page, log, request }} = context;\n",
    "        const uen = \"{uen}\";\n",
    "        log.info(\"Visiting RecordOwl for UEN: \" + uen);\n",
    "\n",
    "        try {{\n",
    "            await page.waitForSelector(\"input[placeholder='Search company name, industry, or address']\", {{ timeout: 30000 }});\n",
    "            const input = await page.$(\"input[placeholder='Search company name, industry, or address']\");\n",
    "            await input.click({{ clickCount: 3 }});\n",
    "            await input.type(uen, {{ delay: 100 }});\n",
    "\n",
    "            await Promise.all([\n",
    "                page.waitForNavigation({{ waitUntil: 'networkidle2', timeout: 60000 }}).catch(() => null),\n",
    "                page.click(\"button[type='submit']\")\n",
    "            ]);\n",
    "\n",
    "            // Wait for results with longer timeout\n",
    "            try {{\n",
    "                await page.waitForSelector(\"a[href*='/company/']\", {{ timeout: 45000 }});\n",
    "            }} catch (e) {{\n",
    "                log.info(\"No company links found, might be not found\");\n",
    "                return {{ status: 'not_found', uen }};\n",
    "            }}\n",
    "\n",
    "            const companyLink = await page.$$eval(\"a[href*='/company/']\", (links, uen) => {{\n",
    "                for (const a of links) {{\n",
    "                    const text = a.innerText || \"\";\n",
    "                    const href = a.href || \"\";\n",
    "                    if (text.includes(uen) || href.includes(uen.toLowerCase())) return a.href;\n",
    "                }}\n",
    "                return links.length > 0 ? links[0].href : null;\n",
    "            }}, uen);\n",
    "\n",
    "            if (!companyLink) return {{ status: 'not_found', uen }};\n",
    "\n",
    "            if (page.url() !== companyLink) {{\n",
    "                await page.goto(companyLink, {{ waitUntil: 'networkidle2', timeout: 60000 }});\n",
    "            }}\n",
    "\n",
    "            // Wait for critical content to load - phone numbers are often in dt/dd tags\n",
    "            await Promise.race([\n",
    "                page.waitForSelector('dt', {{ timeout: 10000 }}).catch(() => null),\n",
    "                new Promise(r => setTimeout(r, 8000)) // Increased from 3s to 8s\n",
    "            ]);\n",
    "            \n",
    "            // Additional wait to ensure all dynamic content loads\n",
    "            await new Promise(r => setTimeout(r, 5000));\n",
    "            \n",
    "            const html_content = await page.content();\n",
    "            const title = await page.title();\n",
    "            const url = page.url();\n",
    "\n",
    "            return {{ status: 'success', uen, url, title, html_content }};\n",
    "        }} catch (err) {{\n",
    "            log.error(\"Error in pageFunction: \" + err.message);\n",
    "            return {{ status: 'error', uen, error: err.message }};\n",
    "        }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    run_input = {\n",
    "        \"startUrls\": [{\"url\": \"https://recordowl.com/\"}],\n",
    "        \"useChrome\": True,\n",
    "        \"headless\": True,\n",
    "        \"stealth\": True,\n",
    "        \"pageFunction\": page_function,\n",
    "        \"ignoreSslErrors\": False,\n",
    "        \"ignoreCorsAndCsp\": False,\n",
    "        \"maxRequestRetries\": 3,  # Increased retry attempts\n",
    "        \"maxRequestsPerCrawl\": 1,  # One page per run\n",
    "        \"maxConcurrency\": 1,  # No parallel requests\n",
    "        \"pageLoadTimeoutSecs\": 90,  # Optimized timeout\n",
    "        \"pageFunctionTimeoutSecs\": 180,  # 3 minutes for pageFunction\n",
    "        \"waitUntil\": [\"networkidle2\"],  # Wait for network to be idle\n",
    "        # OPTIMIZED: Residential proxies with recommended rotation\n",
    "        \"proxyConfiguration\": {\n",
    "            \"useApifyProxy\": True,\n",
    "            \"apifyProxyGroups\": [\"RESIDENTIAL\"],  # Residential IPs less likely to be blocked\n",
    "        },\n",
    "        \"proxyRotation\": \"RECOMMENDED\",  # Optimal proxy rotation strategy\n",
    "    }\n",
    "\n",
    "    # Use retry logic for 403 errors (5 attempts = more chances to recover)\n",
    "    run, error = run_apify_with_retry(client, run_input, uen, max_retries=5)\n",
    "\n",
    "    if error or not run:\n",
    "        print(f\"  ❌ Apify call failed for {uen}: {error}\")\n",
    "        all_results.append({\n",
    "            \"UEN\": uen,\n",
    "            \"Emails\": None,\n",
    "            \"Phones\": None,\n",
    "            \"Website\": None,\n",
    "            \"Facebook\": None,\n",
    "            \"LinkedIn\": None,\n",
    "            \"Instagram\": None,\n",
    "            \"TikTok\": None,\n",
    "            \"RecordOwl_Link\": None,\n",
    "            \"Error\": error or \"No run returned\"\n",
    "        })\n",
    "        time.sleep(10)  # Longer sleep after failure\n",
    "        continue\n",
    "\n",
    "    if not run or \"defaultDatasetId\" not in run:\n",
    "        print(f\"  ⚠️ No valid dataset returned for {uen}\")\n",
    "        all_results.append({\n",
    "            \"UEN\": uen,\n",
    "            \"Emails\": None,\n",
    "            \"Phones\": None,\n",
    "            \"Website\": None,\n",
    "            \"Facebook\": None,\n",
    "            \"LinkedIn\": None,\n",
    "            \"Instagram\": None,\n",
    "            \"TikTok\": None,\n",
    "            \"RecordOwl_Link\": None,\n",
    "            \"Error\": \"No dataset returned\"\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # Wait for dataset to be ready with progressive checking\n",
    "    print(f\"  ⏳ Waiting for dataset to be ready...\")\n",
    "    time.sleep(5)  # Initial wait\n",
    "    \n",
    "    # Try to fetch dataset with progressive waits\n",
    "    dataset_client = client.dataset(run[\"defaultDatasetId\"])\n",
    "    for check_attempt in range(3):\n",
    "        try:\n",
    "            # Quick check if dataset has items\n",
    "            test_fetch = dataset_client.list_items(limit=1, clean=True)\n",
    "            if test_fetch.items:\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if check_attempt < 2:\n",
    "            additional_wait = 3 * (check_attempt + 1)\n",
    "            print(f\"  ⏳ Dataset not ready, waiting {additional_wait}s more...\")\n",
    "            time.sleep(additional_wait)\n",
    "    \n",
    "    scraped_html, record_owl_url = None, None\n",
    "    \n",
    "    # Fetch dataset items with improved error handling\n",
    "    dataset_items = fetch_dataset_items_safe(\n",
    "        dataset_client,\n",
    "        max_retries=5,\n",
    "        initial_wait=5  # Increased from 3 to 5\n",
    "    )\n",
    "    \n",
    "    # Process items\n",
    "    if not dataset_items:\n",
    "        print(f\"  ⚠️ Dataset is empty - no items returned!\")\n",
    "    else:\n",
    "        print(f\"  📊 Dataset has {len(dataset_items)} item(s)\")\n",
    "    \n",
    "    for item in dataset_items:\n",
    "        if item.get(\"status\") == \"success\":\n",
    "            scraped_html = item.get(\"html_content\", \"\")\n",
    "            record_owl_url = item.get(\"url\")\n",
    "            if scraped_html:\n",
    "                print(f\"  ✅ Successfully scraped {uen} ({len(scraped_html)} chars of HTML)\")\n",
    "            else:\n",
    "                print(f\"  ⚠️ Status is 'success' but html_content is empty for {uen}\")\n",
    "        elif item.get(\"status\") == \"not_found\":\n",
    "            print(f\"  ⚠️ Company not found for UEN {uen}\")\n",
    "        elif item.get(\"status\") == \"error\":\n",
    "            print(f\"  ❌ Error for {uen}: {item.get('error')}\")\n",
    "        else:\n",
    "            print(f\"  ⚠️ Unknown item status for {uen}: {item.get('status')}\")\n",
    "            print(f\"  📋 Item keys: {list(item.keys())}\")\n",
    "\n",
    "    if not scraped_html:\n",
    "        # Determine the specific reason for failure\n",
    "        if not dataset_items:\n",
    "            error_reason = \"Dataset empty (likely 403 block at Apify level)\"\n",
    "            print(f\"  ❌ {error_reason}\")\n",
    "        elif any(item.get(\"status\") == \"not_found\" for item in dataset_items):\n",
    "            error_reason = \"Company not found on RecordOwl\"\n",
    "            print(f\"  ❌ {error_reason}\")\n",
    "        elif any(item.get(\"status\") == \"error\" for item in dataset_items):\n",
    "            error_details = [item.get(\"error\", \"Unknown\") for item in dataset_items if item.get(\"status\") == \"error\"]\n",
    "            error_reason = f\"Scraping error: {error_details[0] if error_details else 'Unknown'}\"\n",
    "            print(f\"  ❌ {error_reason}\")\n",
    "        else:\n",
    "            error_reason = \"No HTML content retrieved (unknown reason)\"\n",
    "            print(f\"  ⚠️ {error_reason}\")\n",
    "            # Debug: show what's in dataset items\n",
    "            if dataset_items:\n",
    "                print(f\"  🔍 DEBUG - First item: {dataset_items[0]}\")\n",
    "        \n",
    "        all_results.append({\n",
    "            \"UEN\": uen,\n",
    "            \"Emails\": None,\n",
    "            \"Phones\": None,\n",
    "            \"Website\": None,\n",
    "            \"Facebook\": None,\n",
    "            \"LinkedIn\": None,\n",
    "            \"Instagram\": None,\n",
    "            \"TikTok\": None,\n",
    "            \"RecordOwl_Link\": record_owl_url or None,\n",
    "            \"Error\": error_reason\n",
    "        })\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "\n",
    "    # Parse HTML\n",
    "    try:\n",
    "        soup = BeautifulSoup(scraped_html, \"html.parser\")\n",
    "        parent = soup.select_one(\"div.max-w-7xl.mx-auto.lg\\\\:py-6.sm\\\\:px-6.lg\\\\:px-8\")\n",
    "\n",
    "        emails, phones, website = [], [], None\n",
    "        facebook_links, linkedin_links, instagram_links, tiktok_links = [], [], [], []\n",
    "\n",
    "        if parent:\n",
    "            # Extract emails\n",
    "            for a in parent.select(\"a[href^=mailto]\"):\n",
    "                email = a.get(\"href\", \"\").replace(\"mailto:\", \"\").strip()\n",
    "                if email and email not in emails and \"@\" in email:\n",
    "                    emails.append(email)\n",
    "\n",
    "            # ========== COMPREHENSIVE PHONE EXTRACTION ==========\n",
    "            # This extracts Singapore phone numbers with ANY spacing/formatting:\n",
    "            # - \"65 63 19 2960\" (spaces between digits)\n",
    "            # - \"6563192960\" (no spaces)\n",
    "            # - \"+65-6319-2960\" (dashes)\n",
    "            # - \"65 6 3 1 9 2 9 6 0\" (space between every digit)\n",
    "            # - \"(65) 6319 2960\" (with parentheses)\n",
    "            # Method: Extract ALL digits first, then validate pattern\n",
    "            print(f\"  🔍 Searching for phone numbers...\")\n",
    "            \n",
    "            # Method 1: Look for tel: links (most reliable)\n",
    "            tel_links = parent.select(\"a[href^='tel:'], a[href^='tel']\")\n",
    "            if tel_links:\n",
    "                print(f\"  📱 Found {len(tel_links)} tel: links\")\n",
    "            \n",
    "            for a in tel_links:\n",
    "                tel_href = a.get(\"href\", \"\").replace(\"tel:\", \"\").strip()\n",
    "                tel_text = a.get_text(strip=True)\n",
    "                print(f\"  📞 Tel link - href: '{tel_href}', text: '{tel_text}'\")\n",
    "                \n",
    "                # Extract all digits from tel link\n",
    "                digits_only = re.sub(r\"\\D\", \"\", tel_href)\n",
    "                print(f\"  🔢 Tel digits: {digits_only}\")\n",
    "                \n",
    "                # Handle different digit lengths\n",
    "                if len(digits_only) == 10 and digits_only.startswith(\"65\") and digits_only[2] in \"689\":\n",
    "                    # 10 digits starting with 65 (e.g., \"6563192960\")\n",
    "                    formatted = \"+\" + digits_only\n",
    "                    if formatted not in phones:\n",
    "                        phones.append(formatted)\n",
    "                        print(f\"  ✅ Added from tel link (10 digits): {formatted}\")\n",
    "                elif len(digits_only) == 8 and digits_only[0] in \"689\":\n",
    "                    # 8 digits starting with 6/8/9 (e.g., \"63192960\")\n",
    "                    formatted = \"+65\" + digits_only\n",
    "                    if formatted not in phones:\n",
    "                        phones.append(formatted)\n",
    "                        print(f\"  ✅ Added from tel link (8 digits): {formatted}\")\n",
    "                elif len(digits_only) > 10:\n",
    "                    # More than 10 digits, try to find valid pattern\n",
    "                    print(f\"  🔍 Searching within {len(digits_only)} digits for valid pattern...\")\n",
    "                    found = False\n",
    "                    # Look for 65 followed by 6/8/9\n",
    "                    for i in range(len(digits_only) - 9):\n",
    "                        if digits_only[i:i+2] == \"65\" and digits_only[i+2] in \"689\":\n",
    "                            formatted = \"+\" + digits_only[i:i+10]\n",
    "                            if formatted not in phones:\n",
    "                                phones.append(formatted)\n",
    "                                print(f\"  ✅ Added from tel link (extracted): {formatted}\")\n",
    "                            found = True\n",
    "                            break\n",
    "                    if not found:\n",
    "                        # Try last 8 digits if they start with 6/8/9\n",
    "                        if digits_only[-8] in \"689\":\n",
    "                            formatted = \"+65\" + digits_only[-8:]\n",
    "                            if formatted not in phones:\n",
    "                                phones.append(formatted)\n",
    "                                print(f\"  ✅ Added from tel link (last 8 digits): {formatted}\")\n",
    "            \n",
    "            # Method 2: Look in dt/dd structure with broader keywords\n",
    "            dt_tags = parent.select(\"dt\")\n",
    "            if dt_tags:\n",
    "                print(f\"  📋 Found {len(dt_tags)} dt tags\")\n",
    "            \n",
    "            for dt in dt_tags:\n",
    "                dt_text = dt.get_text(strip=True).lower()\n",
    "                # Check for phone-related keywords but exclude non-phone fields\n",
    "                exclude_keywords = [\"officer\", \"charge\", \"employee\", \"shareholder\", \"director\", \"registration\"]\n",
    "                phone_keywords = [\"contact number\", \"phone\", \"tel\", \"mobile\", \"call\", \"contact no\"]\n",
    "                \n",
    "                is_phone_field = any(kw in dt_text for kw in phone_keywords)\n",
    "                is_excluded = any(excl in dt_text for excl in exclude_keywords)\n",
    "                \n",
    "                if is_phone_field and not is_excluded:\n",
    "                    dd = dt.find_next_sibling(\"dd\")\n",
    "                    if dd:\n",
    "                        number_text = dd.get_text(\" \", strip=True)\n",
    "                        print(f\"  📝 Field '{dt_text}': {number_text}\")\n",
    "                        \n",
    "                        # Extract all digits and check if it forms a valid phone number\n",
    "                        all_digits = re.sub(r\"\\D\", \"\", number_text)\n",
    "                        print(f\"  🔢 Extracted digits: {all_digits}\")\n",
    "                        \n",
    "                        # Check for Singapore phone patterns in the digits\n",
    "                        # Pattern 1: 10 digits starting with 65\n",
    "                        if len(all_digits) == 10 and all_digits.startswith(\"65\") and all_digits[2] in \"689\":\n",
    "                            formatted = \"+\" + all_digits\n",
    "                            if formatted not in phones:\n",
    "                                phones.append(formatted)\n",
    "                                print(f\"  ✅ Added from dt/dd (10 digits): {formatted}\")\n",
    "                        # Pattern 2: 8 digits starting with 6, 8, or 9\n",
    "                        elif len(all_digits) == 8 and all_digits[0] in \"689\":\n",
    "                            formatted = \"+65\" + all_digits\n",
    "                            if formatted not in phones:\n",
    "                                phones.append(formatted)\n",
    "                                print(f\"  ✅ Added from dt/dd (8 digits): {formatted}\")\n",
    "                        # Pattern 3: More than 10 digits, try to extract 10-digit number starting with 65\n",
    "                        elif len(all_digits) > 10:\n",
    "                            # Look for 65 followed by 6/8/9 in the digit string\n",
    "                            for i in range(len(all_digits) - 9):\n",
    "                                if all_digits[i:i+2] == \"65\" and all_digits[i+2] in \"689\":\n",
    "                                    potential_number = all_digits[i:i+10]\n",
    "                                    formatted = \"+\" + potential_number\n",
    "                                    if formatted not in phones:\n",
    "                                        phones.append(formatted)\n",
    "                                        print(f\"  ✅ Added from dt/dd (extracted): {formatted}\")\n",
    "                                    break\n",
    "            \n",
    "            # Method 3: Search entire parent for phone patterns if none found\n",
    "            if not phones:\n",
    "                print(f\"  🔎 No phones found yet, searching entire content...\")\n",
    "                full_text = parent.get_text()\n",
    "                \n",
    "                # Ultra-comprehensive patterns to catch ALL spacing variations\n",
    "                # These patterns allow unlimited spaces/dashes between digits\n",
    "                patterns = [\n",
    "                    # Pattern 1: +65 with any spacing (e.g., \"+65 6 3 1 9 2 9 6 0\", \"+65-6319-2960\")\n",
    "                    r\"\\+[\\s\\-]*65[\\s\\-]+[689][\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d\",\n",
    "                    # Pattern 2: (65) with any spacing\n",
    "                    r\"\\([\\s\\-]*65[\\s\\-]*\\)[\\s\\-]*[689][\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d\",\n",
    "                    # Pattern 3: 65 without + or () but with space/dash (e.g., \"65 6 3 1 9 2 9 6 0\", \"65-6319-2960\")\n",
    "                    r\"(?<!\\d)65[\\s\\-]+[689][\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d(?!\\d)\",\n",
    "                    # Pattern 4: Just 8 digits starting with 6/8/9 with any spacing\n",
    "                    r\"(?<!\\d)[689][\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d[\\s\\-]*\\d(?!\\d)\",\n",
    "                ]\n",
    "                \n",
    "                for pattern_idx, pattern in enumerate(patterns, 1):\n",
    "                    matches = re.findall(pattern, full_text)\n",
    "                    if matches:\n",
    "                        print(f\"  🔍 Pattern {pattern_idx} found {len(matches)} potential matches\")\n",
    "                    \n",
    "                    for match in matches:\n",
    "                        # Extract only digits\n",
    "                        digits = re.sub(r\"\\D\", \"\", match)\n",
    "                        print(f\"  🔢 Pattern {pattern_idx} match: '{match.strip()}' → digits: '{digits}'\")\n",
    "                        \n",
    "                        # Validate and format\n",
    "                        if len(digits) == 10 and digits.startswith(\"65\") and digits[2] in \"689\":\n",
    "                            formatted = \"+\" + digits\n",
    "                            if formatted not in phones:\n",
    "                                phones.append(formatted)\n",
    "                                print(f\"  ✅ Added from pattern {pattern_idx} (10 digits): {formatted}\")\n",
    "                        elif len(digits) == 8 and digits[0] in \"689\":\n",
    "                            formatted = \"+65\" + digits\n",
    "                            if formatted not in phones:\n",
    "                                phones.append(formatted)\n",
    "                                print(f\"  ✅ Added from pattern {pattern_idx} (8 digits): {formatted}\")\n",
    "                        elif len(digits) > 10:\n",
    "                            # Try to find a valid 10-digit number within\n",
    "                            for i in range(len(digits) - 9):\n",
    "                                if digits[i:i+2] == \"65\" and digits[i+2] in \"689\":\n",
    "                                    potential = digits[i:i+10]\n",
    "                                    formatted = \"+\" + potential\n",
    "                                    if formatted not in phones:\n",
    "                                        phones.append(formatted)\n",
    "                                        print(f\"  ✅ Added from pattern {pattern_idx} (extracted): {formatted}\")\n",
    "                                    break\n",
    "            \n",
    "            if phones:\n",
    "                print(f\"  ✅ Total phones found: {phones}\")\n",
    "            else:\n",
    "                print(f\"  ⚠️ WARNING: No phone numbers found for {uen}\")\n",
    "                print(f\"  📄 Showing first 500 chars of parent HTML for debugging:\")\n",
    "                print(parent.prettify()[:500] + \"...\")\n",
    "            # ========== END PHONE EXTRACTION ==========\n",
    "\n",
    "            # Extract website\n",
    "            valid_websites = []\n",
    "            for a in parent.select(\"a[href^=http]\"):\n",
    "                href = a.get(\"href\", \"\").strip()\n",
    "                href_lower = href.lower()\n",
    "                if not any(domain in href_lower for domain in SOCIAL_MEDIA_DOMAINS):\n",
    "                    if not any(skip in href_lower for skip in [\"recordowl\", \"apify.com\"]):\n",
    "                        if any(tld in href for tld in [\".com\", \".sg\", \".net\", \".org\", \".co\"]):\n",
    "                            valid_websites.append(href)\n",
    "            website = valid_websites[0] if valid_websites else None\n",
    "\n",
    "        # Extract social media links from entire page\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a[\"href\"].strip().lower()\n",
    "            if \"facebook.com\" in href and href not in facebook_links:\n",
    "                facebook_links.append(href)\n",
    "            elif \"linkedin.com\" in href and href not in linkedin_links:\n",
    "                linkedin_links.append(href)\n",
    "            elif \"instagram.com\" in href and href not in instagram_links:\n",
    "                instagram_links.append(href)\n",
    "            elif \"tiktok.com\" in href and href not in tiktok_links:\n",
    "                tiktok_links.append(href)\n",
    "\n",
    "        all_results.append({\n",
    "            \"UEN\": uen,\n",
    "            \"Emails\": emails if emails else None,\n",
    "            \"Phones\": phones if phones else None,\n",
    "            \"Website\": website,\n",
    "            \"Facebook\": list(set(facebook_links)) if facebook_links else None,\n",
    "            \"LinkedIn\": list(set(linkedin_links)) if linkedin_links else None,\n",
    "            \"Instagram\": list(set(instagram_links)) if instagram_links else None,\n",
    "            \"TikTok\": list(set(tiktok_links)) if tiktok_links else None,\n",
    "            \"RecordOwl_Link\": record_owl_url,\n",
    "        })\n",
    "        print(f\"  ✅ Processed {uen}: {len(emails) if emails else 0} emails, {len(phones) if phones else 0} phones\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error parsing HTML for {uen}: {e}\")\n",
    "        all_results.append({\n",
    "            \"UEN\": uen,\n",
    "            \"Emails\": None,\n",
    "            \"Phones\": None,\n",
    "            \"Website\": None,\n",
    "            \"Facebook\": None,\n",
    "            \"LinkedIn\": None,\n",
    "            \"Instagram\": None,\n",
    "            \"TikTok\": None,\n",
    "            \"RecordOwl_Link\": record_owl_url or None,\n",
    "            \"Error\": f\"HTML parsing error: {str(e)}\"\n",
    "        })\n",
    "\n",
    "    # Dynamic sleep time to avoid rate limiting and 403 blocks\n",
    "    # Longer delays reduce detection and blocking\n",
    "    base_sleep = 20  # Increased from 10\n",
    "    random_addition = (idx % 10) + 5  # 5-14 seconds random\n",
    "    sleep_time = base_sleep + random_addition  # 25-34 seconds total\n",
    "\n",
    "    print(f\"  💤 Sleeping for {sleep_time}s before next request...\")\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "    # Extra delay after every 5th request to further avoid detection\n",
    "    if idx % 5 == 0:\n",
    "        extra_wait = 30\n",
    "        print(f\"  🛑 Checkpoint pause: waiting extra {extra_wait}s...\")\n",
    "        time.sleep(extra_wait)\n",
    "\n",
    "New_Fresh_Leads = pd.DataFrame(all_results)\n",
    "print(\"\\n✅ Scraping complete!\")\n",
    "print(f\"\\n📊 Results summary:\")\n",
    "print(f\"   Total processed: {len(New_Fresh_Leads)}\")\n",
    "print(f\"   With emails: {New_Fresh_Leads['Emails'].notna().sum()}\")\n",
    "print(f\"   With phones: {New_Fresh_Leads['Phones'].notna().sum()}\")\n",
    "print(f\"   With websites: {New_Fresh_Leads['Website'].notna().sum()}\")\n",
    "New_Fresh_Leads.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ce63aaf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UEN</th>\n",
       "      <th>Emails</th>\n",
       "      <th>Phones</th>\n",
       "      <th>Website</th>\n",
       "      <th>Facebook</th>\n",
       "      <th>LinkedIn</th>\n",
       "      <th>Instagram</th>\n",
       "      <th>TikTok</th>\n",
       "      <th>RecordOwl_Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>197502143C</td>\n",
       "      <td>[enquiry@eldric.sg]</td>\n",
       "      <td>[+6563391188]</td>\n",
       "      <td>https://eldric.sg</td>\n",
       "      <td>[https://www.facebook.com/eldricmarketing/]</td>\n",
       "      <td>[https://sg.linkedin.com/company/eldric-market...</td>\n",
       "      <td>[https://www.instagram.com/eldricmarketing/]</td>\n",
       "      <td>None</td>\n",
       "      <td>https://recordowl.com/company/eldric-marketing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          UEN               Emails         Phones            Website  \\\n",
       "0  197502143C  [enquiry@eldric.sg]  [+6563391188]  https://eldric.sg   \n",
       "\n",
       "                                      Facebook  \\\n",
       "0  [https://www.facebook.com/eldricmarketing/]   \n",
       "\n",
       "                                            LinkedIn  \\\n",
       "0  [https://sg.linkedin.com/company/eldric-market...   \n",
       "\n",
       "                                      Instagram TikTok  \\\n",
       "0  [https://www.instagram.com/eldricmarketing/]   None   \n",
       "\n",
       "                                      RecordOwl_Link  \n",
       "0  https://recordowl.com/company/eldric-marketing...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "New_Fresh_Leads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78537b1",
   "metadata": {},
   "source": [
    "### Append and save into exel sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c4e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Load both Excel files\n",
    "# file_path_1 = \"Fresh_Leads.xlsx\"\n",
    "# Fresh_Leads = pd.read_excel(file_path_1)\n",
    "\n",
    "# # file_path_2 = \"recordowl_results_4.xlsx\"\n",
    "# # recordowl_results_4 = pd.read_excel(file_path_2)\n",
    "\n",
    "# # Append (combine) them\n",
    "# combined_df = pd.concat([Fresh_Leads, Fresh_Leads_with_phones], ignore_index=True)\n",
    "\n",
    "# # Optional: Save to a new Excel file\n",
    "# combined_df.to_excel(\"Fresh_Leads_New.xlsx\", index=False)\n",
    "\n",
    "# # Preview\n",
    "# combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257fbbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_non_nan = combined_df['Phones'].notna().sum()\n",
    "# print(count_non_nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0a4e0f",
   "metadata": {},
   "source": [
    "### Website Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ccaea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "\n",
    "# =====================================================\n",
    "# Validate Website (only if no phone number)\n",
    "# =====================================================\n",
    "async def check_url(url: str) -> bool:\n",
    "    \"\"\"Return True if the URL is reachable (status < 400).\"\"\"\n",
    "    if not url:\n",
    "        return False\n",
    "    try:\n",
    "        async with httpx.AsyncClient(follow_redirects=True, timeout=5) as client:\n",
    "            response = await client.head(url)\n",
    "            return response.status_code < 400\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "async def validate_if_needed(df):\n",
    "    \"\"\"Validate websites only if phone number is missing.\"\"\"\n",
    "    for i, row in df.iterrows():\n",
    "        url = row.get(\"Website\")\n",
    "        phone = row.get(\"Phones\")\n",
    "\n",
    "        # Skip validation if phone exists\n",
    "        if phone:\n",
    "            df.at[i, \"Website_Valid\"] = None\n",
    "            continue\n",
    "\n",
    "        # Validate website if no phone\n",
    "        if url:\n",
    "            is_valid = await check_url(url)\n",
    "            df.at[i, \"Website_Valid\"] = \"valid\" if is_valid else \"invalid\"\n",
    "        else:\n",
    "            df.at[i, \"Website_Valid\"] = \"invalid\"\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Run async validation safely inside Jupyter\n",
    "# =====================================================\n",
    "result_df = await validate_if_needed(result_df)\n",
    "\n",
    "# =====================================================\n",
    "# Final output\n",
    "# =====================================================\n",
    "display(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c268400",
   "metadata": {},
   "source": [
    "### If contact number is invalid, then webscrapped website to get contact number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e742795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import time\n",
    "from apify_client import ApifyClient\n",
    "\n",
    "# --- Initialize Apify client ---\n",
    "APIFY_TOKEN = os.getenv(\"APIFY_TOKEN\", \"apify_api_0HQ8fc5fw5T1aosdacxKQNQYVBAEwi3tXaJc\")\n",
    "client = ApifyClient(APIFY_TOKEN)\n",
    "\n",
    "# --- Async wrapper so you can run in Jupyter ---\n",
    "async def enrich_with_contact_info(df):\n",
    "    \"\"\"Scrape contact info for rows where Website_Valid == 'valid' and Phones is empty.\"\"\"\n",
    "    updated_df = df.copy()\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        website = row.get(\"Website\")\n",
    "        status = row.get(\"Website_Valid\")\n",
    "        phone = row.get(\"Phones\")\n",
    "\n",
    "        if not website or status != \"valid\" or phone:\n",
    "            continue  # Skip invalid or already complete rows\n",
    "\n",
    "        print(f\"🔍 Scraping contact page for: {website}\")\n",
    "\n",
    "        # --- CONVERTED TO PUPPETEER-SCRAPER (same as Cell 20) ---\n",
    "        # Now using native Puppeteer syntax instead of jQuery\n",
    "        run_input = {\n",
    "            \"startUrls\": [{\"url\": website}],\n",
    "            \"pageFunction\": r\"\"\"\n",
    "                async function pageFunction(context) {\n",
    "                    const { page, log, request } = context;\n",
    "                    const isContact = request.userData?.isContact || false;\n",
    "\n",
    "                    // If not on contact page yet, try to find and navigate to it\n",
    "                    if (!isContact) {\n",
    "                        try {\n",
    "                            // Wait for page to load\n",
    "                            await page.waitForSelector('a', { timeout: 10000 }).catch(() => null);\n",
    "                            \n",
    "                            // Find contact page link using Puppeteer\n",
    "                            const contactUrl = await page.evaluate(() => {\n",
    "                                const links = Array.from(document.querySelectorAll('a[href]'));\n",
    "                                for (const link of links) {\n",
    "                                    const href = link.getAttribute('href');\n",
    "                                    if (href && href.toLowerCase().includes('contact')) {\n",
    "                                        return href.startsWith('http') ? href : window.location.origin + href;\n",
    "                                    }\n",
    "                                }\n",
    "                                return null;\n",
    "                            });\n",
    "\n",
    "                            if (contactUrl) {\n",
    "                                await context.enqueueRequest({ \n",
    "                                    url: contactUrl, \n",
    "                                    userData: { isContact: true } \n",
    "                                });\n",
    "                                log.info(`Enqueued contact page: ${contactUrl}`);\n",
    "                            }\n",
    "                            return null;\n",
    "                        } catch (err) {\n",
    "                            log.error(`Error finding contact page: ${err.message}`);\n",
    "                            return null;\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "                    // We're on the contact page - extract emails and phones\n",
    "                    try {\n",
    "                        // Wait for content to load\n",
    "                        await new Promise(r => setTimeout(r, 3000));\n",
    "\n",
    "                        // Extract emails and phones using Puppeteer\n",
    "                        const contactData = await page.evaluate(() => {\n",
    "                            // Helper: check if element is visible\n",
    "                            function isVisible(el) {\n",
    "                                return el && el.offsetParent !== null;\n",
    "                            }\n",
    "\n",
    "                            // Extract emails from mailto links\n",
    "                            const emailLinks = Array.from(document.querySelectorAll('a[href^=\"mailto\"]'));\n",
    "                            const emails = emailLinks\n",
    "                                .filter(el => isVisible(el))\n",
    "                                .map(el => el.getAttribute('href').replace('mailto:', '').trim())\n",
    "                                .filter(email => email.length > 0);\n",
    "\n",
    "                            // Extract phones from tel links\n",
    "                            const phoneLinks = Array.from(document.querySelectorAll('a[href^=\"tel\"]'));\n",
    "                            const phones = phoneLinks\n",
    "                                .filter(el => isVisible(el))\n",
    "                                .map(el => el.getAttribute('href').replace(/[^0-9]/g, ''))\n",
    "                                .filter(phone => phone.length > 0);\n",
    "\n",
    "                            return {\n",
    "                                emails: [...new Set(emails)],\n",
    "                                phones: [...new Set(phones)]\n",
    "                            };\n",
    "                        });\n",
    "\n",
    "                        return {\n",
    "                            contactUrl: request.url,\n",
    "                            emails: contactData.emails.length ? contactData.emails : [],\n",
    "                            phones: contactData.phones.length ? contactData.phones : []\n",
    "                        };\n",
    "                    } catch (err) {\n",
    "                        log.error(`Error extracting contact data: ${err.message}`);\n",
    "                        return {\n",
    "                            contactUrl: request.url,\n",
    "                            emails: [],\n",
    "                            phones: [],\n",
    "                            error: err.message\n",
    "                        };\n",
    "                    }\n",
    "                }\n",
    "            \"\"\",\n",
    "            \"useChrome\": True,\n",
    "            \"headless\": True,\n",
    "            \"stealth\": True,\n",
    "            \"ignoreSslErrors\": False,\n",
    "            \"ignoreCorsAndCsp\": False,\n",
    "            \"maxRequestRetries\": 3,  # Increased retry attempts\n",
    "            \"maxRequestsPerCrawl\": 0,  # No limit (will crawl main + contact pages)\n",
    "            \"maxConcurrency\": 1,  # No parallel requests\n",
    "            \"pageLoadTimeoutSecs\": 90,  # Optimized timeout\n",
    "            \"pageFunctionTimeoutSecs\": 180,  # 3 minutes for pageFunction\n",
    "            \"waitUntil\": [\"networkidle2\"],  # Wait for network to be idle\n",
    "            # OPTIMIZED: Residential proxies with recommended rotation\n",
    "            \"proxyConfiguration\": {\n",
    "                \"useApifyProxy\": True,\n",
    "                \"apifyProxyGroups\": [\"RESIDENTIAL\"],  # Residential IPs less likely to be blocked\n",
    "            },\n",
    "            \"proxyRotation\": \"RECOMMENDED\",  # Optimal proxy rotation strategy\n",
    "        }\n",
    "\n",
    "        # --- Run the Apify scraper (NOW USING PUPPETEER-SCRAPER) ---\n",
    "        try:\n",
    "            print(f\"  📡 Starting Apify puppeteer-scraper...\")\n",
    "            run = client.actor(\"apify/puppeteer-scraper\").call(run_input=run_input)\n",
    "            \n",
    "            # Wait for dataset to be ready\n",
    "            time.sleep(3)\n",
    "            \n",
    "            dataset = client.dataset(run[\"defaultDatasetId\"])\n",
    "            results = list(dataset.iterate_items())\n",
    "            contact_results = [r for r in results if r and (r.get(\"emails\") or r.get(\"phones\"))]\n",
    "\n",
    "            if contact_results:\n",
    "                scraped = contact_results[0]\n",
    "                updated_df.at[i, \"Emails\"] = scraped.get(\"emails\", None)\n",
    "                updated_df.at[i, \"Phones\"] = scraped.get(\"phones\", None)\n",
    "                updated_df.at[i, \"Contact_Page\"] = scraped.get(\"contactUrl\", None)\n",
    "                print(f\"  ✅ Found: {scraped.get('phones', [])} / {scraped.get('emails', [])}\")\n",
    "            else:\n",
    "                print(\"  ⚠️ No contact data found.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error scraping {website}: {e}\")\n",
    "        \n",
    "        # Add delay to avoid rate limiting\n",
    "        time.sleep(5)\n",
    "\n",
    "    return updated_df\n",
    "\n",
    "\n",
    "# --- Run the scraper for valid websites ---\n",
    "result_df = await enrich_with_contact_info(result_df)\n",
    "\n",
    "# --- Display updated results ---\n",
    "display(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad033e9f",
   "metadata": {},
   "source": [
    "### Facebook Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9c9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[apify.facebook-page-contact-information runId:NuFdyfMTFapYz30sz]\u001b[0m -> Status: RUNNING, Message: \n",
      "\u001b[36m[apify.facebook-page-contact-information runId:NuFdyfMTFapYz30sz]\u001b[0m -> 2025-11-04T08:10:05.959Z ACTOR: Pulling container image of build hAtXVuN3UKeSX06iI from registry.\n",
      "\u001b[36m[apify.facebook-page-contact-information runId:NuFdyfMTFapYz30sz]\u001b[0m -> 2025-11-04T08:10:05.961Z ACTOR: Creating container.\n",
      "\u001b[36m[apify.facebook-page-contact-information runId:NuFdyfMTFapYz30sz]\u001b[0m -> 2025-11-04T08:10:06.242Z ACTOR: Starting container.\n",
      "\u001b[36m[apify.facebook-page-contact-information runId:NuFdyfMTFapYz30sz]\u001b[0m -> 2025-11-04T08:10:07.636Z \u001b[32mINFO\u001b[39m  System info\u001b[90m {\"apifyVersion\":\"3.4.3\",\"apifyClientVersion\":\"2.12.5\",\"crawleeVersion\":\"3.13.7\",\"osType\":\"Linux\",\"nodeVersion\":\"v20.19.5\"}\u001b[39m\n",
      "\u001b[36m[apify.facebook-page-contact-information runId:NuFdyfMTFapYz30sz]\u001b[0m -> 2025-11-04T08:10:07.806Z \u001b[32mINFO\u001b[39m  Results Limit undefined\n",
      "\u001b[36m[apify.facebook-page-contact-information runId:NuFdyfMTFapYz30sz]\u001b[0m -> 2025-11-04T08:10:07.812Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Using the old RequestQueue implementation without request locking.\n",
      "\u001b[36m[apify.facebook-page-contact-information runId:NuFdyfMTFapYz30sz]\u001b[0m -> Status: RUNNING, Message: Starting the crawler.\n",
      "\u001b[36m[apify.facebook-page-contact-information runId:NuFdyfMTFapYz30sz]\u001b[0m -> 2025-11-04T08:10:08.121Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Starting the crawler.\n",
      "\u001b[36m[apify.facebook-page-contact-information runId:NuFdyfMTFapYz30sz]\u001b[0m -> 2025-11-04T08:10:13.157Z \u001b[32mINFO\u001b[39m  [PROGRESS]: Found 1 new page_contact_information, 1 for URL https://www.facebook.com/KPECTHub/\n",
      "\u001b[36m[apify.facebook-page-contact-information runId:NuFdyfMTFapYz30sz]\u001b[0m -> 2025-11-04T08:10:13.316Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m All requests from the queue have been processed, the crawler will shut down.\n",
      "\u001b[36m[apify.facebook-page-contact-information runId:NuFdyfMTFapYz30sz]\u001b[0m -> 2025-11-04T08:10:13.403Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Final request statistics:\u001b[90m {\"requestsFinished\":1,\"requestsFailed\":0,\"retryHistogram\":[1],\"requestAvgFailedDurationMillis\":null,\"requestAvgFinishedDurationMillis\":5062,\"requestsFinishedPerMinute\":11,\"requestsFailedPerMinute\":0,\"requestTotalDurationMillis\":5062,\"requestsTotal\":1,\"crawlerRuntimeMillis\":5592}\u001b[39m\n",
      "\u001b[36m[apify.facebook-page-contact-information runId:NuFdyfMTFapYz30sz]\u001b[0m -> 2025-11-04T08:10:13.406Z \u001b[32mINFO\u001b[39m \u001b[33m CheerioCrawler:\u001b[39m Finished! Total 1 requests: 1 succeeded, 0 failed.\u001b[90m {\"terminal\":true}\u001b[39m\n",
      "\u001b[36m[apify.facebook-page-contact-information runId:NuFdyfMTFapYz30sz]\u001b[0m -> Status: RUNNING, Message: Finished! Total 1 requests: 1 succeeded, 0 failed.\n",
      "\u001b[36m[apify.facebook-page-contact-information runId:NuFdyfMTFapYz30sz]\u001b[0m -> 2025-11-04T08:10:13.469Z \u001b[32mINFO\u001b[39m  *** DONE ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>facebook_url</th>\n",
       "      <th>page_name</th>\n",
       "      <th>phone</th>\n",
       "      <th>email</th>\n",
       "      <th>website</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.facebook.com/KPECTHub/</td>\n",
       "      <td>KPECTHub</td>\n",
       "      <td>+65 9799 9960</td>\n",
       "      <td>clubxy@icloud.com</td>\n",
       "      <td>https://maps.google.com/maps?q=2+Kallang+Avenu...</td>\n",
       "      <td>2 Kallang Avenue #01-04 CT Hub S339407, Singap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         facebook_url page_name          phone  \\\n",
       "0  https://www.facebook.com/KPECTHub/  KPECTHub  +65 9799 9960   \n",
       "\n",
       "               email                                            website  \\\n",
       "0  clubxy@icloud.com  https://maps.google.com/maps?q=2+Kallang+Avenu...   \n",
       "\n",
       "                                             address  \n",
       "0  2 Kallang Avenue #01-04 CT Hub S339407, Singap...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the ApifyClient with your API token\n",
    "client = ApifyClient(\"apify_api_yNR85etaHpLtBzPoVozVVXUsCZe54u2Ffog1\")\n",
    "\n",
    "# Function to validate Singapore phone numbers (MUST have country code)\n",
    "def validate_singapore_number(phone):\n",
    "    if not phone:\n",
    "        return None\n",
    "    \n",
    "    # Remove all spaces, dashes, parentheses\n",
    "    cleaned = re.sub(r'[\\s\\-\\(\\)]', '', str(phone))\n",
    "    \n",
    "    # MUST have country code: +65XXXXXXXX or 65XXXXXXXX\n",
    "    # First digit after country code must be 6, 8, or 9\n",
    "    # Total of 8 digits after country code\n",
    "    if re.match(r'^\\+?65[689]\\d{7}$', cleaned):\n",
    "        return phone  # Return original format\n",
    "    \n",
    "    # Not a valid Singapore number with country code\n",
    "    return None\n",
    "\n",
    "# Prepare the Actor input\n",
    "run_input = {\n",
    "    \"pages\": [\n",
    "        \"https://www.facebook.com/KPECTHub/\",\n",
    "    ],\n",
    "    \"language\": \"en-US\",\n",
    "}\n",
    "\n",
    "# Run the Actor and wait for it to finish\n",
    "run = client.actor(\"oJ48ceKNY7ueGPGL0\").call(run_input=run_input)\n",
    "\n",
    "# Collect results\n",
    "results = []\n",
    "for item in client.dataset(run[\"defaultDatasetId\"]).iterate_items():\n",
    "    # Extract phone from multiple possible fields\n",
    "    raw_phone = item.get('phone', None) or item.get('wa_number', None)\n",
    "    \n",
    "    # Validate it's a Singapore number WITH country code\n",
    "    phone = validate_singapore_number(raw_phone)\n",
    "    \n",
    "    # Extract email\n",
    "    email = item.get('email', None)\n",
    "    \n",
    "    # Extract website from the websites list (take first non-Google Maps link if available)\n",
    "    websites = item.get('websites', [])\n",
    "    website = None\n",
    "    if websites:\n",
    "        # Filter out Google Maps links and take the first real website\n",
    "        real_websites = [w for w in websites if 'maps.google.com' not in w]\n",
    "        website = real_websites[0] if real_websites else websites[0]\n",
    "    \n",
    "    results.append({\n",
    "        'facebook_url': item.get('facebookUrl', None),\n",
    "        'page_name': item.get('pageName', None),\n",
    "        'phone': phone,  # Only Singapore numbers WITH country code or None\n",
    "        'email': email,\n",
    "        'website': website,\n",
    "        'address': item.get('address', None)\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
